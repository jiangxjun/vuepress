<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>BERT预训练模型 | Docs Library by #XJ</title>
    <meta name="generator" content="VuePress 1.9.7">
    <script src="https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@babel/standalone/babel.min.js"></script>
    <link rel="alternate" type="application/atom+xml" href="https://jiangxj.top/vuepress/vuepress/atom.xml" title="Docs Library by #XJ Atom Feed">
    <link rel="alternate" type="application/json" href="https://jiangxj.top/vuepress/vuepress/feed.json" title="Docs Library by #XJ JSON Feed">
    <link rel="alternate" type="application/rss+xml" href="https://jiangxj.top/vuepress/vuepress/rss.xml" title="Docs Library by #XJ RSS Feed">
    <link rel="icon" href="/favicon.ico">
    <link rel="icon" href="/assets/icon/chrome-mask-512.png" type="image/png" sizes="512x512">
    <link rel="icon" href="/assets/icon/chrome-mask-192.png" type="image/png" sizes="192x192">
    <link rel="icon" href="/assets/icon/chrome-512.png" type="image/png" sizes="512x512">
    <link rel="icon" href="/assets/icon/chrome-192.png" type="image/png" sizes="192x192">
    <link rel="manifest" href="/vuepress/manifest.webmanifest" crossorigin="use-credentials">
    <link rel="apple-touch-icon" href="/assets/icon/apple-icon-152.png">
    <meta name="description" content="BERT预训练模型原理分析">
    <meta property="og:url" content="/vuepress/note/NLP/bert.html">
    <meta property="og:site_name" content="Docs Library by #XJ">
    <meta property="og:title" content="BERT预训练模型">
    <meta property="og:description" content="BERT预训练模型原理分析">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh-CN">
    <meta property="og:locale:alternate" content="en-US">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image:alt" content="Docs Library by #XJ">
    <meta property="article:author" content="XJ">
    <meta property="article:tag" content="概念理论">
    <meta property="article:published_time" content="2022-11-15T00:00:00.000Z">
    <meta name="theme-color" content="#46bd87">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="msapplication-TileImage" content="/assets/icon/ms-icon-144.png">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
    
    <link rel="preload" href="/vuepress/assets/css/0.styles.79a9e6b0.css" as="style"><link rel="preload" href="/vuepress/assets/js/app.63a6b71e.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Layout.714049f4.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Blog~layout-Layout~layout-NotFound.f5405c06.js" as="script"><link rel="preload" href="/vuepress/assets/js/page-BERT预训练模型.2ee39f07.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Blog~layout-Layout.407483c8.js" as="script"><link rel="prefetch" href="/vuepress/assets/js/62.ddea15ac.js"><link rel="prefetch" href="/vuepress/assets/js/63.2ab102a2.js"><link rel="prefetch" href="/vuepress/assets/js/64.005312f8.js"><link rel="prefetch" href="/vuepress/assets/js/65.b548f3c6.js"><link rel="prefetch" href="/vuepress/assets/js/66.549bc712.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Blog.38f6e5a9.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Layout.e3c1d55e.js"><link rel="prefetch" href="/vuepress/assets/js/layout-NotFound.1dafa333.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Slide.22421d46.js"><link rel="prefetch" href="/vuepress/assets/js/page-BlogHome.af52b4f6.js"><link rel="prefetch" href="/vuepress/assets/js/page-Challengesandapproachedtotime-seriesforecasting.2fcea3e6.js"><link rel="prefetch" href="/vuepress/assets/js/page-Componentdisabled.ffadc3d5.js"><link rel="prefetch" href="/vuepress/assets/js/page-CustomLayout.8fd0154f.js"><link rel="prefetch" href="/vuepress/assets/js/page-DLforLoadForecastingsequencetosequenceRNNswithAttention.a55d615e.js"><link rel="prefetch" href="/vuepress/assets/js/page-Deepleanringfortimeseriesforecastingasurvey.8d059b40.js"><link rel="prefetch" href="/vuepress/assets/js/page-Embedding和word2vec理解.11af4fcf.js"><link rel="prefetch" href="/vuepress/assets/js/page-Encryptionarticle.dc677e6d.js"><link rel="prefetch" href="/vuepress/assets/js/page-Guides.9fab118e.js"><link rel="prefetch" href="/vuepress/assets/js/page-HTML基础知识.f167246e.js"><link rel="prefetch" href="/vuepress/assets/js/page-HelloVuepress.76f46302.js"><link rel="prefetch" href="/vuepress/assets/js/page-Homepage.125f2ea2.js"><link rel="prefetch" href="/vuepress/assets/js/page-IntroPage.9076bdb7.js"><link rel="prefetch" href="/vuepress/assets/js/page-MarkdownEnhance.08702e54.js"><link rel="prefetch" href="/vuepress/assets/js/page-Markdown增强.6b228f3e.js"><link rel="prefetch" href="/vuepress/assets/js/page-Projecthome.64201229.js"><link rel="prefetch" href="/vuepress/assets/js/page-Slidepage.217de802.js"><link rel="prefetch" href="/vuepress/assets/js/page-Transformer-Mask机制理解.d3afe08a.js"><link rel="prefetch" href="/vuepress/assets/js/page-Ubuntu1604配置cuda和cudnn.33c321cd.js"><link rel="prefetch" href="/vuepress/assets/js/page-VuePress默认主题设置.85d22925.js"><link rel="prefetch" href="/vuepress/assets/js/page-Web前端平台搭建.488a35ec.js"><link rel="prefetch" href="/vuepress/assets/js/page-docker使用.8840900d.js"><link rel="prefetch" href="/vuepress/assets/js/page-不完备信息系统-粗糙集理论.abc3021f.js"><link rel="prefetch" href="/vuepress/assets/js/page-主成分分析（PCA）.5488bebd.js"><link rel="prefetch" href="/vuepress/assets/js/page-人工智能的数学基础知识.5d8fb9f0.js"><link rel="prefetch" href="/vuepress/assets/js/page-先进核能概论.8f1a8fee.js"><link rel="prefetch" href="/vuepress/assets/js/page-决策树.16d2f952.js"><link rel="prefetch" href="/vuepress/assets/js/page-分类与回归.2281637d.js"><link rel="prefetch" href="/vuepress/assets/js/page-卷积神经网络理解.2e46266a.js"><link rel="prefetch" href="/vuepress/assets/js/page-基于Transformers模型的时序预测（TSF）方法.1249ee6d.js"><link rel="prefetch" href="/vuepress/assets/js/page-密码加密的文章.a9e4e164.js"><link rel="prefetch" href="/vuepress/assets/js/page-抽样分布.8bedfaef.js"><link rel="prefetch" href="/vuepress/assets/js/page-最小二乘法(LSM).03e08fe3.js"><link rel="prefetch" href="/vuepress/assets/js/page-机器学习的基本设计方法和途径.efd9d259.js"><link rel="prefetch" href="/vuepress/assets/js/page-机器学习算法理解.a013ce47.js"><link rel="prefetch" href="/vuepress/assets/js/page-概率论-贝叶斯公式.b5ab5633.js"><link rel="prefetch" href="/vuepress/assets/js/page-深度强化学习——基础、研究及应用.09723c50.js"><link rel="prefetch" href="/vuepress/assets/js/page-深度强化学习（DeepReinforcementLearning）基础1.e8459927.js"><link rel="prefetch" href="/vuepress/assets/js/page-生成模型.00d27e14.js"><link rel="prefetch" href="/vuepress/assets/js/page-第四代先进核能系统概述.90ddcff2.js"><link rel="prefetch" href="/vuepress/assets/js/page-线性代数-随机变量的数字特征.daada953.js"><link rel="prefetch" href="/vuepress/assets/js/page-组件禁用.19c22b31.js"><link rel="prefetch" href="/vuepress/assets/js/page-自然语言处理基础知识.b7a0d8d3.js"><link rel="prefetch" href="/vuepress/assets/js/page-计算机网络.49bbe557.js"><link rel="prefetch" href="/vuepress/assets/js/page-计算机网络概述.daae640f.js"><link rel="prefetch" href="/vuepress/assets/js/page-语言模型的简要梳理介绍.a45b4449.js"><link rel="prefetch" href="/vuepress/assets/js/page-这主题废弃了标题？.d62b6d66.js"><link rel="prefetch" href="/vuepress/assets/js/page-页面配置.b2b5779f.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~flowchart.7cb4e065.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~mermaid.b9369779.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~photo-swipe.77a3a8a3.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~reveal.0c21a847.js">
    <link rel="stylesheet" href="/vuepress/assets/css/0.styles.79a9e6b0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container has-navbar has-anchor"><header class="navbar"><!----> <div class="content__navbar-start"></div> <button title="Sidebar Button" class="sidebar-button"><span class="icon"></span></button> <a href="/vuepress/" class="home-link router-link-active"><img src="/vuepress/logo.svg" alt="Docs Library by #XJ" class="logo"> <!----> <span class="site-name can-hide">Docs Library by #XJ</span></a> <!----> <div class="content__navbar-center"></div> <div class="links"><button tabindex="-1" aria-hidden="true" class="color-button"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="skin-icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4
        38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32
        51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0
        102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2
        6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4
        0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2
        9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224
        419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4
        470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0
        22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6
        12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128
        505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2
        16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8
        86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4
        80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6
        6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg> <div class="color-picker-menu" style="display:none;"><div class="theme-options"><ul class="themecolor-select"><label for="themecolor-select">主题色:</label> <li><span class="default-theme"></span></li> </ul> <div class="darkmode-toggle"><label for="darkmode-toggle" class="desc">主题模式:</label> <div class="darkmode-switch"><div class="item day"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon light-icon"><path d="M512 256a42.667 42.667 0 0 0 42.667-42.667V128a42.667 42.667 0 0 0-85.334 0v85.333A42.667 42.667 0 0 0 512 256zm384 213.333h-85.333a42.667 42.667 0 0 0 0 85.334H896a42.667 42.667 0 0 0 0-85.334zM256 512a42.667 42.667 0 0 0-42.667-42.667H128a42.667 42.667 0 0 0 0 85.334h85.333A42.667 42.667 0 0 0 256 512zm9.387-298.667a42.667 42.667 0 0 0-59.307 62.72l61.44 59.307a42.667 42.667 0 0 0 31.147 11.947 42.667 42.667 0 0 0 30.72-13.227 42.667 42.667 0 0 0 0-60.16zm459.946 133.974a42.667 42.667 0 0 0 29.44-11.947l61.44-59.307a42.667 42.667 0 0 0-57.6-62.72l-61.44 60.587a42.667 42.667 0 0 0 0 60.16 42.667 42.667 0 0 0 28.16 13.227zM512 768a42.667 42.667 0 0 0-42.667 42.667V896a42.667 42.667 0 0 0 85.334 0v-85.333A42.667 42.667 0 0 0 512 768zm244.48-79.36a42.667 42.667 0 0 0-59.307 61.44l61.44 60.587a42.667 42.667 0 0 0 29.44 11.946 42.667 42.667 0 0 0 30.72-12.8 42.667 42.667 0 0 0 0-60.586zm-488.96 0-61.44 59.307a42.667 42.667 0 0 0 0 60.586 42.667 42.667 0 0 0 30.72 12.8 42.667 42.667 0 0 0 28.587-10.666l61.44-59.307a42.667 42.667 0 0 0-59.307-61.44zM512 341.333A170.667 170.667 0 1 0 682.667 512 170.667 170.667 0 0 0 512 341.333z" fill="currentColor"></path></svg></div> <div class="item auto active"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon auto-icon"><path d="M460.864 539.072H564.8L510.592 376l-49.728 163.072zM872 362.368V149.504H659.648L510.528 0l-149.12 149.504H149.12v212.928L0 511.872l149.12 149.504v212.928h212.352l149.12 149.504 149.12-149.504h212.352V661.376l149.12-149.504L872 362.368zM614.464 693.12l-31.616-90.624H438.272l-31.616 90.624h-85.888l144.576-407.68h90.368l144.576 407.68h-85.824zm0 0" fill="currentColor"></path></svg></div> <div class="item night"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon dark-icon"><path d="M935.539 630.402c-11.43-11.432-28.674-14.739-43.531-8.354-46.734 20.103-96.363 30.297-147.508 30.297-99.59 0-193.221-38.784-263.64-109.203-108.637-108.637-139.61-270.022-78.908-411.148a39.497 39.497 0 0 0-51.886-51.887c-52.637 22.64-100.017 54.81-140.826 95.616-85.346 85.346-132.346 198.821-132.346 319.52 0 120.7 47.001 234.172 132.347 319.519S408.063 947.11 528.76 947.11c120.7 0 234.172-47.003 319.52-132.351 40.809-40.81 72.978-88.19 95.616-140.826a39.497 39.497 0 0 0-8.356-43.532z" fill="currentColor"></path></svg></div></div> <!----></div></div></div></button> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/vuepress/" class="nav-link router-link-active"><i class="iconfont icon-home"></i>
  Home
</a></div><div class="nav-item"><a href="/vuepress/home/" class="nav-link"><i class="iconfont icon-notice"></i>
  Target
</a></div><div class="nav-item"><a href="/vuepress/guide/" class="nav-link"><i class="iconfont icon-creative"></i>
  Guide
</a></div><div class="nav-item"><a href="https://jiangxj.top/blog//" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont icon-blog"></i>
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div></nav> <div class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" style="width:1rem;height:1rem;vertical-align:middle;margin-left:1rem;"><path d="M639.981 344.075c14.805 44.45 34.542 79.023 69.084 113.596 29.603-29.634 49.34-69.146 64.145-113.596H639.981zM314.33 591.024h128.29l-64.145-172.865-64.145 172.865z" fill="currentColor"></path> <path d="M807.746 116.882H215.643c-54.274 0-98.681 44.45-98.681 98.78v592.677c0 54.329 44.407 98.78 98.68 98.78h592.104c54.273 0 98.681-44.451 98.681-98.78V215.66c0-54.329-39.475-98.78-98.68-98.78zM565.971 754.01c-9.866 9.878-19.738 9.878-29.603 9.878-4.94 0-14.805 0-19.738-4.939-4.939-4.939-9.872 0-9.872-4.939s-4.932-9.878-9.865-19.756c-4.94-9.878-4.94-14.817-9.872-24.695L467.29 655.23H294.592l-19.737 54.33c-9.866 19.755-14.805 34.572-19.738 44.45-4.939 9.878-14.804 9.878-29.603 9.878-9.871 0-19.737-4.939-29.609-9.878-9.865-9.878-14.798-14.817-14.798-24.695 0-4.939 0-9.878 4.933-19.756 4.939-9.878 4.939-14.817 9.865-24.695l108.553-276.583c4.939-9.878 4.939-19.756 9.872-29.633 4.932-9.878 9.865-19.756 14.798-24.695 4.939-4.94 9.872-14.817 19.737-19.756 9.872-4.94 19.738-4.94 29.61-4.94 9.865 0 19.73 0 29.603 4.94 9.865 4.939 14.804 9.878 19.737 19.756 4.933 4.939 9.866 14.817 14.798 24.695 4.94 9.877 9.872 19.755 14.805 34.572l108.553 271.644c9.865 19.756 14.804 34.573 14.804 44.451-4.939 4.94-9.872 14.817-14.804 24.695zm271.378-192.62c-54.273-19.756-93.748-44.451-128.29-74.085-34.536 34.573-78.943 59.268-133.223 74.085l-14.798-24.695c54.273-14.817 98.68-34.573 133.223-69.146-34.542-34.573-64.145-79.024-74.017-128.413h-49.34V319.38h133.228c-9.877-14.817-19.743-34.573-29.609-49.39l14.799-4.94c9.871 14.818 24.676 34.574 34.542 54.33h123.35v24.695h-49.34c-14.798 49.39-39.468 93.84-69.077 123.474 34.541 29.634 74.01 54.329 128.29 69.146l-19.738 24.695z" fill="currentColor"></path></svg> <span class="arrow"></span></button> <ul class="nav-dropdown"><li class="dropdown-item"><a href="/vuepress/note/NLP/bert/" aria-current="page" class="nav-link router-link-exact-active router-link-active active"><!---->
  简体中文
</a></li><li class="dropdown-item"><a href="/vuepress/en/" class="nav-link"><!---->
  English
</a></li></ul></div></div></div> <a rel="noopener noreferrer" href="https://github.com/jiangxjun/vuepress" target="_blank" class="repo-link can-hide">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <!----> <div class="content__navbar-end"></div></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><div vocab="https://schema.org/" typeof="Person" class="blogger-info mobile"><div data-balloon-pos="down" role="navigation" class="blogger hasIntro"><img property="image" alt="Blogger Avatar" src="/vuepress/logo.svg" class="avatar round"> <div property="name" class="name">#XJ</div> <meta property="url" content="/vuepress/intro/"></div> <div class="num-wrapper"><div><div class="num">46</div> <div>文章</div></div> <div><div class="num">10</div> <div>分类</div></div> <div><div class="num">22</div> <div>标签</div></div> <div><div class="num">26</div> <div>时间轴</div></div></div> <div class="media-links-wrapper"><a href="https://zhihu.com" rel="noopener noreferrer" target="_blank" aria-label="Zhihu" data-balloon-pos="up" class="media-link"><span class="sr-only">Zhihu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-zhihu"><circle cx="512" cy="512" r="512" fill="#006CE2"></circle> <path d="M513.65 491.261H411.551c1.615-16.154 5.815-60.095 5.815-84.973 0-24.88-.323-60.742-.323-60.742h102.744V329.39c0-21.647-9.37-31.34-17.124-31.34h-178.67s5.169-17.77 10.015-36.186c4.846-18.417 15.832-44.264 15.832-44.264-63.003 4.2-67.958 50.941-81.743 92.729-13.787 41.785-24.556 62.356-44.586 107.912 27.786 0 55.249-13.57 66.879-32.309 11.631-18.74 16.908-40.71 16.908-40.71h62.035v59.019c0 21.107-3.878 87.45-3.878 87.45H254.742c-19.386 0-29.724 48.894-29.724 48.894h133.76c-8.4 75.82-26.493 106.191-51.91 152.716-25.418 46.525-92.728 99.406-92.728 99.406 41.033 11.63 86.589-3.555 105.974-21.972 19.386-18.417 35.863-49.756 47.817-72.838 11.954-23.081 21.972-65.124 21.972-65.124L498.462 766.86s4.846-24.233 6.461-39.418c1.616-15.186-.755-26.385-4.63-35.433-3.878-9.046-15.509-21.54-31.018-39.634-15.507-18.094-48.034-52.879-48.034-52.879s-15.832 11.63-28.108 21.001c9.046-21.97 16.262-79.695 16.262-79.695h122.343v-20.249c.003-17.66-7.319-29.29-18.089-29.29zm287.337-200.747h-234.35a4.308 4.308 0 0 0-4.309 4.308v435.099a4.308 4.308 0 0 0 4.308 4.308h40.226l14.7 50.402 81.096-50.402h98.328a4.308 4.308 0 0 0 4.308-4.308v-435.1a4.308 4.308 0 0 0-4.308-4.308zM755.97 684.47h-52.343l-61.548 39.095-10.823-39.095h-18.738V338.116H755.97v346.355z" fill="#FFF"></path></svg></a><a href="https://baidu.com" rel="noopener noreferrer" target="_blank" aria-label="Baidu" data-balloon-pos="up" class="media-link"><span class="sr-only">Baidu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-baidu"><circle cx="512" cy="512" r="512" fill="#1D2FE3"></circle> <path d="M239.022 704.978c.098-4.865-.314-9.772.162-14.591 5.178-52.464 197.571-253.377 249.641-259.233 42.996-4.833 75.768 16.545 99.824 49.144 37.893 51.351 82.81 95.455 131.292 136.237 52.903 44.503 56.525 99.801 32.6 158.592-23.425 57.56-75.34 69.833-127.771 58.804-84.971-17.874-168.158-13.744-253.37-4.536-86.35 9.333-133.788-39.4-132.378-124.417zM352.464 412.86c-3.58 50.707-17.93 96.128-75.9 98.12-58.053 1.995-80.093-41.432-79.275-91.71.81-49.705 13.416-104.053 76.851-102.136 53.84 1.625 74.74 45.8 78.324 95.726zm386.053 142.168c-68.494-1.735-84.188-43.331-82.635-93.812 1.46-47.519 10.082-97.628 73.299-96.65 61.395.95 81.6 43.207 81.553 98.668-.047 53.156-19.818 89.398-72.217 91.794zm-45.235-278.345c-10.464 42.665-24.513 91.761-85.919 94.502-52.74 2.354-71.705-34.482-72.805-81.242-1.233-52.42 48.08-112.965 87.582-110.373 33.943 2.226 71.146 49.541 71.142 97.113zm-195.147-14.097c-7.005 46.274-13.63 100.025-71.562 101.351-57.077 1.306-73.567-47.922-73.638-97.109-.068-48.054 12.128-99.024 69.345-101.426 59.45-2.493 67.11 51.093 75.855 97.184z" fill="#fff"></path> <path d="M479.52 663.165c.006 12.194 1.498 24.61-.284 36.537-4.707 31.503 18.862 78.749-45.326 77.534-54.226-1.027-103.338-3.31-113.231-73.536-7.164-50.852 7.78-85.674 57.687-102.668 17.67-6.016 39.618 5.058 54.096-14.548 10.84-14.679-2.901-54.592 33.418-41.47 24.075 8.7 11.477 38.922 13.278 59.652 1.68 19.366.359 38.99.363 58.5zm175.45 41.902c4.291 39.657 5.093 78.047-64.709 73.503-60.097-3.912-95.56-20.794-86.293-85.624 4.287-29.991-21.148-83.238 22.19-84.867 42.71-1.606 13.57 50.41 20.825 77.622 5.276 19.794-3.984 46.774 29.753 48.193 41.337 1.738 28.383-30.022 31.099-51.604 1.209-9.61-.85-19.65.528-29.215 2.516-17.474-8.928-44.716 19.554-47.191 36.044-3.133 24.155 28.376 26.678 47.523 1.896 14.387.375 29.225.375 51.66z" fill="#1D2FE3"></path> <path d="M435.669 685.038c-2.255 24.07 5.605 53.68-33.623 52.136-34.594-1.362-35.274-31.818-38.513-53.078-4.028-26.448 11.38-48.18 40.785-50.023 40.967-2.564 27.097 30.764 31.35 50.965z" fill="#fff"></path></svg></a><a href="https://github.com" rel="noopener noreferrer" target="_blank" aria-label="Github" data-balloon-pos="up" class="media-link"><span class="sr-only">Github</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-github"><circle cx="512" cy="512" r="512" fill="#171515"></circle> <path d="M509.423 146.442c-200.317 0-362.756 162.42-362.756 362.8 0 160.266 103.936 296.24 248.109 344.217 18.139 3.327 24.76-7.872 24.76-17.486 0-8.613-.313-31.427-.49-61.702-100.912 21.923-122.205-48.63-122.205-48.63-16.495-41.91-40.28-53.067-40.28-53.067-32.937-22.51 2.492-22.053 2.492-22.053 36.407 2.566 55.568 37.386 55.568 37.386 32.362 55.438 84.907 39.43 105.58 30.143 3.296-23.444 12.667-39.43 23.032-48.498-80.557-9.156-165.246-40.28-165.246-179.297 0-39.604 14.135-71.988 37.342-97.348-3.731-9.178-16.18-46.063 3.556-96.009 0 0 30.46-9.754 99.76 37.19 28.937-8.048 59.97-12.071 90.823-12.211 30.807.14 61.843 4.165 90.822 12.21 69.26-46.944 99.663-37.189 99.663-37.189 19.792 49.946 7.34 86.831 3.61 96.01 23.25 25.359 37.29 57.742 37.29 97.347 0 139.366-84.82 170.033-165.637 179.013 13.026 11.2 24.628 33.342 24.628 67.182 0 48.498-.445 87.627-.445 99.521 0 9.702 6.535 20.988 24.945 17.444 144.03-48.067 247.881-183.95 247.881-344.175 0-200.378-162.442-362.798-362.802-362.798z" fill="#FFF"></path></svg></a></div></div> <hr> <!----> <div class="content__sidebar-top"></div> <nav class="sidebar-nav-links"><div class="nav-item"><a href="/vuepress/" class="nav-link router-link-active"><i class="iconfont icon-home"></i>
  Home
</a></div><div class="nav-item"><a href="/vuepress/home/" class="nav-link"><i class="iconfont icon-notice"></i>
  Target
</a></div><div class="nav-item"><a href="/vuepress/guide/" class="nav-link"><i class="iconfont icon-creative"></i>
  Guide
</a></div><div class="nav-item"><a href="https://jiangxj.top/blog//" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont icon-blog"></i>
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a rel="noopener noreferrer" href="https://github.com/jiangxjun/vuepress" target="_blank" class="repo-link">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav> <!----> <div class="content__sidebar-center"></div> <!----> <!----> <div class="content__sidebar-bottom"></div> <!----></aside> <main class="page"><nav class="breadcrumb"><ol vocab="https://schema.org/" typeof="BreadcrumbList"><li property="itemListElement" typeof="ListItem"><a href="/vuepress/note/NLP/" property="item" typeof="WebPage" class="router-link-active"><!----> <span property="name">自然语言处理基础知识</span></a> <meta property="position" content="1"></li><li property="itemListElement" typeof="ListItem" class="is-active"><a href="/vuepress/note/NLP/bert/" aria-current="page" property="item" typeof="WebPage" class="router-link-exact-active router-link-active"><!----> <span property="name">BERT预训练模型</span></a> <meta property="position" content="2"></li></ol></nav> <!----> <div class="content__page-top"></div> <div vocab="https://schema.org/" typeof="Article" class="page-title"><h1><!----> <span property="headline">BERT预训练模型</span></h1> <div class="page-info"><!----> <span aria-label="作者🖊" data-balloon-pos="down" categoryPath="/category/$category/" tagPath="/tag/$tag/"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon author-icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z" fill="currentColor"></path></svg> <span property="author">XJ</span></span><span aria-label="访问量🔢" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="visitor-info"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon eye-icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 0 0-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z" fill="currentColor"></path></svg> <span id="/vuepress/note/NLP/bert/" data-flag-title="BERT预训练模型" class="leancloud_visitors waline-visitor-count"><span class="leancloud-visitors-count">...</span></span></span><span aria-label="写作日期📅" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="time-info"><svg viewBox="0 0 1030 1024" xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 0 1-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 0 1-33.473-33.473V143.657H180.6A134.314 134.314 0 0 0 46.66 277.595v535.756A134.314 134.314 0 0 0 180.6 947.289h669.74a134.36 134.36 0 0 0 133.94-133.938V277.595a134.314 134.314 0 0 0-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 0 1-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 0 1-33.472 33.473z" fill="currentColor"></path></svg> <span property="datePublished">2022-11-15 </span></span><span role="navigation" aria-label="分类🌈" data-balloon-pos="down" defaultAuthor="#XJ" tagPath="/tag/$tag/" class="category-info enable"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon category-icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zm-.854 446.486H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zm446.371-446.486h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zm136.293 813.51H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z" fill="currentColor"></path></svg> <span property="articleSection">自然语言处理</span></span><span aria-label="标签🏷" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon tag-icon"><path d="M939.902 458.563 910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 0 0 0 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z" fill="currentColor"></path></svg> <ul class="tags-wrapper"><li class="tag clickable tag0"><span role="navigation">概念理论</span></li></ul> <meta property="keywords" content="概念理论"></span><span aria-label="阅读时间⌛" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="reading-time-info"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon timer-icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z" fill="currentColor"></path></svg> <span>大约 11 分钟</span> <meta property="timeRequired" content="PT11M"></span></div> <!----> <hr></div> <div class="anchor-place-holder"><aside id="anchor"><div class="anchor-wrapper"><ul class="anchor-list"><li class="anchor"><a href="/vuepress/note/NLP/bert/#introduction" class="anchor-link heading2"><div>Introduction</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#language-model" class="anchor-link heading2"><div>Language Model</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#bert-architecture" class="anchor-link heading2"><div>BERT Architecture</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#bert-input-representation" class="anchor-link heading2"><div>BERT Input Representation</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#pre-training-tasks" class="anchor-link heading2"><div>Pre-training Tasks</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#masked-lm任务" class="anchor-link heading3"><div>Masked LM任务</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#nsp任务" class="anchor-link heading3"><div>NSP任务</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#pre-training-procedure" class="anchor-link heading2"><div>Pre-training Procedure</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#fine-tuning-procedure" class="anchor-link heading2"><div>Fine-tuning Procedure</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#bert应用到具体nlp任务" class="anchor-link heading2"><div>BERT应用到具体NLP任务</div></a></li><li class="anchor"><a href="/vuepress/note/NLP/bert/#reference" class="anchor-link heading2"><div>Reference</div></a></li></ul></div></aside></div> <!----> <div class="content__content-top"></div> <div class="theme-default-content content__default"><h2 id="introduction"><a href="#introduction" class="header-anchor">#</a> Introduction</h2> <p>BERT（Bidirectional Encoder Representations from Transformers）是一个新的语言表达模型，设计被用于预训练的未标记文本深度双向表示，作用于所有层的左右上下文。预训练的BERT只需要一个额外的输出层就可以实现模型微调，针对问答（question answering）和语言推理（language inference）等问题，不需要进行大量实质性的模型修改。</p> <h2 id="language-model"><a href="#language-model" class="header-anchor">#</a> Language Model</h2> <p>语言模型预训练已经被证明可以有效地提高许多自然语言处理任务的性能。</p> <table><thead><tr><th>任务</th> <th>层次</th></tr></thead> <tbody><tr><td>token-level</td> <td>命名实体识别（NER）、SQuAD问答任务等</td></tr> <tr><td>sentence-level</td> <td>自然语言推断（NLI）、情感识别、句子语义等价判断等</td></tr></tbody></table> <p>针对自然语处理的下游任务（downstream-task），预训练的语言模型发挥重要作用。现有的语言模型（LM）应用到NLP任务主要有两种策略：feature-based和fine-tuning<sup class="footnote-ref"><a href="#footnote1">[1]</a><a id="footnote-ref1" class="footnote-anchor"></a></sup>。</p> <table><thead><tr><th>feature-based</th> <th>fine-tuning</th></tr></thead> <tbody><tr><td>ELMo (<em>Peters et al., 2018</em>)</td> <td>OpenAI GPT (<em>Radford et al., 2018</em>)</td></tr></tbody></table> <p>BERT是基于fine-tuning预训练方法的改进，提出了Masked Language Model（<strong>MLM</strong>）实现深度双向表示（deep bidirectional representation）的预训练；此外，引入了Next Sentence Prediction（<strong>NSP</strong>）任务，目的是让模型能够更好地理解句子间的关系。</p> <h2 id="bert-architecture"><a href="#bert-architecture" class="header-anchor">#</a> BERT Architecture</h2> <center><img src="/vuepress/assets/img/architecture.04aa3eff.png" title="BERT结构" width="80%"></center> <p>BERT的结构是一个多层的双向的Transformer Encoder，模型要比Transformer深。</p> <table><thead><tr><th><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm BERT}_{\rm BASE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathrm">BERT</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">BASE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></th> <th><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mi mathvariant="normal">L</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">G</mi><mi mathvariant="normal">E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm BERT}_{\rm LARGE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathrm">BERT</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">LARGE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></th> <th>Transformer</th></tr></thead> <tbody><tr><td>层数：Transformer blocks (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>)=12</td> <td>Transformer blocks (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>)=24</td> <td>Encode block = 6</td></tr> <tr><td>Hidden size (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>) = 768</td> <td>Hidden size (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>) = 1024</td> <td></td></tr> <tr><td>feed-forward/filter size = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">4\times H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> =3072</td> <td>feed-forward/filter size = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">4\times H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span> =4096</td> <td></td></tr> <tr><td>自注意力头数：self-attention heads (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>) =12</td> <td>self-attention heads (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>) =16</td> <td></td></tr> <tr><td>Total parameters = 110M</td> <td>Total parameters = 340M</td> <td></td></tr></tbody></table> <p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">T</mi></mrow><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm BERT}_{\rm BASE}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathrm">BERT</span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">BASE</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>模型的尺寸和OpenAI GPT保持一致便于性能对比，唯一差别是BERT Transfomer使用的是bidirectional self-attention （也被称为<strong>Transformer Encoder</strong>）。</p> <table><thead><tr><th>模型</th> <th>左右上下文语义</th> <th>是否并行</th></tr></thead> <tbody><tr><td>Word2Vec</td> <td>True</td> <td>True</td></tr> <tr><td>ELMo</td> <td>True (<strong>shallow bidirectional</strong>)</td> <td>False</td></tr> <tr><td>OpenAI GPT</td> <td>False</td> <td>True</td></tr> <tr><td>BERT</td> <td>True (<strong>deep bidirectional</strong>)</td> <td>True</td></tr></tbody></table> <p><strong>OpenAI GPT</strong>使用的是constrained self-attention，每一个token只关注左侧的上下文信息，left-context-only版本的self-attention也被称为<strong>Transformer Decoder</strong>。</p> <p>上图中的<strong>ELMo模型</strong>使用的是<em>concatenation of independency trained left-to-right and right-to-left LSTM</em>来生成用于下游任务的特征，称为<strong>浅层双向模型</strong>（shallow bidirectional model）。</p> <p><strong>Word2Vec的CBOW</strong>模型是通过token的上文信息和下文信息来预测该token，使用的是词袋模型，不知道单词的顺序信息，相当于直接把该token给mask了。</p> <p><strong>BERT</strong>可以将每一个token的左边上下文信息和右边的上下文信息结合起来，这样可以更好地根据全文理解每一个token的意思，训练一个<strong>深度双向模型</strong>（deep bidirectional model）</p> <h2 id="bert-input-representation"><a href="#bert-input-representation" class="header-anchor">#</a> BERT Input Representation</h2> <center><img src="/vuepress/assets/img/input.f3b8aaa6.png" title="BERT的输入表示" width="60%"></center> <p>BERT支持一个句子对输入（句子A和句子B）或单个句子的输入，BERT的输入会在分词的句子里面增加有特殊作用的标志位：</p> <ul><li><p><code>[CLS]</code>放在句首，classification，用于下游的分类任务。该符号对应的输出向量作为整个输入文本的语义表示，用于文本分类。</p> <ul><li><strong>单句子任务</strong>：如情感分析，<code>[CLS]</code>符号对于的输出向量作为整个输入文本的语义表示，用于文本分类。</li> <li><strong>语句对分类任务</strong>：包括问答（判断问题和答案是否匹配）、语句匹配（两句话是否表达同一个意思）等，使用<code>[CLS]</code>作为文本的语义表示，同时也利用<code>[SEP]</code>作为分割标识符，分别对两句话附加两个不同的文本向量作为区分。</li></ul></li> <li><p><code>[SEP]</code>用于分开两个输入的句子（A和B），分别在句子A和B的最后增加该标志</p></li> <li><p><code>[UNK]</code>表示未知字符</p></li> <li><p><code>[MASK]</code>用于遮盖句子中的一些单词，将单词用MASK遮盖后，根据上下文语境，利用BERT输出的[MASK]向量预测单词是什么。</p></li></ul> <p>BERT得到要输入的句子进行分词<strong>tokenizer</strong>得到token，然后将句子的token转成<strong>Embedding</strong>，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi></mrow><annotation encoding="application/x-tex">E</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span></span></span></span>表示。</p> <div class="custom-block tip"><p class="custom-block-title">提示</p> <p>BERT使用一个词表文件<code>vocab.txt</code>作为对输入进行分词的一个依据，对输入文本进行<code>split</code>操作后得到token。BERT包含三个tokenizer方式：</p> <ul><li><p><strong>BasicsTokenizer</strong>：清理特殊字符，包括控制符以及替换空白字符为空格。</p></li> <li><p><strong>WordpieceTokenier</strong>：对输入进行<code>split</code>，得到token的列表。如果token的一部分仍然在<code>vocab.txt</code>中，则继续分词，分割后除去第一部分外，其他部分都会加上<code>##</code>，得到WordPiece。如<code>playing</code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">→</span></span></span></span><code>play</code><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">+</span></span></span></span><code>##ing</code>。</p></li> <li><p><strong>FullTokenier</strong>：调用BasicTokenier得到初步的token列表，对于token再调用WordpieceTokenizer，得到更细分的token。</p></li></ul></div> <p>BERT的输入Embedding包括三个部分构成：Token Embedding，Segment Embedding，Position Embedding。</p> <ul><li><p><strong>Token Embedding</strong>：通过训练学习得到，如上图中的<code>[CLS]</code>,<code>dog</code>等。</p></li> <li><p><strong>Segment Embedding</strong>：用于区分每一个单词属于句子A或B，如果只有一个句子，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>E</mi><mi>A</mi></msub></mrow><annotation encoding="application/x-tex">E_A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>表示，通过学习得到</p></li> <li><p><strong>Position Embedding</strong>：编码单词出现的位置，通过学习得到。在BERT中，默认的句子最大长度为<strong>512</strong>。</p></li></ul> <h2 id="pre-training-tasks"><a href="#pre-training-tasks" class="header-anchor">#</a> Pre-training Tasks</h2> <p>BERT预训练模型包含两个任务：<strong>Masked LM</strong>和<strong>NSP</strong>。</p> <h3 id="masked-lm任务"><a href="#masked-lm任务" class="header-anchor">#</a> Masked LM任务</h3> <p>在句子中随机用<code>[MASK]</code>替换一部分单词，然后将句子传入BERT模型编码每一个token的信息，最终用<code>[MASK]</code>的编码信息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">T</mi><mrow><mi mathvariant="normal">M</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">K</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm T_{MASK}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathrm">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">MASK</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span>预测该位置正确的token。</p> <center><img src="/vuepress/assets/img/pre-training.c561818e.png" title="BERT预训练过程" width="70%"></center> <p>图源<sup class="footnote-ref"><a href="#footnote2">[2]</a><a id="footnote-ref2" class="footnote-anchor"></a></sup>。</p> <p>在原文<sup class="footnote-ref"><a href="#footnote1">[1:1]</a><a id="footnote-ref1:1" class="footnote-anchor"></a></sup>中，随机mask了每一个sentence的全部WordPiece token的15%。</p> <p>为了减缓<strong>错配</strong>的发生，并不是所有的待遮盖的token都会用<code>[MASK]</code>符号代替，在上述任意选择的15%待遮盖的token中，再任意选择其中的80%用<code>[MASK]</code>代替，10%用任意的其他随机token取代（i.e., 被随机取代的token占总的token的1.5%），剩余的10%保持不变。这样Transformer Encoder就无法知道哪一个token是要被预测，哪一个token被其他任意单词取代。</p> <p>每一个batch的15%的token会被预测，这样会导致MLM需要<strong>更多的epoch才能达到收敛</strong>。文章也说明了花费相对较长一些的computational cost获得更高的预测准确率是值得的。</p> <div class="custom-block tip"><p class="custom-block-title">提示</p> <p>在语言模型（LM）中，常需要用上一个词预测下一个词，BERT中融合了左右两侧上下文信息实现预训练深度双向Transformer，要使得在<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻看不到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻之后的信息，需要用<code>[MASK]</code>来“遮盖”。</p> <center><img src="/vuepress/assets/img/MLM.a5d3681b.png" title="BERT的Masked LM" width="50%"></center> <p>图源<sup class="footnote-ref"><a href="#footnote2">[2:1]</a><a id="footnote-ref2:1" class="footnote-anchor"></a></sup>。</p></div> <h3 id="nsp任务"><a href="#nsp任务" class="header-anchor">#</a> NSP任务</h3> <p>下一句预测是将句子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>输入BERT，预测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>是否是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>的下一句，使用<code>[CLS]</code>的编码信息<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>进行预测。</p> <p>NSP应用的下游任务场景包括：Question Answering (QA)，Natural Language Inference (NLI)等，主要解决的是理解<strong>两个文本句子</strong>之间的关系。</p> <p>原文<sup class="footnote-ref"><a href="#footnote1">[1:2]</a><a id="footnote-ref1:2" class="footnote-anchor"></a></sup>中的预训练样本，有50%的sentence pair是连续相连的，另外50%的是从语料库中随机选择不连续的。然后通过<code>[CLS]</code>标志位的输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span>预测句子<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">A</span></span></span></span>的下一句<code>IsNext</code> or <code>NotNext</code>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span>。</p> <h2 id="pre-training-procedure"><a href="#pre-training-procedure" class="header-anchor">#</a> Pre-training Procedure</h2> <table><thead><tr><th>Hyperparameter</th> <th>value</th></tr></thead> <tbody><tr><td>batch size</td> <td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">256</span></span></span></span> sequences=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn><mo>×</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">256 \times 512</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">256</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span> tokens=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn><mo separator="true">,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">128,000</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">128</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span></span></span></span> tokens/batch</td></tr> <tr><td>epoch</td> <td>40 epoches for 1M steps</td></tr> <tr><td>word corpus size</td> <td>3.3 billion</td></tr> <tr><td>Optimizer</td> <td>Adam</td></tr> <tr><td>learning rate</td> <td>warmup over the first 10000 steps at <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>e</mi><mo>−</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">1e-4</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">1</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">4</span></span></span></span>  and linear decay after 10000 steps</td></tr> <tr><td>Dropout</td> <td>0.1 on all layers</td></tr> <tr><td>activation function</td> <td>gelu</td></tr> <tr><td>training loss</td> <td>MLM似然值的均值和NSP似然值的均值的和</td></tr></tbody></table> <p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><msub><mi mathvariant="normal">T</mi><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm BERT_{BASE}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">BER</span><span class="mord"><span class="mord mathrm">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">BASE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span>用到了16个TPU（张量处理器），<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">B</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">R</mi><msub><mi mathvariant="normal">T</mi><mrow><mi mathvariant="normal">L</mi><mi mathvariant="normal">A</mi><mi mathvariant="normal">R</mi><mi mathvariant="normal">G</mi><mi mathvariant="normal">E</mi></mrow></msub></mrow><annotation encoding="application/x-tex">{\rm BERT_{LARGE}}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">BER</span><span class="mord"><span class="mord mathrm">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">LARGE</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span>用到了64个TPU，每一次训练时间4 days。</p> <h2 id="fine-tuning-procedure"><a href="#fine-tuning-procedure" class="header-anchor">#</a> Fine-tuning Procedure</h2> <p>对于序列级（sequence-level）的分类任务，BERT将最终的隐藏层输出（i.e., Transformer的输出）给第一个token（i.e., <code>[CLS]</code>）的word embedding。<code>[CLS]</code>标志位的输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">C \in \mathbb{R}^H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span></span></span></span></span></span></span>，利用softmax输出标签概率<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">(</mo><mi>C</mi><msup><mi>W</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P = {\rm softmax}(CW^T)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0913em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">softmax</span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>K</mi><mo>×</mo><mi>H</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W \in \mathbb{R}^{K \times H}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span></span></span></span></span></span></span></span>是唯一个在fine-tuning阶段分类层引入的参数，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>是分类器数量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span></span></span></span>是Transformer Encoder的隐含层大小。</p> <h2 id="bert应用到具体nlp任务"><a href="#bert应用到具体nlp任务" class="header-anchor">#</a> BERT应用到具体NLP任务</h2> <center><img src="/vuepress/assets/img/tasks.e3d7030d.png" title="BERT应用在具体的NLP任务" width="60%"></center> <p>(a)<strong>一对句子的分类任务</strong>：多中心的自然语言推断（Multi-centre Natural Language Inference，MNLI）是一个众包蕴含（crowdsource entailment）的分类任务，句子语义等价判断（Quara Question Pairs, QQP）是一个二值分类任务。将两个句子传入BERT，通过<code>[CLS]</code>标志位的输出值（来自Transformer Encoder的输出值）进行句子分类。</p> <p>(b)<strong>单个句子分类任务</strong>：如句子情感分析（Stanford Sentiment Treebank, SST-2），判断句子语法是否可以接受（The Corpus of Linguistic Acceptability, CoLA）等。输入一个句子，通过<code>[CLS]</code>标志位的输出值进行分类。</p> <p>(c)<strong>问答任务</strong>：样本是一个语句对（Question，Paragraph），训练的目标是在Paragraph找出答案的起始位置（Start，End）。</p> <p>(d)<strong>单个句子标注</strong>：如命名实体识别（NER），输入单个句子，然后根据BERT对于每个token的输出<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi></mrow><annotation encoding="application/x-tex">T</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span></span></span></span>预测该token的类别，是属于Person，Organization，Location，Miscellaneous 还是 Other (非命名实体)。</p> <h1 id="questions"><a href="#questions" class="header-anchor">#</a> Questions</h1> <p>1、BERT的三个Embedding为什么可以进行相加？</p> <blockquote><p>一串文本可以看作是一些不同频率的时序信号进行叠加，其中每个词的特征可以用叠加波来表示，整个文本序列也可以进一步叠加。哪些是低频信号（如词性？），哪些是高频信号（如语义？），这些都隐藏在Embedding中，也可能已经解耦在不同的维度中。</p> <p>本质上三个Embedding相加，可以看做是一个特征的融合，BERT可以学习到融合特征的语义信息。</p> <p>Answer From：https://www.zhihu.com/question/374835153——邱锡鹏老师的回答</p></blockquote> <p>2、使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？</p> <blockquote><p>在BERT中，Token，Position，Segement Embedding都是<strong>通过学习得到</strong>的，Pytorch代码中：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>self<span class="token punctuation">.</span>word_embeddings <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>position_embeddings <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>max_position_embeddings<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>token_type_embeddings <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>config<span class="token punctuation">.</span>type_vocab_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>Pytorch版的BERT代码地址：https://github.com/xieyufei1993/Bert-Pytorch-Chinese-TextClassification</p> <p>在BERT config中：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token string">&quot;max_position_embeddings&quot;</span><span class="token punctuation">:</span> <span class="token number">512</span>
<span class="token string">&quot;type_vocab_size&quot;</span><span class="token punctuation">:</span> <span class="token number">2</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>因此，在直接使用Google 的BERT预训练模型时，输入最多512个词（还要除掉<code>[CLS]</code>和<code>[SEP]</code>，即实际510个词），最多两个句子合成一句。这之外的词和句子会没有对应的embedding。</p> <p>BERT本身的训练对于显存有很高的要求，参数量特别大。如果有足够的硬件资源自己重新训练BERT，可以更改 BERT config，设置更大max_position_embeddings 和 type_vocab_size值去满足自己的需求。</p> <p>Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点</p></blockquote> <p>3、BERT的第一句前加一个<code>[CLS]</code>标志位作用？</p> <blockquote><p>在Transformer Encoder的最后一层该位的输出向量可以作为整个语句的语义表示，从而用于下游的分类任务。与文中已有的其他词相比，这一个无明显语义的信息符号<strong>更能“公平”地融合</strong>文本中各个词的语义信息。</p> <p>Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点</p></blockquote> <p>4、BERT的非线性来源？</p> <blockquote><p>前馈层的gelu激活函数和self-attention，self-attention是非线性的。</p> <p>Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点</p></blockquote> <p>5、BERT中，长文本问题解决？</p> <blockquote><p>BERT在进行MLM预训练时，规定了最大的输入长度不超过512个token。对于超过512个token的长文本，无法直接调用开源的pretrained-model进行fine-tuning。在比如阅读理解问题中，输入语句通常远大于512。</p> <p>(1)<strong>Clipping（截断法）</strong>：挑选其中重要的token输入模型。挑选前TOP N个token和文末TOP K个token，保证<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>+</mo><mi>K</mi><mo>≤</mo><mn>510</mn></mrow><annotation encoding="application/x-tex">N+K \le 510</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8193em;vertical-align:-0.136em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">510</span></span></span></span>；会丢失信息。</p> <p>(2)<strong>RNN循环法</strong></p> <p>(3)<strong>Pooling（池化法）</strong>：借鉴CNN中池化层作用是选取一个区域内最重要的特征值代替整个feature map的特征值（包括最大池化和平均池化），来实现降维功能，整个过程也被称为下采样（down samping）。同理，对于一个1000token的输入文本，得到1000个Embedding，通过Pooling方式获取其中最重要的510个Embedding（另两个是<code>[CLS]</code>,<code>[SEP]</code>）代表整段文本的Embedding。</p> <p>Answer From：<a href="https://zhuanlan.zhihu.com/p/504204038#:~:text=%E7%94%B1%E4%BA%8E%20BERT%20%E6%9C%80%E5%A4%9A%E5%8F%AA%E8%83%BD%E6%8E%A5%E5%8F%97%20510%20%E4%B8%AAtoken%20%E7%9A%84%E8%BE%93%E5%85%A5%EF%BC%8C%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%B0%86%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%87%E5%89%B2%E6%88%90%E8%8B%A5%E5%B9%B2%E6%AE%B5%E3%80%82%20%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E6%9C%89%202,%2B%20%E7%AC%AC%202%20%E4%B8%AA%E5%8F%A5%E5%AD%90%E7%9A%84%202%20%E6%AE%B5%EF%BC%89%EF%BC%8C%E5%B9%B6%E6%94%BE%E5%88%B0%E4%B8%80%E4%B8%AA%20batch%20%E7%9A%84%E8%BE%93%E5%85%A5%E4%B8%AD%E5%96%82%E7%BB%99%E6%A8%A1%E5%9E%8B%E3%80%82" target="_blank" rel="noopener noreferrer">一般有如下几种方法——此文主要介绍Pooling<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <h2 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h2> <hr class="footnotes-sep"> <section class="footnotes"><ol class="footnotes-list"><li id="footnote1" class="footnote-item"><p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., NAACL 2019) <a href="#footnote-ref1" class="footnote-backref">↩︎</a> <a href="#footnote-ref1:1" class="footnote-backref">↩︎</a> <a href="#footnote-ref1:2" class="footnote-backref">↩︎</a></p></li> <li id="footnote2" class="footnote-item"><p>理解BERT模型：https://www.jianshu.com/p/46cb208d45c3 <a href="#footnote-ref2" class="footnote-backref">↩︎</a> <a href="#footnote-ref2:1" class="footnote-backref">↩︎</a></p></li></ol></section></div> <!----> <div class="content__content-bottom"></div> <footer class="page-meta"><!----> <!----> <!----></footer> <!----> <!----> <!----> <div class="content__page-bottom"></div></main> <footer class="footer-wrapper"><div class="media-links-wrapper"><a href="https://zhihu.com" rel="noopener noreferrer" target="_blank" aria-label="Zhihu" data-balloon-pos="up" class="media-link"><span class="sr-only">Zhihu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-zhihu"><circle cx="512" cy="512" r="512" fill="#006CE2"></circle> <path d="M513.65 491.261H411.551c1.615-16.154 5.815-60.095 5.815-84.973 0-24.88-.323-60.742-.323-60.742h102.744V329.39c0-21.647-9.37-31.34-17.124-31.34h-178.67s5.169-17.77 10.015-36.186c4.846-18.417 15.832-44.264 15.832-44.264-63.003 4.2-67.958 50.941-81.743 92.729-13.787 41.785-24.556 62.356-44.586 107.912 27.786 0 55.249-13.57 66.879-32.309 11.631-18.74 16.908-40.71 16.908-40.71h62.035v59.019c0 21.107-3.878 87.45-3.878 87.45H254.742c-19.386 0-29.724 48.894-29.724 48.894h133.76c-8.4 75.82-26.493 106.191-51.91 152.716-25.418 46.525-92.728 99.406-92.728 99.406 41.033 11.63 86.589-3.555 105.974-21.972 19.386-18.417 35.863-49.756 47.817-72.838 11.954-23.081 21.972-65.124 21.972-65.124L498.462 766.86s4.846-24.233 6.461-39.418c1.616-15.186-.755-26.385-4.63-35.433-3.878-9.046-15.509-21.54-31.018-39.634-15.507-18.094-48.034-52.879-48.034-52.879s-15.832 11.63-28.108 21.001c9.046-21.97 16.262-79.695 16.262-79.695h122.343v-20.249c.003-17.66-7.319-29.29-18.089-29.29zm287.337-200.747h-234.35a4.308 4.308 0 0 0-4.309 4.308v435.099a4.308 4.308 0 0 0 4.308 4.308h40.226l14.7 50.402 81.096-50.402h98.328a4.308 4.308 0 0 0 4.308-4.308v-435.1a4.308 4.308 0 0 0-4.308-4.308zM755.97 684.47h-52.343l-61.548 39.095-10.823-39.095h-18.738V338.116H755.97v346.355z" fill="#FFF"></path></svg></a><a href="https://baidu.com" rel="noopener noreferrer" target="_blank" aria-label="Baidu" data-balloon-pos="up" class="media-link"><span class="sr-only">Baidu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-baidu"><circle cx="512" cy="512" r="512" fill="#1D2FE3"></circle> <path d="M239.022 704.978c.098-4.865-.314-9.772.162-14.591 5.178-52.464 197.571-253.377 249.641-259.233 42.996-4.833 75.768 16.545 99.824 49.144 37.893 51.351 82.81 95.455 131.292 136.237 52.903 44.503 56.525 99.801 32.6 158.592-23.425 57.56-75.34 69.833-127.771 58.804-84.971-17.874-168.158-13.744-253.37-4.536-86.35 9.333-133.788-39.4-132.378-124.417zM352.464 412.86c-3.58 50.707-17.93 96.128-75.9 98.12-58.053 1.995-80.093-41.432-79.275-91.71.81-49.705 13.416-104.053 76.851-102.136 53.84 1.625 74.74 45.8 78.324 95.726zm386.053 142.168c-68.494-1.735-84.188-43.331-82.635-93.812 1.46-47.519 10.082-97.628 73.299-96.65 61.395.95 81.6 43.207 81.553 98.668-.047 53.156-19.818 89.398-72.217 91.794zm-45.235-278.345c-10.464 42.665-24.513 91.761-85.919 94.502-52.74 2.354-71.705-34.482-72.805-81.242-1.233-52.42 48.08-112.965 87.582-110.373 33.943 2.226 71.146 49.541 71.142 97.113zm-195.147-14.097c-7.005 46.274-13.63 100.025-71.562 101.351-57.077 1.306-73.567-47.922-73.638-97.109-.068-48.054 12.128-99.024 69.345-101.426 59.45-2.493 67.11 51.093 75.855 97.184z" fill="#fff"></path> <path d="M479.52 663.165c.006 12.194 1.498 24.61-.284 36.537-4.707 31.503 18.862 78.749-45.326 77.534-54.226-1.027-103.338-3.31-113.231-73.536-7.164-50.852 7.78-85.674 57.687-102.668 17.67-6.016 39.618 5.058 54.096-14.548 10.84-14.679-2.901-54.592 33.418-41.47 24.075 8.7 11.477 38.922 13.278 59.652 1.68 19.366.359 38.99.363 58.5zm175.45 41.902c4.291 39.657 5.093 78.047-64.709 73.503-60.097-3.912-95.56-20.794-86.293-85.624 4.287-29.991-21.148-83.238 22.19-84.867 42.71-1.606 13.57 50.41 20.825 77.622 5.276 19.794-3.984 46.774 29.753 48.193 41.337 1.738 28.383-30.022 31.099-51.604 1.209-9.61-.85-19.65.528-29.215 2.516-17.474-8.928-44.716 19.554-47.191 36.044-3.133 24.155 28.376 26.678 47.523 1.896 14.387.375 29.225.375 51.66z" fill="#1D2FE3"></path> <path d="M435.669 685.038c-2.255 24.07 5.605 53.68-33.623 52.136-34.594-1.362-35.274-31.818-38.513-53.078-4.028-26.448 11.38-48.18 40.785-50.023 40.967-2.564 27.097 30.764 31.35 50.965z" fill="#fff"></path></svg></a><a href="https://github.com" rel="noopener noreferrer" target="_blank" aria-label="Github" data-balloon-pos="up" class="media-link"><span class="sr-only">Github</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-github"><circle cx="512" cy="512" r="512" fill="#171515"></circle> <path d="M509.423 146.442c-200.317 0-362.756 162.42-362.756 362.8 0 160.266 103.936 296.24 248.109 344.217 18.139 3.327 24.76-7.872 24.76-17.486 0-8.613-.313-31.427-.49-61.702-100.912 21.923-122.205-48.63-122.205-48.63-16.495-41.91-40.28-53.067-40.28-53.067-32.937-22.51 2.492-22.053 2.492-22.053 36.407 2.566 55.568 37.386 55.568 37.386 32.362 55.438 84.907 39.43 105.58 30.143 3.296-23.444 12.667-39.43 23.032-48.498-80.557-9.156-165.246-40.28-165.246-179.297 0-39.604 14.135-71.988 37.342-97.348-3.731-9.178-16.18-46.063 3.556-96.009 0 0 30.46-9.754 99.76 37.19 28.937-8.048 59.97-12.071 90.823-12.211 30.807.14 61.843 4.165 90.822 12.21 69.26-46.944 99.663-37.189 99.663-37.189 19.792 49.946 7.34 86.831 3.61 96.01 23.25 25.359 37.29 57.742 37.29 97.347 0 139.366-84.82 170.033-165.637 179.013 13.026 11.2 24.628 33.342 24.628 67.182 0 48.498-.445 87.627-.445 99.521 0 9.702 6.535 20.988 24.945 17.444 144.03-48.067 247.881-183.95 247.881-344.175 0-200.378-162.442-362.798-362.802-362.798z" fill="#FFF"></path></svg></a></div> <div class="footer">页面已经到底啦👆</div> <div class="copyright">Copyright © 2024 #XJ</div></footer></div><div class="global-ui"><!----><!----><div id="pwa-install"><!----> <div id="install-modal-wrapper" style="display:none;"><div class="background"></div> <div class="install-modal"><div class="header"><button aria-label="关闭" class="close-button"><svg width="23" height="22" xmlns="http://www.w3.org/2000/svg" class="icon close-icon"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.12.358a1.224 1.224 0 011.729 0l8.92 8.914L20.686.358a1.224 1.224 0 011.73 1.728L13.497 11l8.92 8.913a1.222 1.222 0 11-1.73 1.729l-8.919-8.913-8.92 8.913a1.224 1.224 0 01-1.729-1.729L10.04 11l-8.92-8.914a1.222 1.222 0 010-1.728z" fill="currentColor"></path></svg></button> <div class="logo"><!----> <div class="title"><h1></h1> <p class="desc">该应用可以安装在您的 PC 或移动设备上。这将使该 Web 应用程序外观和行为与其他应用程序相同。它将在出现在应用程序列表中，并可以固定到主屏幕，开始菜单或任务栏。此 Web 应用程序还将能够与其他应用程序和您的操作系统安全地进行交互。</p></div></div></div> <div class="content"><div class="highlight"><!----> <!----></div> <div class="description"><h3>详情</h3> <p></p></div></div> <div class="button-wrapper"><button class="install-button">
        安装 <span></span></button> <button class="cancel-button">
        取消
      </button></div></div></div></div><div tabindex="-1" role="dialog" aria-hidden="true" class="pswp"><div class="pswp__bg"></div> <div class="pswp__scroll-wrap"><div class="pswp__container"><div class="pswp__item"></div> <div class="pswp__item"></div> <div class="pswp__item"></div></div> <div class="pswp__ui pswp__ui--hidden"><div class="pswp__top-bar"><div class="pswp__counter"></div> <button title="关闭" aria-label="关闭" class="pswp__button pswp__button--close"></button> <button title="分享" aria-label="分享" class="pswp__button pswp__button--share"></button> <button title="切换全屏" aria-label="切换全屏" class="pswp__button pswp__button--fs"></button> <button title="缩放" aria-label="缩放" class="pswp__button pswp__button--zoom"></button> <div class="pswp__preloader"><div class="pswp__preloader__icn"><div class="pswp__preloader__cut"><div class="pswp__preloader__donut"></div></div></div></div></div> <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class="pswp__share-tooltip"></div></div> <button title="上一个 (左箭头)" aria-label="上一个 (左箭头)" class="pswp__button pswp__button--arrow--left"></button> <button title="下一个 (右箭头)" aria-label="下一个 (右箭头)" class="pswp__button pswp__button--arrow--right"></button> <div class="pswp__caption"><div class="pswp__caption__center"></div></div></div></div></div></div></div>
    <script src="/vuepress/assets/js/app.63a6b71e.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Layout.714049f4.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Blog~layout-Layout~layout-NotFound.f5405c06.js" defer></script><script src="/vuepress/assets/js/page-BERT预训练模型.2ee39f07.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Blog~layout-Layout.407483c8.js" defer></script>
  </body>
</html>
