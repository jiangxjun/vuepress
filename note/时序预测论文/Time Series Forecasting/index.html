<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>基于Transformers模型的时序预测（TSF）方法 | Docs Library by #XJ</title>
    <meta name="generator" content="VuePress 1.9.7">
    <script src="https://cdn.jsdelivr.net/npm/react/umd/react.production.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/react-dom/umd/react-dom.production.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@babel/standalone/babel.min.js"></script>
    <link rel="alternate" type="application/atom+xml" href="https://jiangxj.top/vuepress/vuepress/atom.xml" title="Docs Library by #XJ Atom Feed">
    <link rel="alternate" type="application/json" href="https://jiangxj.top/vuepress/vuepress/feed.json" title="Docs Library by #XJ JSON Feed">
    <link rel="alternate" type="application/rss+xml" href="https://jiangxj.top/vuepress/vuepress/rss.xml" title="Docs Library by #XJ RSS Feed">
    <link rel="icon" href="/favicon.ico">
    <link rel="icon" href="/assets/icon/chrome-mask-512.png" type="image/png" sizes="512x512">
    <link rel="icon" href="/assets/icon/chrome-mask-192.png" type="image/png" sizes="192x192">
    <link rel="icon" href="/assets/icon/chrome-512.png" type="image/png" sizes="512x512">
    <link rel="icon" href="/assets/icon/chrome-192.png" type="image/png" sizes="192x192">
    <link rel="manifest" href="/vuepress/manifest.webmanifest" crossorigin="use-credentials">
    <link rel="apple-touch-icon" href="/assets/icon/apple-icon-152.png">
    <meta name="description" content="基于Transformer的时序预测（TSF）方法。">
    <meta property="og:url" content="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting.html">
    <meta property="og:site_name" content="Docs Library by #XJ">
    <meta property="og:title" content="基于Transformers模型的时序预测（TSF）方法">
    <meta property="og:description" content="基于Transformer的时序预测（TSF）方法。">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh-CN">
    <meta property="og:locale:alternate" content="en-US">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image:alt" content="Docs Library by #XJ">
    <meta property="article:author" content="XJ">
    <meta property="article:tag" content="时序预测">
    <meta property="article:published_time" content="2023-07-01T00:00:00.000Z">
    <meta name="theme-color" content="#46bd87">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="msapplication-TileImage" content="/assets/icon/ms-icon-144.png">
    <meta name="msapplication-TileColor" content="#ffffff">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover">
    
    <link rel="preload" href="/vuepress/assets/css/0.styles.79a9e6b0.css" as="style"><link rel="preload" href="/vuepress/assets/js/app.63a6b71e.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Layout.714049f4.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Blog~layout-Layout~layout-NotFound.f5405c06.js" as="script"><link rel="preload" href="/vuepress/assets/js/page-基于Transformers模型的时序预测（TSF）方法.1249ee6d.js" as="script"><link rel="preload" href="/vuepress/assets/js/vendors~layout-Blog~layout-Layout.407483c8.js" as="script"><link rel="prefetch" href="/vuepress/assets/js/62.ddea15ac.js"><link rel="prefetch" href="/vuepress/assets/js/63.2ab102a2.js"><link rel="prefetch" href="/vuepress/assets/js/64.005312f8.js"><link rel="prefetch" href="/vuepress/assets/js/65.b548f3c6.js"><link rel="prefetch" href="/vuepress/assets/js/66.549bc712.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Blog.38f6e5a9.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Layout.e3c1d55e.js"><link rel="prefetch" href="/vuepress/assets/js/layout-NotFound.1dafa333.js"><link rel="prefetch" href="/vuepress/assets/js/layout-Slide.22421d46.js"><link rel="prefetch" href="/vuepress/assets/js/page-BERT预训练模型.2ee39f07.js"><link rel="prefetch" href="/vuepress/assets/js/page-BlogHome.af52b4f6.js"><link rel="prefetch" href="/vuepress/assets/js/page-Challengesandapproachedtotime-seriesforecasting.2fcea3e6.js"><link rel="prefetch" href="/vuepress/assets/js/page-Componentdisabled.ffadc3d5.js"><link rel="prefetch" href="/vuepress/assets/js/page-CustomLayout.8fd0154f.js"><link rel="prefetch" href="/vuepress/assets/js/page-DLforLoadForecastingsequencetosequenceRNNswithAttention.a55d615e.js"><link rel="prefetch" href="/vuepress/assets/js/page-Deepleanringfortimeseriesforecastingasurvey.8d059b40.js"><link rel="prefetch" href="/vuepress/assets/js/page-Embedding和word2vec理解.11af4fcf.js"><link rel="prefetch" href="/vuepress/assets/js/page-Encryptionarticle.dc677e6d.js"><link rel="prefetch" href="/vuepress/assets/js/page-Guides.9fab118e.js"><link rel="prefetch" href="/vuepress/assets/js/page-HTML基础知识.f167246e.js"><link rel="prefetch" href="/vuepress/assets/js/page-HelloVuepress.76f46302.js"><link rel="prefetch" href="/vuepress/assets/js/page-Homepage.125f2ea2.js"><link rel="prefetch" href="/vuepress/assets/js/page-IntroPage.9076bdb7.js"><link rel="prefetch" href="/vuepress/assets/js/page-MarkdownEnhance.08702e54.js"><link rel="prefetch" href="/vuepress/assets/js/page-Markdown增强.6b228f3e.js"><link rel="prefetch" href="/vuepress/assets/js/page-Projecthome.64201229.js"><link rel="prefetch" href="/vuepress/assets/js/page-Slidepage.217de802.js"><link rel="prefetch" href="/vuepress/assets/js/page-Transformer-Mask机制理解.d3afe08a.js"><link rel="prefetch" href="/vuepress/assets/js/page-Ubuntu1604配置cuda和cudnn.33c321cd.js"><link rel="prefetch" href="/vuepress/assets/js/page-VuePress默认主题设置.85d22925.js"><link rel="prefetch" href="/vuepress/assets/js/page-Web前端平台搭建.488a35ec.js"><link rel="prefetch" href="/vuepress/assets/js/page-docker使用.8840900d.js"><link rel="prefetch" href="/vuepress/assets/js/page-不完备信息系统-粗糙集理论.abc3021f.js"><link rel="prefetch" href="/vuepress/assets/js/page-主成分分析（PCA）.5488bebd.js"><link rel="prefetch" href="/vuepress/assets/js/page-人工智能的数学基础知识.5d8fb9f0.js"><link rel="prefetch" href="/vuepress/assets/js/page-先进核能概论.8f1a8fee.js"><link rel="prefetch" href="/vuepress/assets/js/page-决策树.16d2f952.js"><link rel="prefetch" href="/vuepress/assets/js/page-分类与回归.2281637d.js"><link rel="prefetch" href="/vuepress/assets/js/page-卷积神经网络理解.2e46266a.js"><link rel="prefetch" href="/vuepress/assets/js/page-密码加密的文章.a9e4e164.js"><link rel="prefetch" href="/vuepress/assets/js/page-抽样分布.8bedfaef.js"><link rel="prefetch" href="/vuepress/assets/js/page-最小二乘法(LSM).03e08fe3.js"><link rel="prefetch" href="/vuepress/assets/js/page-机器学习的基本设计方法和途径.efd9d259.js"><link rel="prefetch" href="/vuepress/assets/js/page-机器学习算法理解.a013ce47.js"><link rel="prefetch" href="/vuepress/assets/js/page-概率论-贝叶斯公式.b5ab5633.js"><link rel="prefetch" href="/vuepress/assets/js/page-深度强化学习——基础、研究及应用.09723c50.js"><link rel="prefetch" href="/vuepress/assets/js/page-深度强化学习（DeepReinforcementLearning）基础1.e8459927.js"><link rel="prefetch" href="/vuepress/assets/js/page-生成模型.00d27e14.js"><link rel="prefetch" href="/vuepress/assets/js/page-第四代先进核能系统概述.90ddcff2.js"><link rel="prefetch" href="/vuepress/assets/js/page-线性代数-随机变量的数字特征.daada953.js"><link rel="prefetch" href="/vuepress/assets/js/page-组件禁用.19c22b31.js"><link rel="prefetch" href="/vuepress/assets/js/page-自然语言处理基础知识.b7a0d8d3.js"><link rel="prefetch" href="/vuepress/assets/js/page-计算机网络.49bbe557.js"><link rel="prefetch" href="/vuepress/assets/js/page-计算机网络概述.daae640f.js"><link rel="prefetch" href="/vuepress/assets/js/page-语言模型的简要梳理介绍.a45b4449.js"><link rel="prefetch" href="/vuepress/assets/js/page-这主题废弃了标题？.d62b6d66.js"><link rel="prefetch" href="/vuepress/assets/js/page-页面配置.b2b5779f.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~flowchart.7cb4e065.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~mermaid.b9369779.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~photo-swipe.77a3a8a3.js"><link rel="prefetch" href="/vuepress/assets/js/vendors~reveal.0c21a847.js">
    <link rel="stylesheet" href="/vuepress/assets/css/0.styles.79a9e6b0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container has-navbar has-anchor"><header class="navbar"><!----> <div class="content__navbar-start"></div> <button title="Sidebar Button" class="sidebar-button"><span class="icon"></span></button> <a href="/vuepress/" class="home-link router-link-active"><img src="/vuepress/logo.svg" alt="Docs Library by #XJ" class="logo"> <!----> <span class="site-name can-hide">Docs Library by #XJ</span></a> <!----> <div class="content__navbar-center"></div> <div class="links"><button tabindex="-1" aria-hidden="true" class="color-button"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="skin-icon"><path d="M224 800c0 9.6 3.2 44.8 6.4 54.4 6.4 48-48 76.8-48 76.8s80 41.6 147.2 0 134.4-134.4
        38.4-195.2c-22.4-12.8-41.6-19.2-57.6-19.2C259.2 716.8 227.2 761.6 224 800zM560 675.2l-32
        51.2c-51.2 51.2-83.2 32-83.2 32 25.6 67.2 0 112-12.8 128 25.6 6.4 51.2 9.6 80 9.6 54.4 0
        102.4-9.6 150.4-32l0 0c3.2 0 3.2-3.2 3.2-3.2 22.4-16 12.8-35.2
        6.4-44.8-9.6-12.8-12.8-25.6-12.8-41.6 0-54.4 60.8-99.2 137.6-99.2 6.4 0 12.8 0 22.4
        0 12.8 0 38.4 9.6 48-25.6 0-3.2 0-3.2 3.2-6.4 0-3.2 3.2-6.4 3.2-6.4 6.4-16 6.4-16 6.4-19.2
        9.6-35.2 16-73.6 16-115.2 0-105.6-41.6-198.4-108.8-268.8C704 396.8 560 675.2 560 675.2zM224
        419.2c0-28.8 22.4-51.2 51.2-51.2 28.8 0 51.2 22.4 51.2 51.2 0 28.8-22.4 51.2-51.2 51.2C246.4
        470.4 224 448 224 419.2zM320 284.8c0-22.4 19.2-41.6 41.6-41.6 22.4 0 41.6 19.2 41.6 41.6 0
        22.4-19.2 41.6-41.6 41.6C339.2 326.4 320 307.2 320 284.8zM457.6 208c0-12.8 12.8-25.6 25.6-25.6
        12.8 0 25.6 12.8 25.6 25.6 0 12.8-12.8 25.6-25.6 25.6C470.4 233.6 457.6 220.8 457.6 208zM128
        505.6C128 592 153.6 672 201.6 736c28.8-60.8 112-60.8 124.8-60.8-16-51.2 16-99.2
        16-99.2l316.8-422.4c-48-19.2-99.2-32-150.4-32C297.6 118.4 128 291.2 128 505.6zM764.8
        86.4c-22.4 19.2-390.4 518.4-390.4 518.4-22.4 28.8-12.8 76.8 22.4 99.2l9.6 6.4c35.2 22.4
        80 12.8 99.2-25.6 0 0 6.4-12.8 9.6-19.2 54.4-105.6 275.2-524.8 288-553.6
        6.4-19.2-3.2-32-19.2-32C777.6 76.8 771.2 80 764.8 86.4z"></path></svg> <div class="color-picker-menu" style="display:none;"><div class="theme-options"><ul class="themecolor-select"><label for="themecolor-select">主题色:</label> <li><span class="default-theme"></span></li> </ul> <div class="darkmode-toggle"><label for="darkmode-toggle" class="desc">主题模式:</label> <div class="darkmode-switch"><div class="item day"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon light-icon"><path d="M512 256a42.667 42.667 0 0 0 42.667-42.667V128a42.667 42.667 0 0 0-85.334 0v85.333A42.667 42.667 0 0 0 512 256zm384 213.333h-85.333a42.667 42.667 0 0 0 0 85.334H896a42.667 42.667 0 0 0 0-85.334zM256 512a42.667 42.667 0 0 0-42.667-42.667H128a42.667 42.667 0 0 0 0 85.334h85.333A42.667 42.667 0 0 0 256 512zm9.387-298.667a42.667 42.667 0 0 0-59.307 62.72l61.44 59.307a42.667 42.667 0 0 0 31.147 11.947 42.667 42.667 0 0 0 30.72-13.227 42.667 42.667 0 0 0 0-60.16zm459.946 133.974a42.667 42.667 0 0 0 29.44-11.947l61.44-59.307a42.667 42.667 0 0 0-57.6-62.72l-61.44 60.587a42.667 42.667 0 0 0 0 60.16 42.667 42.667 0 0 0 28.16 13.227zM512 768a42.667 42.667 0 0 0-42.667 42.667V896a42.667 42.667 0 0 0 85.334 0v-85.333A42.667 42.667 0 0 0 512 768zm244.48-79.36a42.667 42.667 0 0 0-59.307 61.44l61.44 60.587a42.667 42.667 0 0 0 29.44 11.946 42.667 42.667 0 0 0 30.72-12.8 42.667 42.667 0 0 0 0-60.586zm-488.96 0-61.44 59.307a42.667 42.667 0 0 0 0 60.586 42.667 42.667 0 0 0 30.72 12.8 42.667 42.667 0 0 0 28.587-10.666l61.44-59.307a42.667 42.667 0 0 0-59.307-61.44zM512 341.333A170.667 170.667 0 1 0 682.667 512 170.667 170.667 0 0 0 512 341.333z" fill="currentColor"></path></svg></div> <div class="item auto active"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon auto-icon"><path d="M460.864 539.072H564.8L510.592 376l-49.728 163.072zM872 362.368V149.504H659.648L510.528 0l-149.12 149.504H149.12v212.928L0 511.872l149.12 149.504v212.928h212.352l149.12 149.504 149.12-149.504h212.352V661.376l149.12-149.504L872 362.368zM614.464 693.12l-31.616-90.624H438.272l-31.616 90.624h-85.888l144.576-407.68h90.368l144.576 407.68h-85.824zm0 0" fill="currentColor"></path></svg></div> <div class="item night"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon dark-icon"><path d="M935.539 630.402c-11.43-11.432-28.674-14.739-43.531-8.354-46.734 20.103-96.363 30.297-147.508 30.297-99.59 0-193.221-38.784-263.64-109.203-108.637-108.637-139.61-270.022-78.908-411.148a39.497 39.497 0 0 0-51.886-51.887c-52.637 22.64-100.017 54.81-140.826 95.616-85.346 85.346-132.346 198.821-132.346 319.52 0 120.7 47.001 234.172 132.347 319.519S408.063 947.11 528.76 947.11c120.7 0 234.172-47.003 319.52-132.351 40.809-40.81 72.978-88.19 95.616-140.826a39.497 39.497 0 0 0-8.356-43.532z" fill="currentColor"></path></svg></div></div> <!----></div></div></div></button> <div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/vuepress/" class="nav-link router-link-active"><i class="iconfont icon-home"></i>
  Home
</a></div><div class="nav-item"><a href="/vuepress/home/" class="nav-link"><i class="iconfont icon-notice"></i>
  Target
</a></div><div class="nav-item"><a href="/vuepress/guide/" class="nav-link"><i class="iconfont icon-creative"></i>
  Guide
</a></div><div class="nav-item"><a href="https://jiangxj.top/blog//" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont icon-blog"></i>
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div></nav> <div class="nav-links"><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon i18n-icon" style="width:1rem;height:1rem;vertical-align:middle;margin-left:1rem;"><path d="M639.981 344.075c14.805 44.45 34.542 79.023 69.084 113.596 29.603-29.634 49.34-69.146 64.145-113.596H639.981zM314.33 591.024h128.29l-64.145-172.865-64.145 172.865z" fill="currentColor"></path> <path d="M807.746 116.882H215.643c-54.274 0-98.681 44.45-98.681 98.78v592.677c0 54.329 44.407 98.78 98.68 98.78h592.104c54.273 0 98.681-44.451 98.681-98.78V215.66c0-54.329-39.475-98.78-98.68-98.78zM565.971 754.01c-9.866 9.878-19.738 9.878-29.603 9.878-4.94 0-14.805 0-19.738-4.939-4.939-4.939-9.872 0-9.872-4.939s-4.932-9.878-9.865-19.756c-4.94-9.878-4.94-14.817-9.872-24.695L467.29 655.23H294.592l-19.737 54.33c-9.866 19.755-14.805 34.572-19.738 44.45-4.939 9.878-14.804 9.878-29.603 9.878-9.871 0-19.737-4.939-29.609-9.878-9.865-9.878-14.798-14.817-14.798-24.695 0-4.939 0-9.878 4.933-19.756 4.939-9.878 4.939-14.817 9.865-24.695l108.553-276.583c4.939-9.878 4.939-19.756 9.872-29.633 4.932-9.878 9.865-19.756 14.798-24.695 4.939-4.94 9.872-14.817 19.737-19.756 9.872-4.94 19.738-4.94 29.61-4.94 9.865 0 19.73 0 29.603 4.94 9.865 4.939 14.804 9.878 19.737 19.756 4.933 4.939 9.866 14.817 14.798 24.695 4.94 9.877 9.872 19.755 14.805 34.572l108.553 271.644c9.865 19.756 14.804 34.573 14.804 44.451-4.939 4.94-9.872 14.817-14.804 24.695zm271.378-192.62c-54.273-19.756-93.748-44.451-128.29-74.085-34.536 34.573-78.943 59.268-133.223 74.085l-14.798-24.695c54.273-14.817 98.68-34.573 133.223-69.146-34.542-34.573-64.145-79.024-74.017-128.413h-49.34V319.38h133.228c-9.877-14.817-19.743-34.573-29.609-49.39l14.799-4.94c9.871 14.818 24.676 34.574 34.542 54.33h123.35v24.695h-49.34c-14.798 49.39-39.468 93.84-69.077 123.474 34.541 29.634 74.01 54.329 128.29 69.146l-19.738 24.695z" fill="currentColor"></path></svg> <span class="arrow"></span></button> <ul class="nav-dropdown"><li class="dropdown-item"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/" aria-current="page" class="nav-link router-link-exact-active router-link-active active"><!---->
  简体中文
</a></li><li class="dropdown-item"><a href="/vuepress/en/" class="nav-link"><!---->
  English
</a></li></ul></div></div></div> <a rel="noopener noreferrer" href="https://github.com/jiangxjun/vuepress" target="_blank" class="repo-link can-hide">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <!----> <div class="content__navbar-end"></div></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><div vocab="https://schema.org/" typeof="Person" class="blogger-info mobile"><div data-balloon-pos="down" role="navigation" class="blogger hasIntro"><img property="image" alt="Blogger Avatar" src="/vuepress/logo.svg" class="avatar round"> <div property="name" class="name">#XJ</div> <meta property="url" content="/vuepress/intro/"></div> <div class="num-wrapper"><div><div class="num">46</div> <div>文章</div></div> <div><div class="num">10</div> <div>分类</div></div> <div><div class="num">22</div> <div>标签</div></div> <div><div class="num">26</div> <div>时间轴</div></div></div> <div class="media-links-wrapper"><a href="https://zhihu.com" rel="noopener noreferrer" target="_blank" aria-label="Zhihu" data-balloon-pos="up" class="media-link"><span class="sr-only">Zhihu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-zhihu"><circle cx="512" cy="512" r="512" fill="#006CE2"></circle> <path d="M513.65 491.261H411.551c1.615-16.154 5.815-60.095 5.815-84.973 0-24.88-.323-60.742-.323-60.742h102.744V329.39c0-21.647-9.37-31.34-17.124-31.34h-178.67s5.169-17.77 10.015-36.186c4.846-18.417 15.832-44.264 15.832-44.264-63.003 4.2-67.958 50.941-81.743 92.729-13.787 41.785-24.556 62.356-44.586 107.912 27.786 0 55.249-13.57 66.879-32.309 11.631-18.74 16.908-40.71 16.908-40.71h62.035v59.019c0 21.107-3.878 87.45-3.878 87.45H254.742c-19.386 0-29.724 48.894-29.724 48.894h133.76c-8.4 75.82-26.493 106.191-51.91 152.716-25.418 46.525-92.728 99.406-92.728 99.406 41.033 11.63 86.589-3.555 105.974-21.972 19.386-18.417 35.863-49.756 47.817-72.838 11.954-23.081 21.972-65.124 21.972-65.124L498.462 766.86s4.846-24.233 6.461-39.418c1.616-15.186-.755-26.385-4.63-35.433-3.878-9.046-15.509-21.54-31.018-39.634-15.507-18.094-48.034-52.879-48.034-52.879s-15.832 11.63-28.108 21.001c9.046-21.97 16.262-79.695 16.262-79.695h122.343v-20.249c.003-17.66-7.319-29.29-18.089-29.29zm287.337-200.747h-234.35a4.308 4.308 0 0 0-4.309 4.308v435.099a4.308 4.308 0 0 0 4.308 4.308h40.226l14.7 50.402 81.096-50.402h98.328a4.308 4.308 0 0 0 4.308-4.308v-435.1a4.308 4.308 0 0 0-4.308-4.308zM755.97 684.47h-52.343l-61.548 39.095-10.823-39.095h-18.738V338.116H755.97v346.355z" fill="#FFF"></path></svg></a><a href="https://baidu.com" rel="noopener noreferrer" target="_blank" aria-label="Baidu" data-balloon-pos="up" class="media-link"><span class="sr-only">Baidu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-baidu"><circle cx="512" cy="512" r="512" fill="#1D2FE3"></circle> <path d="M239.022 704.978c.098-4.865-.314-9.772.162-14.591 5.178-52.464 197.571-253.377 249.641-259.233 42.996-4.833 75.768 16.545 99.824 49.144 37.893 51.351 82.81 95.455 131.292 136.237 52.903 44.503 56.525 99.801 32.6 158.592-23.425 57.56-75.34 69.833-127.771 58.804-84.971-17.874-168.158-13.744-253.37-4.536-86.35 9.333-133.788-39.4-132.378-124.417zM352.464 412.86c-3.58 50.707-17.93 96.128-75.9 98.12-58.053 1.995-80.093-41.432-79.275-91.71.81-49.705 13.416-104.053 76.851-102.136 53.84 1.625 74.74 45.8 78.324 95.726zm386.053 142.168c-68.494-1.735-84.188-43.331-82.635-93.812 1.46-47.519 10.082-97.628 73.299-96.65 61.395.95 81.6 43.207 81.553 98.668-.047 53.156-19.818 89.398-72.217 91.794zm-45.235-278.345c-10.464 42.665-24.513 91.761-85.919 94.502-52.74 2.354-71.705-34.482-72.805-81.242-1.233-52.42 48.08-112.965 87.582-110.373 33.943 2.226 71.146 49.541 71.142 97.113zm-195.147-14.097c-7.005 46.274-13.63 100.025-71.562 101.351-57.077 1.306-73.567-47.922-73.638-97.109-.068-48.054 12.128-99.024 69.345-101.426 59.45-2.493 67.11 51.093 75.855 97.184z" fill="#fff"></path> <path d="M479.52 663.165c.006 12.194 1.498 24.61-.284 36.537-4.707 31.503 18.862 78.749-45.326 77.534-54.226-1.027-103.338-3.31-113.231-73.536-7.164-50.852 7.78-85.674 57.687-102.668 17.67-6.016 39.618 5.058 54.096-14.548 10.84-14.679-2.901-54.592 33.418-41.47 24.075 8.7 11.477 38.922 13.278 59.652 1.68 19.366.359 38.99.363 58.5zm175.45 41.902c4.291 39.657 5.093 78.047-64.709 73.503-60.097-3.912-95.56-20.794-86.293-85.624 4.287-29.991-21.148-83.238 22.19-84.867 42.71-1.606 13.57 50.41 20.825 77.622 5.276 19.794-3.984 46.774 29.753 48.193 41.337 1.738 28.383-30.022 31.099-51.604 1.209-9.61-.85-19.65.528-29.215 2.516-17.474-8.928-44.716 19.554-47.191 36.044-3.133 24.155 28.376 26.678 47.523 1.896 14.387.375 29.225.375 51.66z" fill="#1D2FE3"></path> <path d="M435.669 685.038c-2.255 24.07 5.605 53.68-33.623 52.136-34.594-1.362-35.274-31.818-38.513-53.078-4.028-26.448 11.38-48.18 40.785-50.023 40.967-2.564 27.097 30.764 31.35 50.965z" fill="#fff"></path></svg></a><a href="https://github.com" rel="noopener noreferrer" target="_blank" aria-label="Github" data-balloon-pos="up" class="media-link"><span class="sr-only">Github</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-github"><circle cx="512" cy="512" r="512" fill="#171515"></circle> <path d="M509.423 146.442c-200.317 0-362.756 162.42-362.756 362.8 0 160.266 103.936 296.24 248.109 344.217 18.139 3.327 24.76-7.872 24.76-17.486 0-8.613-.313-31.427-.49-61.702-100.912 21.923-122.205-48.63-122.205-48.63-16.495-41.91-40.28-53.067-40.28-53.067-32.937-22.51 2.492-22.053 2.492-22.053 36.407 2.566 55.568 37.386 55.568 37.386 32.362 55.438 84.907 39.43 105.58 30.143 3.296-23.444 12.667-39.43 23.032-48.498-80.557-9.156-165.246-40.28-165.246-179.297 0-39.604 14.135-71.988 37.342-97.348-3.731-9.178-16.18-46.063 3.556-96.009 0 0 30.46-9.754 99.76 37.19 28.937-8.048 59.97-12.071 90.823-12.211 30.807.14 61.843 4.165 90.822 12.21 69.26-46.944 99.663-37.189 99.663-37.189 19.792 49.946 7.34 86.831 3.61 96.01 23.25 25.359 37.29 57.742 37.29 97.347 0 139.366-84.82 170.033-165.637 179.013 13.026 11.2 24.628 33.342 24.628 67.182 0 48.498-.445 87.627-.445 99.521 0 9.702 6.535 20.988 24.945 17.444 144.03-48.067 247.881-183.95 247.881-344.175 0-200.378-162.442-362.798-362.802-362.798z" fill="#FFF"></path></svg></a></div></div> <hr> <!----> <div class="content__sidebar-top"></div> <nav class="sidebar-nav-links"><div class="nav-item"><a href="/vuepress/" class="nav-link router-link-active"><i class="iconfont icon-home"></i>
  Home
</a></div><div class="nav-item"><a href="/vuepress/home/" class="nav-link"><i class="iconfont icon-notice"></i>
  Target
</a></div><div class="nav-item"><a href="/vuepress/guide/" class="nav-link"><i class="iconfont icon-creative"></i>
  Guide
</a></div><div class="nav-item"><a href="https://jiangxj.top/blog//" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont icon-blog"></i>
  Blog
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <a rel="noopener noreferrer" href="https://github.com/jiangxjun/vuepress" target="_blank" class="repo-link">
  GitHub
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav> <!----> <div class="content__sidebar-center"></div> <!----> <!----> <div class="content__sidebar-bottom"></div> <!----></aside> <main class="page"><nav class="breadcrumb disable"><!----></nav> <!----> <div class="content__page-top"></div> <div vocab="https://schema.org/" typeof="Article" class="page-title"><h1><!----> <span property="headline">基于Transformers模型的时序预测（TSF）方法</span></h1> <div class="page-info"><!----> <span aria-label="作者🖊" data-balloon-pos="down" categoryPath="/category/$category/" tagPath="/tag/$tag/"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon author-icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z" fill="currentColor"></path></svg> <span property="author">XJ</span></span><span aria-label="访问量🔢" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="visitor-info"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon eye-icon"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 0 0-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z" fill="currentColor"></path></svg> <span id="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/" data-flag-title="基于Transformers模型的时序预测（TSF）方法" class="leancloud_visitors waline-visitor-count"><span class="leancloud-visitors-count">...</span></span></span><span aria-label="写作日期📅" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="time-info"><svg viewBox="0 0 1030 1024" xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 0 1-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 0 1-33.473-33.473V143.657H180.6A134.314 134.314 0 0 0 46.66 277.595v535.756A134.314 134.314 0 0 0 180.6 947.289h669.74a134.36 134.36 0 0 0 133.94-133.938V277.595a134.314 134.314 0 0 0-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 0 1-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 0 1-33.472 33.473z" fill="currentColor"></path></svg> <span property="datePublished">2023-07-01 </span></span><span role="navigation" aria-label="分类🌈" data-balloon-pos="down" defaultAuthor="#XJ" tagPath="/tag/$tag/" class="category-info enable"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon category-icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zm-.854 446.486H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zm446.371-446.486h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zm136.293 813.51H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z" fill="currentColor"></path></svg> <span property="articleSection">PapersReading</span></span><span aria-label="标签🏷" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon tag-icon"><path d="M939.902 458.563 910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 0 0 0 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z" fill="currentColor"></path></svg> <ul class="tags-wrapper"><li class="tag clickable tag0"><span role="navigation">时序预测</span></li></ul> <meta property="keywords" content="时序预测"></span><span aria-label="阅读时间⌛" data-balloon-pos="down" defaultAuthor="#XJ" categoryPath="/category/$category/" tagPath="/tag/$tag/" class="reading-time-info"><svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon timer-icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z" fill="currentColor"></path></svg> <span>大约 21 分钟</span> <meta property="timeRequired" content="PT21M"></span></div> <!----> <hr></div> <div class="anchor-place-holder"><aside id="anchor"><div class="anchor-wrapper"><ul class="anchor-list"><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#transformers在时序任务上的研究综述" class="anchor-link heading2"><div>Transformers在时序任务上的研究综述</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#transformers在时序预测上的应用研究" class="anchor-link heading2"><div>Transformers在时序预测上的应用研究</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#logsparse-transformer模型" class="anchor-link heading3"><div>LogSparse Transformer模型</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#informer模型" class="anchor-link heading3"><div>Informer模型</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#fedformer模型" class="anchor-link heading3"><div>FEDformer模型</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#transformers在时序异常检测上的应用研究" class="anchor-link heading2"><div>Transformers在时序异常检测上的应用研究</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#基于关联性差异的时序异常检测" class="anchor-link heading3"><div>基于关联性差异的时序异常检测</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#transformers在ltsf任务中是否真的有效" class="anchor-link heading2"><div>Transformers在LTSF任务中是否真的有效？</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#术语理解" class="anchor-link heading2"><div>术语理解</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#自回归模型-autoregression-model" class="anchor-link heading3"><div>自回归模型（autoregression model）</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#向量自回归-vector-autoregression" class="anchor-link heading3"><div>向量自回归（vector autoregression）</div></a></li><li class="anchor"><a href="/vuepress/note/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87/Time%20Series%20Forecasting/#归纳偏置-inductive-bias" class="anchor-link heading3"><div>归纳偏置（inductive bias）</div></a></li></ul></div></aside></div> <!----> <div class="content__content-top"></div> <div class="theme-default-content content__default"><blockquote><p>参考：相关顶会论文</p></blockquote> <h2 id="transformers在时序任务上的研究综述"><a href="#transformers在时序任务上的研究综述" class="header-anchor">#</a> Transformers在时序任务上的研究综述</h2> <p>论文从网络架构（network structure）和使用场景（applications）两方面对Transfomer在时序数据上的研究进行了综述。在<strong>网络结构</strong>方面，论文阐述了low-level（module层）和high-level（架构层）的创新研究。在<strong>应用</strong>场景上，论文对Transformer在处理主流的时序任务过程中的insights、strengths和limitations进行了阐述。此外，论文也对<strong>时序建模的性能评价指标</strong>进行了说明，包括robustness analysis、model size analysis和seasonal-trend decomposition analysis。最后，论文给出了应用Transformer处理时序任务的可能的<strong>未来研究方向</strong>，包括：归纳偏置、Transformer和GNN相结合、预训练Transfomer、架构层次的Transformer变体、结合神经网络架构搜索（Neural architecture search，<strong>NAS</strong>）的Transformer。</p> <p>Transformer在自然语言处理、计算机视觉、文本处理等领域显示出了强大的性能，Transformer在建模序列数据中的长范围依赖和交互上具有明显优势，并成功应用在多项时序任务，包括：预测（forecasting）、异常检测（anomaly detection）和分类（classification）等。</p> <p>对时序数据长期时间依赖和短期时间依赖性进行建模，并且同时能捕捉时序数据季节性特征仍然是一个挑战。</p> <p>已有的关于应用深度学习方法处理时序型任务的综述主要集中在：预测、分类、异常检测和数据增强等。Transformer作为深度学习领域中新出现的一个分支，已经在时序领域产生重要影响。</p> <p>论文总结了Transformer在网络结构方面的创新主要包括：（1）增加位置编码；（2）注意力模块修改；（3）架构层面创新，即Transformer的变体。Transformer应用方面主要包括预测、异常检测和分类三种任务。</p> <center><img src="/vuepress/assets/img/image1.57a2afdd.png" title="Transformer在时序建模上的分类" width="60%"></center> <p>【时序Transformers的网络设计】</p> <p>与LSTM或RNN不同，Transformer没有递归和卷积，输入嵌入中加入的位置编码，对序列信息进行建模。</p> <p><strong>（1）position encoding</strong></p> <p>由于Transformer相互交换位置是等价的，但是顺序信息对于时间序列非常重要，因此对输入时间序列的位置进行编码具有重要意义。一种常见的设计是：将位置信息编码为向量，然后作为附加输入与输入时间序列一起注入模型。位置信息编码主要包括以下三种形式：</p> <ul><li><p>vanilla positional encoding</p> <p>手工制作，性能和适应性相对较差，不能充分利用时间序列数据的重要特征。</p></li> <li><p>learnable positional encoding</p> <p>学习嵌入具有更强的灵活性，能够适应特定的任务。如使用LSTM对位置嵌入进行编码，以便于更好地利用时序中的顺序排列信息。</p></li> <li><p>timestamp encoding</p> <p>针对现实场景中可访问的时间戳信息。</p></li></ul> <p><strong>（2）attention module</strong></p> <p>自注意力机制是Transformer的核心，它可以看作是一个<strong>全连接层</strong>，其权值是根据输入模式的两两相似度动态生成的。self-attention与完全连接的层共享相同的最大路径长度，但参数量要小很多，适合建模长期依赖关系，但其计算消耗非常大。原始的Transformer的时间和内存复杂度都是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>L</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。一些工作关注如何减少Transformer的时间和内存复杂度。</p> <p><strong>（3）architecture-level innovation</strong></p> <p>架构层面创新主要聚焦于提升模型对信息的整合能力和改进计算消耗两方面。</p> <p>【时序Transformers的应用】
<strong>（1）forecasting</strong></p> <p>主要包括时间序列预测（如销量预测、股价预测、气温预测等）、时空预测（如交通领域）和事件预测（根据过去事件的历史信息来预测未来事件发生的时间和标志）。</p> <p>时间序列预测典型的方法包括：Informer、AST、Autoformer、FEDformer、TFT、SSDNet、Pyraformer、Aliformer等。</p> <p>spatio-temporal forecasting典型方法包括：Traffic Transformer、时空图Transformer等</p> <p>Event forecasting典型方法包括：SAHP、Transformer Hawkes、ANDTT等。</p> <p><strong>（2）anomaly detection</strong></p> <p>时序异常检测是从正常的时间序列中识别异常事件的出现，在工业届和医疗领域有较多应用。</p> <p><strong>（3）classification</strong></p> <p>时间序列分类任务。其中，分类Transfomer通常采样简单的编码器结构，自注意力层执行表示学习，前馈层得到每个类别的概率。</p> <p>典型方法包括：GTN等。</p> <blockquote><p>Wen Q, Zhou T, Zhang C, et al. Transformers in time series: A survey[J]. arXiv preprint arXiv:2202.07125, 2022.</p></blockquote> <h2 id="transformers在时序预测上的应用研究"><a href="#transformers在时序预测上的应用研究" class="header-anchor">#</a> Transformers在时序预测上的应用研究</h2> <h3 id="logsparse-transformer模型"><a href="#logsparse-transformer模型" class="header-anchor">#</a> LogSparse Transformer模型</h3> <p>时序预测（times series forecasting）在太阳能发电厂能源输出、电力消耗、交通拥堵等场景中有广泛应用。</p> <p>传统的时序预测模型包括状态空间模型（state space models，SSMs）和自回归模型（autoregressive models）。设计该类模型用于单独的适应每个时间序列，并且需要借助专业知识来手动选择趋势、季节性及其它特征。这极大地限制了该类方法在现代大规模时序预测任务中的应用。</p> <p>深度神经网络（deep neural networks）被提出用于解决上述问题，其中一个典型的模型是循环神经网络（recurrent neural network，RNN）。RNNs模型容易受到梯度消失和梯度爆炸（gradident vanishing and exploding）的影响，导致<strong>模型难训练</strong>。针对此提出了RNN的变体模型LSTM和GRU，但对于<strong>长期依赖性</strong>（long-term dependency）建模性能表现不佳，尤其是现实世界中长期和短期依赖性重复性并存。因此，对长期依赖性进行有效建模是解决该问题的关键步骤。</p> <p>Transformer结合注意力机制在处理序列数据取得很大成功，Transformer能够很好的理解存在长期依赖性的时序数据中重复出现的特征模式。但原始的Transfomer存在一定问题，包括：（1）局部不可感知（locality-agnostics）；（2）内存瓶颈（memeory bottleneck）。</p> <p>针对Transformer存在的问题，论文提出了：</p> <p>（1）卷积自注意力机制（convolutional self-attention）用于捕捉时序局部上下文信息。</p> <center><img src="/vuepress/assets/img/image2.6eec5006.png" title="卷积自注意力机制" width="60%"></center> <p>传统的Transformer中，是对每个点单独进行<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Q,K,V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>的投影进行计算。会导致如图（a）中两个红点，尽管在时间序列上的特征不同（一个陡增，一个是缓趋势），但由于绝对值一样，得到的两个attention是很接近的。同理，（c）中框起来的两个区域，其局部特征是类似的，但由于绝对值不一样，计算得到的attention差异不大（理论应该是较大的）。</p> <p>引入卷积自注意力，使用stride为1，kernel大小为k的因果卷积来计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">Q,K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>。通过因果卷积，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi></mrow><annotation encoding="application/x-tex">Q,K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span>可以更好地知晓当前时刻的局部时间序列信息。传统的Transformer相当于k的值为1。</p> <p>（2）LogSparse Transformer模型，实现了将传统Transformer空间和时间复杂度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>L</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>降为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>L</mi><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L(logL)^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">gL</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p> <blockquote><p>Li S, Jin X, Xuan Y, et al. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting[J]. Advances in neural information processing systems, 2019, 32.</p> <p>https://blog.csdn.net/qq_40206371/article/details/126369053</p></blockquote> <h3 id="informer模型"><a href="#informer模型" class="header-anchor">#</a> Informer模型</h3> <p>时间序列预测（time-series forecasting）任务场景包括银行交易量预测、电力系统用电量预测、云服务访问量预测等。如果能提前预测到未来一段时间内的访问量/用电量，就可以提前进行资源部署，防止访问量过大耗尽现有计算资源，拖垮服务。</p> <p>随着预测序列长度增加，预测难度越来越大，对于长序列预测（long sequence time-series forecasting，LSTF）问题，需要模型具有较强的处理长距离依赖（long-range dependency）问题的能力。Transformer已经在LSTF任务上表现出可以增强预测能力的潜力，但同时也存在以下几个问题：</p> <p>（1）quadratic computation of self-attention</p> <p>self-attention的时间和空间复杂度都是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>L</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>是序列长度。</p> <p>（2）high memory usage</p> <p>堆叠的encoder/decoder内存消耗为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>J</mi><mo>⋅</mo><msup><mi>L</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(J\cdot L^2)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span>是堆叠数量，限制了模型在处理长序列输入的扩展性。</p> <p>（3）inherent limitation of the encoder-decoder architecture</p> <p>encoder-decoder结构在解码时是采用step-by-step形式进行推理，预测序列越长，预测需要耗时越长（和RNN类似）。</p> <p>针对上面问题，论文提出的改进如下：</p> <p>（1）提出ProbSparse self-attention机制，时间复杂度将为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>L</mi><mo>⋅</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>L</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L\cdot log L)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal">gL</span><span class="mclose">)</span></span></span></span></p> <p>（2）提出self-attention蒸馏机制来缩短每一层的输入序列长度，降低计算量和内存消耗。</p> <p>（3）提出生成式decoder机制，在inference阶段时一步（one forward step）得到结果，将预测时间复杂度由<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>降到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span>。</p> <blockquote><p>Zhou H, Zhang S, Peng J, et al. Informer: Beyond efficient transformer for long sequence time-series forecasting[C]//Proceedings of the AAAI conference on artificial intelligence. 2021, 35(12): 11106-11115.</p> <p>https://zhuanlan.zhihu.com/p/355133560</p></blockquote> <h3 id="fedformer模型"><a href="#fedformer模型" class="header-anchor">#</a> FEDformer模型</h3> <p>long-term series forecasting任务是需要根据现有的数据对未来做出较长时间段的预测。在部分场景中，模型的输出长度可能达到1000以上，覆盖若干个周期，这对于预测模型的精度和计算效率提出了更高的要求。此外，时序数据往往会受到<strong>分布偏移</strong>和<strong>噪声</strong>的影响，增加了预测的难度。</p> <p>Transformers用于long-term series forecasting任务存在两个问题：（1）模型训练代价昂贵；（2）不能捕捉时序数据全局特征（global view）。</p> <p>针对于上诉问题，论文提出了两种思路：（1）周期趋势项分解（seasonal-trend decomposition）降低输入输出的分布差异；（2）提出一种在频域应用注意力机制的Transformer模型，增加对噪声的鲁棒性。</p> <blockquote><p>Zhou T, Ma Z, Wen Q, et al. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting[C]//International Conference on Machine Learning. PMLR, 2022: 27268-27286.</p> <p>https://zhuanlan.zhihu.com/p/528131016</p></blockquote> <h2 id="transformers在时序异常检测上的应用研究"><a href="#transformers在时序异常检测上的应用研究" class="header-anchor">#</a> Transformers在时序异常检测上的应用研究</h2> <h3 id="基于关联性差异的时序异常检测"><a href="#基于关联性差异的时序异常检测" class="header-anchor">#</a> 基于关联性差异的时序异常检测</h3> <p>论文关注无监督时序异常检测问题。传统的带有无监督范式的时序异常检测方法，包括LOF，OC-SVM，SVDD等，很少考虑时间信息，并且很难泛化应用到未知的真实场景中。</p> <p>基于深度神经网络的时序异常检测方法，往往是通过类型循环网络（RNN）学习时序数据点级别（pointwise）的表征，进而依赖重建或预测误差进行判断是否是异常点。该类方法学习的点级别表征信息量较小，并且可能会被正常模式主导，使得异常值难以区分。此外，通过计算点与点之间的重构误差或者预测误差很难对时间上下文信息（temporal context）进行深刻描述。</p> <p>基于显式关联建模（explicit association modeling）检测异常，包括向量自回归（vector autoregression，VAR）、状态空间建模等。此外，图卷积神经网络（GNN）也被用于学习多变量时序数据存储的信息，虽然图的表达能力更强，但仍然受限于单个时间点，这对于处理复杂时序数据来说是不够的。</p> <p>基于子序列的方法通过计算子序列之间的相似性来检测异常，这在探索更广泛的时间上下文时，这些方法无法捕捉每个时间点与整个序列之间的细粒度（fine-grained）时间关联。</p> <p>论文提出了anomaly transformer方法，通过使用<strong>关联差异</strong>（association discrepancy）来进行异常检测。应用Transformer模型处理时序数据，可以通过自注意力机制获取每一个时间点的时间关联，这可以提供更为丰富的时间上下文信息来表征时序数据动态模式，包括周期性和趋势性（这一部分称为<strong>series-association</strong>）。此外，正常模式数据在长时序数据中占据主导地位，异常数据少，很难对整个时序数据的异常点建立强相关性。由于时序数据的连续，使得异常数据邻近的时间点和异常点具有相似性（<strong>prior-association</strong>）。论文提出的<strong>关联差异</strong>概念是利用每一个时间点的prior-association和series-association的距离（distance）来度量。</p> <blockquote><p>参考文献</p> <p>Xu J, Wu H, Wang J, et al. Anomaly transformer: Time series anomaly detection with association discrepancy[J]. arXiv preprint arXiv:2110.02642, 2021.</p> <p>https://zhuanlan.zhihu.com/p/470844801</p></blockquote> <h2 id="transformers在ltsf任务中是否真的有效"><a href="#transformers在ltsf任务中是否真的有效" class="header-anchor">#</a> Transformers在LTSF任务中是否真的有效？</h2> <p>时序预测（time series forecasting, TSF）广泛应用在交通流量估算、能源管理，金融投资等领域。TSF方法的发展历经传统的统计方法（如ARIMA），到机器学习（如GBRT），再到深度学习阶段。</p> <p>鉴于Transformer在序列建模上的优越性，时间序列预测架构的Transformer系列研究不断发展，包括LogTrans（NeurIPS 2019） Informer（AAAI 2021 Best paper），Autoformer（NeurIPS 2021），Pyraformer（ICLR 2022 Oral），Triformer（IJCAI 2022），FEDformer（ICML 2022）等。</p> <p>多头自注意机制（multi-head self-attention mechanism）在提取长序列中的语义相关性任务（如长文本中的单词或者2D图像的补丁）中具有显著优越性。</p> <p>自注意力机制通过positional encoding来保存序列中的语义信息，但直接应用在时间序列预测任务中容易丢失temporal information。时间序列数据本身是纯数值型数据，基本没有逐点语义关联性（point-wise semantic correlations），语义信息缺乏。时间序列数据中更多的是关注一些列时间点数据的关联性（temporal relations）以及数据点的顺序（order），自注意力机制固有的排列不变性（permutation invariant）不可避免地会丢失时间信息。</p> <p>因此，<strong>Transformers在long-term time series forecasing（LTSF）任务中有效性还有待验证</strong>。</p> <p>现有的基于Transformers的LTSF解决方案已经证明了比传统方法在预测精度上有了很大提高，但实验对比的基线模型都是采用autoregressive（自回归）或iterated multi-step（<strong>IMS</strong>，迭代多步骤）的方法，这些方法存在的一个显著问题是<strong>误差累积</strong>。基于Transformers的LTSF还需要和直接多步（direct multi-step，<strong>DMS</strong>）预测方法进行比较以验证实际性能。</p> <p>需要注意的是，不是所有的时间预测数据都是可预测的，LTSF只对那些具有相对明确的趋势和周期性的时间序列是可行的。</p> <p>现有的Transformer-based方法处理LTSF任务采用的策略主要包括：</p> <p><strong>（1）时序分解（time series decomposition）</strong>：常用的零均值归一化（normalization with zero-mean）；Autoformer提出季节性趋势分解（seasonal-trend decomposition）；FEDformer提出混合专家策略，设置多个大小的移动平均核来提取趋势分量。</p> <p><strong>（2）输入嵌入策略（input embedding strategies）</strong>：Transformer中的自注意力层无法保存时序数据的位置信息，充分利用包括：时序数据的顺序（即local positional information）、周月年等分层时间戳（hierarchical timestamps）、假期活动等不可知时间戳（agnostic timestamps）等来增强模型时间序列输入的时间上下文（temporal context），通过在输入序列中嵌入多个embedding实现，包括：fixed positional encoding，channel projection embedding，learnable temporal embedding等。</p> <p><strong>（3）自注意力机制模式（self-attention schemes）</strong>：LogTrans和Pyraformer引入稀疏性偏差（sparsity bias）提高模型有效性；Informer和FEDformer使用使用自注意力矩阵的低秩（low-rank）特性。</p> <p><strong>（4）解码器（Decoders）</strong>：原版的Transformer解码器是以自回归形式输出，在处理LTSP任务中容易存在误差累积问题。针对该问题，提出了直接多步预测（DMS）策略，例如Informer设计了一种生成式的解码器用于直接多步（DMS）预测；Pyraformer使用一个连接时空轴的全连接层作为解码器；Autoformer分别从趋势周期分量和采用堆叠自相关机制（stacked auto-correlation）的季节分量来实现特征的精细分解，得到最终预测结果；FEDformer提出频率注意力块（frequency attention block）进行解码得到最终结果。</p> <blockquote><p><strong>vanilla Transformer model</strong>：将原版的Transformer模型应用于LTSF任务，存在一定的局限性，包括自注意力机制存在的二次的时间/内存复杂度，自回归编码器存在的累积误差。</p> <p><strong>Informer</strong>：针对上述问题所提出的用于降低复杂度的结构，并采用了直接多步（DMS）策略。</p> <p><strong>autoformer</strong>：扩展改进了Informer模型，增加了自动关联机制，使得模型比标准注意力更好地学习时间依赖性，旨在准确分解时序数据的趋势和季节成分。特别的，该模型使用一个移动平均核来提取输入序列的趋势周期性（trend-cyclical）。</p> <p><strong>FEDformer</strong>：进一步提出混合专家策略，设置多个大小的移动平均核来提取趋势分量。该模型侧重于在时间序列数据中捕捉全局特征，并提出了一个季节性趋势分解模型。</p> <p><strong>Pyraformer</strong>：介绍了金字塔注意模型（PAM），其中尺度间树结构总结了不同分辨率下的特征，尺度内相邻连接对不同范围的时间依赖性进行建模。</p> <p><strong>Earthformer</strong>：该模型专注于预测地球系统，如天气、气候和农业等。介绍了一种cuboid注意力架构。</p> <p><strong>Non-Stationary Transformer</strong>：旨在调整Transformer以处理非平稳时间序列。提出了两种机制：去平稳注意力和一系列平稳化机制。这些机制在Informer、Autoformer、FEDformer和传统的Transformer等模型中都可以提高性能。</p></blockquote> <p>论文认为现有的Transformer-based LTSF解决方案和non-Transformer模型进行性能对比时，所有的non-Transformer比较对象都是采用IMS预测技术（很容易受到累积误差的影响）。论文提出假设：现有的这些方案性能的提升主要是因为采用了DMS预测技术。为验证该假设，论文采用一种最简单的DMS模型LTSF-Linear作为基线模型，和现有的Transformer-based模型进行对比实验。LTSF-Linear是一系列线性模型的组合，不同变量之间共享权重，并且不对任何空间相关性（spatial correlations）进行建模。</p> <p>实验在九个真实数据集上进行，结果表明LTSF-Linear性能优于现有的Transformer-based方法，并且还具有很大的优势。此外，通过消融实验探讨了LTSF模型各个组成部分对时间关系（temporal relation）提取的影响。</p> <p>论文提出两种预处理方法的变体<strong>DLinear</strong>和<strong>NLinear</strong>，用于处理时序跨域问题（包括金融、交通和能源等领域）。DLinear可以增强原始线性模型对趋势明显的时序数据预测的性能。Linear可以提高LTSF-Linear模型对存在分布偏移（distribution shift）数据集处理能力。</p> <p><strong>模型的评估方法</strong>：所有模型（除Earthformer）都在电力、交通、金融和天气数据集上进行了评估。使用的评价指标包括均方误差（MSE）和平均绝对误差（MAE）。</p> <p>该论文只对比了Transformer-based模型，缺乏和其它简单模型的比较，如线性回归、LSTM/GRU、XGB等树形模型。改论文实验仅局限在一些标准数据集，在其他相关数据集上表现还有待验证，如Informer在预测河流流量存在巨大问题，与LSTM甚至是普通的Transformer相比，表现较差。</p> <p>此外，时间序列数据在长度、周期性、趋势和季节性方面存在较大差异，因此模型需要更大范围的数据集来训练和验证。</p> <p>所有的Transformer论文都同样存在有限评估问题，一个复杂的模型最初可能并不总是优于简单模型，这些需要在论文中进行严格比较和缺点的明确说明，而不是掩盖或者简单地假设没有这种情况。</p> <blockquote><p>参考文献</p> <p>Zeng A, Chen M, Zhang L, et al. Are transformers effective for time series forecasting?[C]//Proceedings of the AAAI conference on artificial intelligence. 2023, 37(9): 11121-11128.</p></blockquote> <h2 id="术语理解"><a href="#术语理解" class="header-anchor">#</a> 术语理解</h2> <h3 id="自回归模型-autoregression-model"><a href="#自回归模型-autoregression-model" class="header-anchor">#</a> 自回归模型（autoregression model）</h3> <p>简称AR模型，统计上处理时序的方法，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{t-1}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>相关，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">X_{t-1}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span></span>预测<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。一个最简单的预测为线性组合，即：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>X</mi><mi>t</mi></msub><mo>=</mo><msub><mi>ϕ</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>ϕ</mi><mi>i</mi></msub><msub><mi>X</mi><mrow><mi>t</mi><mo>−</mo><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>ϵ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">X_t = \phi_0 +\sum_{i=1}^p \phi_iX_{t-i}+\epsilon_t
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.9762em;vertical-align:-1.2777em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6985em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3471em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p> <p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\phi_0</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为常数项；<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\epsilon_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">ϵ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是零均值且方差为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>的随机误差值（白噪声），其值对任何<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>时刻都不变。</p> <p>即：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>的当前值等于一个或数个前期值的线性组合+常数项+随机误差。</p> <h3 id="向量自回归-vector-autoregression"><a href="#向量自回归-vector-autoregression" class="header-anchor">#</a> 向量自回归（vector autoregression）</h3> <p>描述在同一个样本期间内的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>个变量（内生变量）可以作为它们过去值得线性函数。一个VAR模型可以写成：</p> <p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>c</mi><mo>+</mo><msub><mi>A</mi><mn>1</mn></msub><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>A</mi><mn>2</mn></msub><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>2</mn></mrow></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>A</mi><mi>p</mi></msub><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mi>p</mi></mrow></msub><mo>+</mo><msub><mi>e</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = c +A_1y_{t-1} + A_{2}y_{t-2} + \dots + A_py_{t-p} +e_t
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p> <p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">c</span></span></span></span>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>得常数向量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">A_i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">n</span></span></span></span>矩阵，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">e_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>误差向量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>是最多使用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>个滞后期。</p> <p>VAR模型是一种用于多变量时间序列分析的统计模型，特别是在变量之间有相互影响关系的时间序列。VAR模型的方程随着时间序列中变量数量的增加而增加，因此它允许对多变量时序数据进行分析和预测，被广泛应用于经济学和天气预报中。</p> <p>ARIMA系列与VAR模型的基本区别在于，所有ARIMA模型都用于单变量时间序列，而VAR模型则用于多变量时间序列。此外，ARIMA模型是单向模型，这意味着因变量受其过去或滞后值本身的影响，而VAR是双向模型，这意味着因变量受其过去的值或另一变量的值的影响，或受两者的影响。</p> <p>链接：https://juejin.cn/post/7087845757713121310</p> <h3 id="归纳偏置-inductive-bias"><a href="#归纳偏置-inductive-bias" class="header-anchor">#</a> 归纳偏置（inductive bias）</h3> <p>机器学习中很多算法会对要处理的任务做一些关于目标函数的必要假设，称为归纳偏置。【<strong>归纳性偏好</strong>】</p> <p>通俗理解：归纳偏置是从现实中的一些现象归纳出一定的规则（heuristic），然后对模型做一定的约束，从而起到“模型选择”的作用，类似贝叶斯学习中的“先验”。</p> <p>西瓜书解释：机器学习算法在学习过程中对某种类型假设的 偏好，称为 归纳偏好。归纳偏好可以看作学习算法自身在一个庞大的假设空间中对假设进行 选择 的 启发式 或 “价值观”。</p> <p>维基百科解释：如果学习器需要去预测 “其未遇到过的输入” 的结果时，则需要一些 假设 来 帮助它做出选择。</p> <p>广义解释：归纳偏置会促使学习算法优先考虑具有某些属性的解。</p> <p>例如：深度神经网络偏好性地认为，层次化处理信息有更好效果；循环神经网络认为信息具有空间局限性，可用滑动卷积共享权重的方式降低参数空间；图网络认为中心节点与邻居节点的相似性会更好地引导信息流动。</p> <p>通常，模型容量（capacity）很大但inductive bias匮乏，容易导致过拟合（overfitting），如Transformer。</p> <blockquote><p>引用自：https://blog.csdn.net/qq_39478403/article/details/121107057</p></blockquote> <p>网络架构搜索（neural architecture search）</p> <p>机器学习算法的效果很大程度上取决于各种超参数，深度学习兴起之前，超参数自动搜索优化方法主要包括：随机搜索（random search）、网络搜索（grid search）、贝叶斯优化（Bayesian optimization）、强化学习（reinforcement learning）、进化算法（evolutional algorithm）等，统称为Hyperparameter optimization（HO）。</p> <p>深度学习领域中主要涉及到的超参数分为两类：（1）训练参数，包括learning rate、batch-size，weight decay等；（2）网络结构参数，包括网络层数，每一层算子，卷积中的filter size等。对于（1）中的参数自动调优仍属于HO范畴，（2）中参数具有纬度高，离散且相互依赖等特点，其自动调优一般称为网络结构搜索（NAS）。</p> <p>网络结构的设计很大程度上还是需要hand-craft，且依赖经验。寻求各种新的更优的网络子结构是一个研究热点。</p> <blockquote><p>参考资料：https://blog.csdn.net/jinzhuojun/article/details/84698471</p></blockquote></div> <!----> <div class="content__content-bottom"></div> <footer class="page-meta"><!----> <!----> <!----></footer> <!----> <!----> <!----> <div class="content__page-bottom"></div></main> <footer class="footer-wrapper"><div class="media-links-wrapper"><a href="https://zhihu.com" rel="noopener noreferrer" target="_blank" aria-label="Zhihu" data-balloon-pos="up" class="media-link"><span class="sr-only">Zhihu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-zhihu"><circle cx="512" cy="512" r="512" fill="#006CE2"></circle> <path d="M513.65 491.261H411.551c1.615-16.154 5.815-60.095 5.815-84.973 0-24.88-.323-60.742-.323-60.742h102.744V329.39c0-21.647-9.37-31.34-17.124-31.34h-178.67s5.169-17.77 10.015-36.186c4.846-18.417 15.832-44.264 15.832-44.264-63.003 4.2-67.958 50.941-81.743 92.729-13.787 41.785-24.556 62.356-44.586 107.912 27.786 0 55.249-13.57 66.879-32.309 11.631-18.74 16.908-40.71 16.908-40.71h62.035v59.019c0 21.107-3.878 87.45-3.878 87.45H254.742c-19.386 0-29.724 48.894-29.724 48.894h133.76c-8.4 75.82-26.493 106.191-51.91 152.716-25.418 46.525-92.728 99.406-92.728 99.406 41.033 11.63 86.589-3.555 105.974-21.972 19.386-18.417 35.863-49.756 47.817-72.838 11.954-23.081 21.972-65.124 21.972-65.124L498.462 766.86s4.846-24.233 6.461-39.418c1.616-15.186-.755-26.385-4.63-35.433-3.878-9.046-15.509-21.54-31.018-39.634-15.507-18.094-48.034-52.879-48.034-52.879s-15.832 11.63-28.108 21.001c9.046-21.97 16.262-79.695 16.262-79.695h122.343v-20.249c.003-17.66-7.319-29.29-18.089-29.29zm287.337-200.747h-234.35a4.308 4.308 0 0 0-4.309 4.308v435.099a4.308 4.308 0 0 0 4.308 4.308h40.226l14.7 50.402 81.096-50.402h98.328a4.308 4.308 0 0 0 4.308-4.308v-435.1a4.308 4.308 0 0 0-4.308-4.308zM755.97 684.47h-52.343l-61.548 39.095-10.823-39.095h-18.738V338.116H755.97v346.355z" fill="#FFF"></path></svg></a><a href="https://baidu.com" rel="noopener noreferrer" target="_blank" aria-label="Baidu" data-balloon-pos="up" class="media-link"><span class="sr-only">Baidu</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-baidu"><circle cx="512" cy="512" r="512" fill="#1D2FE3"></circle> <path d="M239.022 704.978c.098-4.865-.314-9.772.162-14.591 5.178-52.464 197.571-253.377 249.641-259.233 42.996-4.833 75.768 16.545 99.824 49.144 37.893 51.351 82.81 95.455 131.292 136.237 52.903 44.503 56.525 99.801 32.6 158.592-23.425 57.56-75.34 69.833-127.771 58.804-84.971-17.874-168.158-13.744-253.37-4.536-86.35 9.333-133.788-39.4-132.378-124.417zM352.464 412.86c-3.58 50.707-17.93 96.128-75.9 98.12-58.053 1.995-80.093-41.432-79.275-91.71.81-49.705 13.416-104.053 76.851-102.136 53.84 1.625 74.74 45.8 78.324 95.726zm386.053 142.168c-68.494-1.735-84.188-43.331-82.635-93.812 1.46-47.519 10.082-97.628 73.299-96.65 61.395.95 81.6 43.207 81.553 98.668-.047 53.156-19.818 89.398-72.217 91.794zm-45.235-278.345c-10.464 42.665-24.513 91.761-85.919 94.502-52.74 2.354-71.705-34.482-72.805-81.242-1.233-52.42 48.08-112.965 87.582-110.373 33.943 2.226 71.146 49.541 71.142 97.113zm-195.147-14.097c-7.005 46.274-13.63 100.025-71.562 101.351-57.077 1.306-73.567-47.922-73.638-97.109-.068-48.054 12.128-99.024 69.345-101.426 59.45-2.493 67.11 51.093 75.855 97.184z" fill="#fff"></path> <path d="M479.52 663.165c.006 12.194 1.498 24.61-.284 36.537-4.707 31.503 18.862 78.749-45.326 77.534-54.226-1.027-103.338-3.31-113.231-73.536-7.164-50.852 7.78-85.674 57.687-102.668 17.67-6.016 39.618 5.058 54.096-14.548 10.84-14.679-2.901-54.592 33.418-41.47 24.075 8.7 11.477 38.922 13.278 59.652 1.68 19.366.359 38.99.363 58.5zm175.45 41.902c4.291 39.657 5.093 78.047-64.709 73.503-60.097-3.912-95.56-20.794-86.293-85.624 4.287-29.991-21.148-83.238 22.19-84.867 42.71-1.606 13.57 50.41 20.825 77.622 5.276 19.794-3.984 46.774 29.753 48.193 41.337 1.738 28.383-30.022 31.099-51.604 1.209-9.61-.85-19.65.528-29.215 2.516-17.474-8.928-44.716 19.554-47.191 36.044-3.133 24.155 28.376 26.678 47.523 1.896 14.387.375 29.225.375 51.66z" fill="#1D2FE3"></path> <path d="M435.669 685.038c-2.255 24.07 5.605 53.68-33.623 52.136-34.594-1.362-35.274-31.818-38.513-53.078-4.028-26.448 11.38-48.18 40.785-50.023 40.967-2.564 27.097 30.764 31.35 50.965z" fill="#fff"></path></svg></a><a href="https://github.com" rel="noopener noreferrer" target="_blank" aria-label="Github" data-balloon-pos="up" class="media-link"><span class="sr-only">Github</span> <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" class="icon icon-github"><circle cx="512" cy="512" r="512" fill="#171515"></circle> <path d="M509.423 146.442c-200.317 0-362.756 162.42-362.756 362.8 0 160.266 103.936 296.24 248.109 344.217 18.139 3.327 24.76-7.872 24.76-17.486 0-8.613-.313-31.427-.49-61.702-100.912 21.923-122.205-48.63-122.205-48.63-16.495-41.91-40.28-53.067-40.28-53.067-32.937-22.51 2.492-22.053 2.492-22.053 36.407 2.566 55.568 37.386 55.568 37.386 32.362 55.438 84.907 39.43 105.58 30.143 3.296-23.444 12.667-39.43 23.032-48.498-80.557-9.156-165.246-40.28-165.246-179.297 0-39.604 14.135-71.988 37.342-97.348-3.731-9.178-16.18-46.063 3.556-96.009 0 0 30.46-9.754 99.76 37.19 28.937-8.048 59.97-12.071 90.823-12.211 30.807.14 61.843 4.165 90.822 12.21 69.26-46.944 99.663-37.189 99.663-37.189 19.792 49.946 7.34 86.831 3.61 96.01 23.25 25.359 37.29 57.742 37.29 97.347 0 139.366-84.82 170.033-165.637 179.013 13.026 11.2 24.628 33.342 24.628 67.182 0 48.498-.445 87.627-.445 99.521 0 9.702 6.535 20.988 24.945 17.444 144.03-48.067 247.881-183.95 247.881-344.175 0-200.378-162.442-362.798-362.802-362.798z" fill="#FFF"></path></svg></a></div> <div class="footer">页面已经到底啦👆</div> <div class="copyright">Copyright © 2024 #XJ</div></footer></div><div class="global-ui"><!----><!----><div id="pwa-install"><!----> <div id="install-modal-wrapper" style="display:none;"><div class="background"></div> <div class="install-modal"><div class="header"><button aria-label="关闭" class="close-button"><svg width="23" height="22" xmlns="http://www.w3.org/2000/svg" class="icon close-icon"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.12.358a1.224 1.224 0 011.729 0l8.92 8.914L20.686.358a1.224 1.224 0 011.73 1.728L13.497 11l8.92 8.913a1.222 1.222 0 11-1.73 1.729l-8.919-8.913-8.92 8.913a1.224 1.224 0 01-1.729-1.729L10.04 11l-8.92-8.914a1.222 1.222 0 010-1.728z" fill="currentColor"></path></svg></button> <div class="logo"><!----> <div class="title"><h1></h1> <p class="desc">该应用可以安装在您的 PC 或移动设备上。这将使该 Web 应用程序外观和行为与其他应用程序相同。它将在出现在应用程序列表中，并可以固定到主屏幕，开始菜单或任务栏。此 Web 应用程序还将能够与其他应用程序和您的操作系统安全地进行交互。</p></div></div></div> <div class="content"><div class="highlight"><!----> <!----></div> <div class="description"><h3>详情</h3> <p></p></div></div> <div class="button-wrapper"><button class="install-button">
        安装 <span></span></button> <button class="cancel-button">
        取消
      </button></div></div></div></div><div tabindex="-1" role="dialog" aria-hidden="true" class="pswp"><div class="pswp__bg"></div> <div class="pswp__scroll-wrap"><div class="pswp__container"><div class="pswp__item"></div> <div class="pswp__item"></div> <div class="pswp__item"></div></div> <div class="pswp__ui pswp__ui--hidden"><div class="pswp__top-bar"><div class="pswp__counter"></div> <button title="关闭" aria-label="关闭" class="pswp__button pswp__button--close"></button> <button title="分享" aria-label="分享" class="pswp__button pswp__button--share"></button> <button title="切换全屏" aria-label="切换全屏" class="pswp__button pswp__button--fs"></button> <button title="缩放" aria-label="缩放" class="pswp__button pswp__button--zoom"></button> <div class="pswp__preloader"><div class="pswp__preloader__icn"><div class="pswp__preloader__cut"><div class="pswp__preloader__donut"></div></div></div></div></div> <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class="pswp__share-tooltip"></div></div> <button title="上一个 (左箭头)" aria-label="上一个 (左箭头)" class="pswp__button pswp__button--arrow--left"></button> <button title="下一个 (右箭头)" aria-label="下一个 (右箭头)" class="pswp__button pswp__button--arrow--right"></button> <div class="pswp__caption"><div class="pswp__caption__center"></div></div></div></div></div></div></div>
    <script src="/vuepress/assets/js/app.63a6b71e.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Layout.714049f4.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Blog~layout-Layout~layout-NotFound.f5405c06.js" defer></script><script src="/vuepress/assets/js/page-基于Transformers模型的时序预测（TSF）方法.1249ee6d.js" defer></script><script src="/vuepress/assets/js/vendors~layout-Blog~layout-Layout.407483c8.js" defer></script>
  </body>
</html>
