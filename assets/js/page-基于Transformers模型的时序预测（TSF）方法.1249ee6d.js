(window.webpackJsonp=window.webpackJsonp||[]).push([[38],{626:function(t,s,a){t.exports=a.p+"assets/img/image1.57a2afdd.png"},627:function(t,s,a){t.exports=a.p+"assets/img/image2.6eec5006.png"},706:function(t,s,a){"use strict";a.r(s);var i=a(1),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,i=t._self._c||s;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("blockquote",[i("p",[t._v("参考：相关顶会论文")])]),t._v(" "),i("h2",{attrs:{id:"transformers在时序任务上的研究综述"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#transformers在时序任务上的研究综述"}},[t._v("#")]),t._v(" Transformers在时序任务上的研究综述")]),t._v(" "),i("p",[t._v("论文从网络架构（network structure）和使用场景（applications）两方面对Transfomer在时序数据上的研究进行了综述。在"),i("strong",[t._v("网络结构")]),t._v("方面，论文阐述了low-level（module层）和high-level（架构层）的创新研究。在"),i("strong",[t._v("应用")]),t._v("场景上，论文对Transformer在处理主流的时序任务过程中的insights、strengths和limitations进行了阐述。此外，论文也对"),i("strong",[t._v("时序建模的性能评价指标")]),t._v("进行了说明，包括robustness analysis、model size analysis和seasonal-trend decomposition analysis。最后，论文给出了应用Transformer处理时序任务的可能的"),i("strong",[t._v("未来研究方向")]),t._v("，包括：归纳偏置、Transformer和GNN相结合、预训练Transfomer、架构层次的Transformer变体、结合神经网络架构搜索（Neural architecture search，"),i("strong",[t._v("NAS")]),t._v("）的Transformer。")]),t._v(" "),i("p",[t._v("Transformer在自然语言处理、计算机视觉、文本处理等领域显示出了强大的性能，Transformer在建模序列数据中的长范围依赖和交互上具有明显优势，并成功应用在多项时序任务，包括：预测（forecasting）、异常检测（anomaly detection）和分类（classification）等。")]),t._v(" "),i("p",[t._v("对时序数据长期时间依赖和短期时间依赖性进行建模，并且同时能捕捉时序数据季节性特征仍然是一个挑战。")]),t._v(" "),i("p",[t._v("已有的关于应用深度学习方法处理时序型任务的综述主要集中在：预测、分类、异常检测和数据增强等。Transformer作为深度学习领域中新出现的一个分支，已经在时序领域产生重要影响。")]),t._v(" "),i("p",[t._v("论文总结了Transformer在网络结构方面的创新主要包括：（1）增加位置编码；（2）注意力模块修改；（3）架构层面创新，即Transformer的变体。Transformer应用方面主要包括预测、异常检测和分类三种任务。")]),t._v(" "),i("center",[i("img",{attrs:{src:a(626),title:"Transformer在时序建模上的分类",width:"60%"}})]),t._v(" "),i("p",[t._v("【时序Transformers的网络设计】")]),t._v(" "),i("p",[t._v("与LSTM或RNN不同，Transformer没有递归和卷积，输入嵌入中加入的位置编码，对序列信息进行建模。")]),t._v(" "),i("p",[i("strong",[t._v("（1）position encoding")])]),t._v(" "),i("p",[t._v("由于Transformer相互交换位置是等价的，但是顺序信息对于时间序列非常重要，因此对输入时间序列的位置进行编码具有重要意义。一种常见的设计是：将位置信息编码为向量，然后作为附加输入与输入时间序列一起注入模型。位置信息编码主要包括以下三种形式：")]),t._v(" "),i("ul",[i("li",[i("p",[t._v("vanilla positional encoding")]),t._v(" "),i("p",[t._v("手工制作，性能和适应性相对较差，不能充分利用时间序列数据的重要特征。")])]),t._v(" "),i("li",[i("p",[t._v("learnable positional encoding")]),t._v(" "),i("p",[t._v("学习嵌入具有更强的灵活性，能够适应特定的任务。如使用LSTM对位置嵌入进行编码，以便于更好地利用时序中的顺序排列信息。")])]),t._v(" "),i("li",[i("p",[t._v("timestamp encoding")]),t._v(" "),i("p",[t._v("针对现实场景中可访问的时间戳信息。")])])]),t._v(" "),i("p",[i("strong",[t._v("（2）attention module")])]),t._v(" "),i("p",[t._v("自注意力机制是Transformer的核心，它可以看作是一个"),i("strong",[t._v("全连接层")]),t._v("，其权值是根据输入模式的两两相似度动态生成的。self-attention与完全连接的层共享相同的最大路径长度，但参数量要小很多，适合建模长期依赖关系，但其计算消耗非常大。原始的Transformer的时间和内存复杂度都是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("msup",[i("mi",[t._v("L")]),i("mn",[t._v("2")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(L^2)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0641em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("。一些工作关注如何减少Transformer的时间和内存复杂度。")]),t._v(" "),i("p",[i("strong",[t._v("（3）architecture-level innovation")])]),t._v(" "),i("p",[t._v("架构层面创新主要聚焦于提升模型对信息的整合能力和改进计算消耗两方面。")]),t._v(" "),i("p",[t._v("【时序Transformers的应用】\n"),i("strong",[t._v("（1）forecasting")])]),t._v(" "),i("p",[t._v("主要包括时间序列预测（如销量预测、股价预测、气温预测等）、时空预测（如交通领域）和事件预测（根据过去事件的历史信息来预测未来事件发生的时间和标志）。")]),t._v(" "),i("p",[t._v("时间序列预测典型的方法包括：Informer、AST、Autoformer、FEDformer、TFT、SSDNet、Pyraformer、Aliformer等。")]),t._v(" "),i("p",[t._v("spatio-temporal forecasting典型方法包括：Traffic Transformer、时空图Transformer等")]),t._v(" "),i("p",[t._v("Event forecasting典型方法包括：SAHP、Transformer Hawkes、ANDTT等。")]),t._v(" "),i("p",[i("strong",[t._v("（2）anomaly detection")])]),t._v(" "),i("p",[t._v("时序异常检测是从正常的时间序列中识别异常事件的出现，在工业届和医疗领域有较多应用。")]),t._v(" "),i("p",[i("strong",[t._v("（3）classification")])]),t._v(" "),i("p",[t._v("时间序列分类任务。其中，分类Transfomer通常采样简单的编码器结构，自注意力层执行表示学习，前馈层得到每个类别的概率。")]),t._v(" "),i("p",[t._v("典型方法包括：GTN等。")]),t._v(" "),i("blockquote",[i("p",[t._v("Wen Q, Zhou T, Zhang C, et al. Transformers in time series: A survey[J]. arXiv preprint arXiv:2202.07125, 2022.")])]),t._v(" "),i("h2",{attrs:{id:"transformers在时序预测上的应用研究"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#transformers在时序预测上的应用研究"}},[t._v("#")]),t._v(" Transformers在时序预测上的应用研究")]),t._v(" "),i("h3",{attrs:{id:"logsparse-transformer模型"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#logsparse-transformer模型"}},[t._v("#")]),t._v(" LogSparse Transformer模型")]),t._v(" "),i("p",[t._v("时序预测（times series forecasting）在太阳能发电厂能源输出、电力消耗、交通拥堵等场景中有广泛应用。")]),t._v(" "),i("p",[t._v("传统的时序预测模型包括状态空间模型（state space models，SSMs）和自回归模型（autoregressive models）。设计该类模型用于单独的适应每个时间序列，并且需要借助专业知识来手动选择趋势、季节性及其它特征。这极大地限制了该类方法在现代大规模时序预测任务中的应用。")]),t._v(" "),i("p",[t._v("深度神经网络（deep neural networks）被提出用于解决上述问题，其中一个典型的模型是循环神经网络（recurrent neural network，RNN）。RNNs模型容易受到梯度消失和梯度爆炸（gradident vanishing and exploding）的影响，导致"),i("strong",[t._v("模型难训练")]),t._v("。针对此提出了RNN的变体模型LSTM和GRU，但对于"),i("strong",[t._v("长期依赖性")]),t._v("（long-term dependency）建模性能表现不佳，尤其是现实世界中长期和短期依赖性重复性并存。因此，对长期依赖性进行有效建模是解决该问题的关键步骤。")]),t._v(" "),i("p",[t._v("Transformer结合注意力机制在处理序列数据取得很大成功，Transformer能够很好的理解存在长期依赖性的时序数据中重复出现的特征模式。但原始的Transfomer存在一定问题，包括：（1）局部不可感知（locality-agnostics）；（2）内存瓶颈（memeory bottleneck）。")]),t._v(" "),i("p",[t._v("针对Transformer存在的问题，论文提出了：")]),t._v(" "),i("p",[t._v("（1）卷积自注意力机制（convolutional self-attention）用于捕捉时序局部上下文信息。")]),t._v(" "),i("center",[i("img",{attrs:{src:a(627),title:"卷积自注意力机制",width:"60%"}})]),t._v(" "),i("p",[t._v("传统的Transformer中，是对每个点单独进行"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("Q")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mi",[t._v("K")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mi",[t._v("V")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Q,K,V")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8778em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("Q")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.22222em"}},[t._v("V")])])])]),t._v("的投影进行计算。会导致如图（a）中两个红点，尽管在时间序列上的特征不同（一个陡增，一个是缓趋势），但由于绝对值一样，得到的两个attention是很接近的。同理，（c）中框起来的两个区域，其局部特征是类似的，但由于绝对值不一样，计算得到的attention差异不大（理论应该是较大的）。")]),t._v(" "),i("p",[t._v("引入卷积自注意力，使用stride为1，kernel大小为k的因果卷积来计算"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("Q")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mi",[t._v("K")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Q,K")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8778em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("Q")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")])])])]),t._v("。通过因果卷积，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("Q")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mi",[t._v("K")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Q,K")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8778em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("Q")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")])])])]),t._v("可以更好地知晓当前时刻的局部时间序列信息。传统的Transformer相当于k的值为1。")]),t._v(" "),i("p",[t._v("（2）LogSparse Transformer模型，实现了将传统Transformer空间和时间复杂度"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("msup",[i("mi",[t._v("L")]),i("mn",[t._v("2")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(L^2)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0641em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("降为"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("L")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("l")]),i("mi",[t._v("o")]),i("mi",[t._v("g")]),i("mi",[t._v("L")]),i("msup",[i("mo",{attrs:{stretchy:"false"}},[t._v(")")]),i("mn",[t._v("2")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(L(logL)^2)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0641em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),i("span",{staticClass:"mord mathnormal"},[t._v("o")]),i("span",{staticClass:"mord mathnormal"},[t._v("gL")]),i("span",{staticClass:"mclose"},[i("span",{staticClass:"mclose"},[t._v(")")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("。")]),t._v(" "),i("blockquote",[i("p",[t._v("Li S, Jin X, Xuan Y, et al. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting[J]. Advances in neural information processing systems, 2019, 32.")]),t._v(" "),i("p",[t._v("https://blog.csdn.net/qq_40206371/article/details/126369053")])]),t._v(" "),i("h3",{attrs:{id:"informer模型"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#informer模型"}},[t._v("#")]),t._v(" Informer模型")]),t._v(" "),i("p",[t._v("时间序列预测（time-series forecasting）任务场景包括银行交易量预测、电力系统用电量预测、云服务访问量预测等。如果能提前预测到未来一段时间内的访问量/用电量，就可以提前进行资源部署，防止访问量过大耗尽现有计算资源，拖垮服务。")]),t._v(" "),i("p",[t._v("随着预测序列长度增加，预测难度越来越大，对于长序列预测（long sequence time-series forecasting，LSTF）问题，需要模型具有较强的处理长距离依赖（long-range dependency）问题的能力。Transformer已经在LSTF任务上表现出可以增强预测能力的潜力，但同时也存在以下几个问题：")]),t._v(" "),i("p",[t._v("（1）quadratic computation of self-attention")]),t._v(" "),i("p",[t._v("self-attention的时间和空间复杂度都是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("msup",[i("mi",[t._v("L")]),i("mn",[t._v("2")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(L^2)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0641em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，其中"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("L")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("L")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("L")])])])]),t._v("是序列长度。")]),t._v(" "),i("p",[t._v("（2）high memory usage")]),t._v(" "),i("p",[t._v("堆叠的encoder/decoder内存消耗为"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("J")]),i("mo",[t._v("⋅")]),i("msup",[i("mi",[t._v("L")]),i("mn",[t._v("2")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(J\\cdot L^2)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.09618em"}},[t._v("J")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("⋅")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0641em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，其中"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("J")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("J")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.09618em"}},[t._v("J")])])])]),t._v("是堆叠数量，限制了模型在处理长序列输入的扩展性。")]),t._v(" "),i("p",[t._v("（3）inherent limitation of the encoder-decoder architecture")]),t._v(" "),i("p",[t._v("encoder-decoder结构在解码时是采用step-by-step形式进行推理，预测序列越长，预测需要耗时越长（和RNN类似）。")]),t._v(" "),i("p",[t._v("针对上面问题，论文提出的改进如下：")]),t._v(" "),i("p",[t._v("（1）提出ProbSparse self-attention机制，时间复杂度将为"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("L")]),i("mo",[t._v("⋅")]),i("mi",[t._v("l")]),i("mi",[t._v("o")]),i("mi",[t._v("g")]),i("mi",[t._v("L")]),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(L\\cdot log L)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal"},[t._v("L")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("⋅")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),i("span",{staticClass:"mord mathnormal"},[t._v("o")]),i("span",{staticClass:"mord mathnormal"},[t._v("gL")]),i("span",{staticClass:"mclose"},[t._v(")")])])])])]),t._v(" "),i("p",[t._v("（2）提出self-attention蒸馏机制来缩短每一层的输入序列长度，降低计算量和内存消耗。")]),t._v(" "),i("p",[t._v("（3）提出生成式decoder机制，在inference阶段时一步（one forward step）得到结果，将预测时间复杂度由"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("N")]),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(N)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("降到"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("O")]),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mn",[t._v("1")]),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("O(1)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.02778em"}},[t._v("O")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord"},[t._v("1")]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("。")]),t._v(" "),i("blockquote",[i("p",[t._v("Zhou H, Zhang S, Peng J, et al. Informer: Beyond efficient transformer for long sequence time-series forecasting[C]//Proceedings of the AAAI conference on artificial intelligence. 2021, 35(12): 11106-11115.")]),t._v(" "),i("p",[t._v("https://zhuanlan.zhihu.com/p/355133560")])]),t._v(" "),i("h3",{attrs:{id:"fedformer模型"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#fedformer模型"}},[t._v("#")]),t._v(" FEDformer模型")]),t._v(" "),i("p",[t._v("long-term series forecasting任务是需要根据现有的数据对未来做出较长时间段的预测。在部分场景中，模型的输出长度可能达到1000以上，覆盖若干个周期，这对于预测模型的精度和计算效率提出了更高的要求。此外，时序数据往往会受到"),i("strong",[t._v("分布偏移")]),t._v("和"),i("strong",[t._v("噪声")]),t._v("的影响，增加了预测的难度。")]),t._v(" "),i("p",[t._v("Transformers用于long-term series forecasting任务存在两个问题：（1）模型训练代价昂贵；（2）不能捕捉时序数据全局特征（global view）。")]),t._v(" "),i("p",[t._v("针对于上诉问题，论文提出了两种思路：（1）周期趋势项分解（seasonal-trend decomposition）降低输入输出的分布差异；（2）提出一种在频域应用注意力机制的Transformer模型，增加对噪声的鲁棒性。")]),t._v(" "),i("blockquote",[i("p",[t._v("Zhou T, Ma Z, Wen Q, et al. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting[C]//International Conference on Machine Learning. PMLR, 2022: 27268-27286.")]),t._v(" "),i("p",[t._v("https://zhuanlan.zhihu.com/p/528131016")])]),t._v(" "),i("h2",{attrs:{id:"transformers在时序异常检测上的应用研究"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#transformers在时序异常检测上的应用研究"}},[t._v("#")]),t._v(" Transformers在时序异常检测上的应用研究")]),t._v(" "),i("h3",{attrs:{id:"基于关联性差异的时序异常检测"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#基于关联性差异的时序异常检测"}},[t._v("#")]),t._v(" 基于关联性差异的时序异常检测")]),t._v(" "),i("p",[t._v("论文关注无监督时序异常检测问题。传统的带有无监督范式的时序异常检测方法，包括LOF，OC-SVM，SVDD等，很少考虑时间信息，并且很难泛化应用到未知的真实场景中。")]),t._v(" "),i("p",[t._v("基于深度神经网络的时序异常检测方法，往往是通过类型循环网络（RNN）学习时序数据点级别（pointwise）的表征，进而依赖重建或预测误差进行判断是否是异常点。该类方法学习的点级别表征信息量较小，并且可能会被正常模式主导，使得异常值难以区分。此外，通过计算点与点之间的重构误差或者预测误差很难对时间上下文信息（temporal context）进行深刻描述。")]),t._v(" "),i("p",[t._v("基于显式关联建模（explicit association modeling）检测异常，包括向量自回归（vector autoregression，VAR）、状态空间建模等。此外，图卷积神经网络（GNN）也被用于学习多变量时序数据存储的信息，虽然图的表达能力更强，但仍然受限于单个时间点，这对于处理复杂时序数据来说是不够的。")]),t._v(" "),i("p",[t._v("基于子序列的方法通过计算子序列之间的相似性来检测异常，这在探索更广泛的时间上下文时，这些方法无法捕捉每个时间点与整个序列之间的细粒度（fine-grained）时间关联。")]),t._v(" "),i("p",[t._v("论文提出了anomaly transformer方法，通过使用"),i("strong",[t._v("关联差异")]),t._v("（association discrepancy）来进行异常检测。应用Transformer模型处理时序数据，可以通过自注意力机制获取每一个时间点的时间关联，这可以提供更为丰富的时间上下文信息来表征时序数据动态模式，包括周期性和趋势性（这一部分称为"),i("strong",[t._v("series-association")]),t._v("）。此外，正常模式数据在长时序数据中占据主导地位，异常数据少，很难对整个时序数据的异常点建立强相关性。由于时序数据的连续，使得异常数据邻近的时间点和异常点具有相似性（"),i("strong",[t._v("prior-association")]),t._v("）。论文提出的"),i("strong",[t._v("关联差异")]),t._v("概念是利用每一个时间点的prior-association和series-association的距离（distance）来度量。")]),t._v(" "),i("blockquote",[i("p",[t._v("参考文献")]),t._v(" "),i("p",[t._v("Xu J, Wu H, Wang J, et al. Anomaly transformer: Time series anomaly detection with association discrepancy[J]. arXiv preprint arXiv:2110.02642, 2021.")]),t._v(" "),i("p",[t._v("https://zhuanlan.zhihu.com/p/470844801")])]),t._v(" "),i("h2",{attrs:{id:"transformers在ltsf任务中是否真的有效"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#transformers在ltsf任务中是否真的有效"}},[t._v("#")]),t._v(" Transformers在LTSF任务中是否真的有效？")]),t._v(" "),i("p",[t._v("时序预测（time series forecasting, TSF）广泛应用在交通流量估算、能源管理，金融投资等领域。TSF方法的发展历经传统的统计方法（如ARIMA），到机器学习（如GBRT），再到深度学习阶段。")]),t._v(" "),i("p",[t._v("鉴于Transformer在序列建模上的优越性，时间序列预测架构的Transformer系列研究不断发展，包括LogTrans（NeurIPS 2019） Informer（AAAI 2021 Best paper），Autoformer（NeurIPS 2021），Pyraformer（ICLR 2022 Oral），Triformer（IJCAI 2022），FEDformer（ICML 2022）等。")]),t._v(" "),i("p",[t._v("多头自注意机制（multi-head self-attention mechanism）在提取长序列中的语义相关性任务（如长文本中的单词或者2D图像的补丁）中具有显著优越性。")]),t._v(" "),i("p",[t._v("自注意力机制通过positional encoding来保存序列中的语义信息，但直接应用在时间序列预测任务中容易丢失temporal information。时间序列数据本身是纯数值型数据，基本没有逐点语义关联性（point-wise semantic correlations），语义信息缺乏。时间序列数据中更多的是关注一些列时间点数据的关联性（temporal relations）以及数据点的顺序（order），自注意力机制固有的排列不变性（permutation invariant）不可避免地会丢失时间信息。")]),t._v(" "),i("p",[t._v("因此，"),i("strong",[t._v("Transformers在long-term time series forecasing（LTSF）任务中有效性还有待验证")]),t._v("。")]),t._v(" "),i("p",[t._v("现有的基于Transformers的LTSF解决方案已经证明了比传统方法在预测精度上有了很大提高，但实验对比的基线模型都是采用autoregressive（自回归）或iterated multi-step（"),i("strong",[t._v("IMS")]),t._v("，迭代多步骤）的方法，这些方法存在的一个显著问题是"),i("strong",[t._v("误差累积")]),t._v("。基于Transformers的LTSF还需要和直接多步（direct multi-step，"),i("strong",[t._v("DMS")]),t._v("）预测方法进行比较以验证实际性能。")]),t._v(" "),i("p",[t._v("需要注意的是，不是所有的时间预测数据都是可预测的，LTSF只对那些具有相对明确的趋势和周期性的时间序列是可行的。")]),t._v(" "),i("p",[t._v("现有的Transformer-based方法处理LTSF任务采用的策略主要包括：")]),t._v(" "),i("p",[i("strong",[t._v("（1）时序分解（time series decomposition）")]),t._v("：常用的零均值归一化（normalization with zero-mean）；Autoformer提出季节性趋势分解（seasonal-trend decomposition）；FEDformer提出混合专家策略，设置多个大小的移动平均核来提取趋势分量。")]),t._v(" "),i("p",[i("strong",[t._v("（2）输入嵌入策略（input embedding strategies）")]),t._v("：Transformer中的自注意力层无法保存时序数据的位置信息，充分利用包括：时序数据的顺序（即local positional information）、周月年等分层时间戳（hierarchical timestamps）、假期活动等不可知时间戳（agnostic timestamps）等来增强模型时间序列输入的时间上下文（temporal context），通过在输入序列中嵌入多个embedding实现，包括：fixed positional encoding，channel projection embedding，learnable temporal embedding等。")]),t._v(" "),i("p",[i("strong",[t._v("（3）自注意力机制模式（self-attention schemes）")]),t._v("：LogTrans和Pyraformer引入稀疏性偏差（sparsity bias）提高模型有效性；Informer和FEDformer使用使用自注意力矩阵的低秩（low-rank）特性。")]),t._v(" "),i("p",[i("strong",[t._v("（4）解码器（Decoders）")]),t._v("：原版的Transformer解码器是以自回归形式输出，在处理LTSP任务中容易存在误差累积问题。针对该问题，提出了直接多步预测（DMS）策略，例如Informer设计了一种生成式的解码器用于直接多步（DMS）预测；Pyraformer使用一个连接时空轴的全连接层作为解码器；Autoformer分别从趋势周期分量和采用堆叠自相关机制（stacked auto-correlation）的季节分量来实现特征的精细分解，得到最终预测结果；FEDformer提出频率注意力块（frequency attention block）进行解码得到最终结果。")]),t._v(" "),i("blockquote",[i("p",[i("strong",[t._v("vanilla Transformer model")]),t._v("：将原版的Transformer模型应用于LTSF任务，存在一定的局限性，包括自注意力机制存在的二次的时间/内存复杂度，自回归编码器存在的累积误差。")]),t._v(" "),i("p",[i("strong",[t._v("Informer")]),t._v("：针对上述问题所提出的用于降低复杂度的结构，并采用了直接多步（DMS）策略。")]),t._v(" "),i("p",[i("strong",[t._v("autoformer")]),t._v("：扩展改进了Informer模型，增加了自动关联机制，使得模型比标准注意力更好地学习时间依赖性，旨在准确分解时序数据的趋势和季节成分。特别的，该模型使用一个移动平均核来提取输入序列的趋势周期性（trend-cyclical）。")]),t._v(" "),i("p",[i("strong",[t._v("FEDformer")]),t._v("：进一步提出混合专家策略，设置多个大小的移动平均核来提取趋势分量。该模型侧重于在时间序列数据中捕捉全局特征，并提出了一个季节性趋势分解模型。")]),t._v(" "),i("p",[i("strong",[t._v("Pyraformer")]),t._v("：介绍了金字塔注意模型（PAM），其中尺度间树结构总结了不同分辨率下的特征，尺度内相邻连接对不同范围的时间依赖性进行建模。")]),t._v(" "),i("p",[i("strong",[t._v("Earthformer")]),t._v("：该模型专注于预测地球系统，如天气、气候和农业等。介绍了一种cuboid注意力架构。")]),t._v(" "),i("p",[i("strong",[t._v("Non-Stationary Transformer")]),t._v("：旨在调整Transformer以处理非平稳时间序列。提出了两种机制：去平稳注意力和一系列平稳化机制。这些机制在Informer、Autoformer、FEDformer和传统的Transformer等模型中都可以提高性能。")])]),t._v(" "),i("p",[t._v("论文认为现有的Transformer-based LTSF解决方案和non-Transformer模型进行性能对比时，所有的non-Transformer比较对象都是采用IMS预测技术（很容易受到累积误差的影响）。论文提出假设：现有的这些方案性能的提升主要是因为采用了DMS预测技术。为验证该假设，论文采用一种最简单的DMS模型LTSF-Linear作为基线模型，和现有的Transformer-based模型进行对比实验。LTSF-Linear是一系列线性模型的组合，不同变量之间共享权重，并且不对任何空间相关性（spatial correlations）进行建模。")]),t._v(" "),i("p",[t._v("实验在九个真实数据集上进行，结果表明LTSF-Linear性能优于现有的Transformer-based方法，并且还具有很大的优势。此外，通过消融实验探讨了LTSF模型各个组成部分对时间关系（temporal relation）提取的影响。")]),t._v(" "),i("p",[t._v("论文提出两种预处理方法的变体"),i("strong",[t._v("DLinear")]),t._v("和"),i("strong",[t._v("NLinear")]),t._v("，用于处理时序跨域问题（包括金融、交通和能源等领域）。DLinear可以增强原始线性模型对趋势明显的时序数据预测的性能。Linear可以提高LTSF-Linear模型对存在分布偏移（distribution shift）数据集处理能力。")]),t._v(" "),i("p",[i("strong",[t._v("模型的评估方法")]),t._v("：所有模型（除Earthformer）都在电力、交通、金融和天气数据集上进行了评估。使用的评价指标包括均方误差（MSE）和平均绝对误差（MAE）。")]),t._v(" "),i("p",[t._v("该论文只对比了Transformer-based模型，缺乏和其它简单模型的比较，如线性回归、LSTM/GRU、XGB等树形模型。改论文实验仅局限在一些标准数据集，在其他相关数据集上表现还有待验证，如Informer在预测河流流量存在巨大问题，与LSTM甚至是普通的Transformer相比，表现较差。")]),t._v(" "),i("p",[t._v("此外，时间序列数据在长度、周期性、趋势和季节性方面存在较大差异，因此模型需要更大范围的数据集来训练和验证。")]),t._v(" "),i("p",[t._v("所有的Transformer论文都同样存在有限评估问题，一个复杂的模型最初可能并不总是优于简单模型，这些需要在论文中进行严格比较和缺点的明确说明，而不是掩盖或者简单地假设没有这种情况。")]),t._v(" "),i("blockquote",[i("p",[t._v("参考文献")]),t._v(" "),i("p",[t._v("Zeng A, Chen M, Zhang L, et al. Are transformers effective for time series forecasting?[C]//Proceedings of the AAAI conference on artificial intelligence. 2023, 37(9): 11121-11128.")])]),t._v(" "),i("h2",{attrs:{id:"术语理解"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#术语理解"}},[t._v("#")]),t._v(" 术语理解")]),t._v(" "),i("h3",{attrs:{id:"自回归模型-autoregression-model"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#自回归模型-autoregression-model"}},[t._v("#")]),t._v(" 自回归模型（autoregression model）")]),t._v(" "),i("p",[t._v("简称AR模型，统计上处理时序的方法，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("X")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("与"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("X")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mn",[t._v("1")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_{t-1}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8917em","vertical-align":"-0.2083em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mtight"},[t._v("1")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2083em"}},[i("span")])])])])])])])]),t._v("相关，用"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("X")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mn",[t._v("1")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_{t-1}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8917em","vertical-align":"-0.2083em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mtight"},[t._v("1")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2083em"}},[i("span")])])])])])])])]),t._v("预测"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("X")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("。一个最简单的预测为线性组合，即：")]),t._v(" "),i("p",{staticClass:"katex-block"},[i("span",{staticClass:"katex-display"},[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("X")]),i("mi",[t._v("t")])],1),i("mo",[t._v("=")]),i("msub",[i("mi",[t._v("ϕ")]),i("mn",[t._v("0")])],1),i("mo",[t._v("+")]),i("munderover",[i("mo",[t._v("∑")]),i("mrow",[i("mi",[t._v("i")]),i("mo",[t._v("=")]),i("mn",[t._v("1")])],1),i("mi",[t._v("p")])],1),i("msub",[i("mi",[t._v("ϕ")]),i("mi",[t._v("i")])],1),i("msub",[i("mi",[t._v("X")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mi",[t._v("i")])],1)],1),i("mo",[t._v("+")]),i("msub",[i("mi",[t._v("ϵ")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X_t = \\phi_0 +\\sum_{i=1}^p \\phi_iX_{t-i}+\\epsilon_t\n")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("=")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("ϕ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("0")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"2.9762em","vertical-align":"-1.2777em"}}),i("span",{staticClass:"mop op-limits"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"1.6985em"}},[i("span",{staticStyle:{top:"-1.8723em","margin-left":"0em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("i")]),i("span",{staticClass:"mrel mtight"},[t._v("=")]),i("span",{staticClass:"mord mtight"},[t._v("1")])])])]),i("span",{staticStyle:{top:"-3.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),i("span",[i("span",{staticClass:"mop op-symbol large-op"},[t._v("∑")])])]),i("span",{staticStyle:{top:"-4.3471em","margin-left":"0em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"3.05em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("p")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"1.2777em"}},[i("span")])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("ϕ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3117em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3117em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0785em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2083em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.5806em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("ϵ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])])]),t._v(" "),i("p",[t._v("其中，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("ϕ")]),i("mn",[t._v("0")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\phi_0")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8889em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("ϕ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("0")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("为常数项；"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("ϵ")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\epsilon_t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.5806em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("ϵ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("是零均值且方差为"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msup",[i("mi",[t._v("σ")]),i("mn",[t._v("2")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\sigma^2")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8141em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("σ")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8141em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])])])])])])])]),t._v("的随机误差值（白噪声），其值对任何"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("t")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6151em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("t")])])])]),t._v("时刻都不变。")]),t._v(" "),i("p",[t._v("即："),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("X")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")])])])]),t._v("的当前值等于一个或数个前期值的线性组合+常数项+随机误差。")]),t._v(" "),i("h3",{attrs:{id:"向量自回归-vector-autoregression"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#向量自回归-vector-autoregression"}},[t._v("#")]),t._v(" 向量自回归（vector autoregression）")]),t._v(" "),i("p",[t._v("描述在同一个样本期间内的"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("n")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("n")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.4306em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("n")])])])]),t._v("个变量（内生变量）可以作为它们过去值得线性函数。一个VAR模型可以写成：")]),t._v(" "),i("p",{staticClass:"katex-block"},[i("span",{staticClass:"katex-display"},[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("y")]),i("mi",[t._v("t")])],1),i("mo",[t._v("=")]),i("mi",[t._v("c")]),i("mo",[t._v("+")]),i("msub",[i("mi",[t._v("A")]),i("mn",[t._v("1")])],1),i("msub",[i("mi",[t._v("y")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mn",[t._v("1")])],1)],1),i("mo",[t._v("+")]),i("msub",[i("mi",[t._v("A")]),i("mn",[t._v("2")])],1),i("msub",[i("mi",[t._v("y")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mn",[t._v("2")])],1)],1),i("mo",[t._v("+")]),i("mo",[t._v("⋯")]),i("mo",[t._v("+")]),i("msub",[i("mi",[t._v("A")]),i("mi",[t._v("p")])],1),i("msub",[i("mi",[t._v("y")]),i("mrow",[i("mi",[t._v("t")]),i("mo",[t._v("−")]),i("mi",[t._v("p")])],1)],1),i("mo",[t._v("+")]),i("msub",[i("mi",[t._v("e")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("y_t = c +A_1y_{t-1} + A_{2}y_{t-2} + \\dots + A_py_{t-p} +e_t\n")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("=")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("c")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8917em","vertical-align":"-0.2083em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("A")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[t._v("1")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mtight"},[t._v("1")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2083em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8917em","vertical-align":"-0.2083em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("A")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mtight"},[t._v("2")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3011em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mtight"},[t._v("2")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2083em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"minner"},[t._v("⋯")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.9694em","vertical-align":"-0.2861em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("A")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.1514em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("p")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2861em"}},[i("span")])])])])]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0359em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")]),i("span",{staticClass:"mbin mtight"},[t._v("−")]),i("span",{staticClass:"mord mathnormal mtight"},[t._v("p")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2861em"}},[i("span")])])])])]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.5806em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("e")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])])]),t._v(" "),i("p",[t._v("其中，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("c")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("c")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.4306em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("c")])])])]),t._v("是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("n")]),i("mo",[t._v("×")]),i("mn",[t._v("1")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("n \\times 1")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("n")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("1")])])])]),t._v("得常数向量，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("A")]),i("mi",[t._v("i")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A_i")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("A")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3117em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("i")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("n")]),i("mo",[t._v("×")]),i("mi",[t._v("n")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("n \\times n")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("n")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.4306em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("n")])])])]),t._v("矩阵，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("e")]),i("mi",[t._v("t")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("e_t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.5806em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal"},[t._v("e")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.2806em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("t")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("n")]),i("mo",[t._v("×")]),i("mn",[t._v("1")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("n \\times 1")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("n")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("1")])])])]),t._v("误差向量，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("p")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("p")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("p")])])])]),t._v("是最多使用"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("y")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("y")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")])])])]),t._v("的"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("p")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("p")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.625em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("p")])])])]),t._v("个滞后期。")]),t._v(" "),i("p",[t._v("VAR模型是一种用于多变量时间序列分析的统计模型，特别是在变量之间有相互影响关系的时间序列。VAR模型的方程随着时间序列中变量数量的增加而增加，因此它允许对多变量时序数据进行分析和预测，被广泛应用于经济学和天气预报中。")]),t._v(" "),i("p",[t._v("ARIMA系列与VAR模型的基本区别在于，所有ARIMA模型都用于单变量时间序列，而VAR模型则用于多变量时间序列。此外，ARIMA模型是单向模型，这意味着因变量受其过去或滞后值本身的影响，而VAR是双向模型，这意味着因变量受其过去的值或另一变量的值的影响，或受两者的影响。")]),t._v(" "),i("p",[t._v("链接：https://juejin.cn/post/7087845757713121310")]),t._v(" "),i("h3",{attrs:{id:"归纳偏置-inductive-bias"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#归纳偏置-inductive-bias"}},[t._v("#")]),t._v(" 归纳偏置（inductive bias）")]),t._v(" "),i("p",[t._v("机器学习中很多算法会对要处理的任务做一些关于目标函数的必要假设，称为归纳偏置。【"),i("strong",[t._v("归纳性偏好")]),t._v("】")]),t._v(" "),i("p",[t._v("通俗理解：归纳偏置是从现实中的一些现象归纳出一定的规则（heuristic），然后对模型做一定的约束，从而起到“模型选择”的作用，类似贝叶斯学习中的“先验”。")]),t._v(" "),i("p",[t._v("西瓜书解释：机器学习算法在学习过程中对某种类型假设的 偏好，称为 归纳偏好。归纳偏好可以看作学习算法自身在一个庞大的假设空间中对假设进行 选择 的 启发式 或 “价值观”。")]),t._v(" "),i("p",[t._v("维基百科解释：如果学习器需要去预测 “其未遇到过的输入” 的结果时，则需要一些 假设 来 帮助它做出选择。")]),t._v(" "),i("p",[t._v("广义解释：归纳偏置会促使学习算法优先考虑具有某些属性的解。")]),t._v(" "),i("p",[t._v("例如：深度神经网络偏好性地认为，层次化处理信息有更好效果；循环神经网络认为信息具有空间局限性，可用滑动卷积共享权重的方式降低参数空间；图网络认为中心节点与邻居节点的相似性会更好地引导信息流动。")]),t._v(" "),i("p",[t._v("通常，模型容量（capacity）很大但inductive bias匮乏，容易导致过拟合（overfitting），如Transformer。")]),t._v(" "),i("blockquote",[i("p",[t._v("引用自：https://blog.csdn.net/qq_39478403/article/details/121107057")])]),t._v(" "),i("p",[t._v("网络架构搜索（neural architecture search）")]),t._v(" "),i("p",[t._v("机器学习算法的效果很大程度上取决于各种超参数，深度学习兴起之前，超参数自动搜索优化方法主要包括：随机搜索（random search）、网络搜索（grid search）、贝叶斯优化（Bayesian optimization）、强化学习（reinforcement learning）、进化算法（evolutional algorithm）等，统称为Hyperparameter optimization（HO）。")]),t._v(" "),i("p",[t._v("深度学习领域中主要涉及到的超参数分为两类：（1）训练参数，包括learning rate、batch-size，weight decay等；（2）网络结构参数，包括网络层数，每一层算子，卷积中的filter size等。对于（1）中的参数自动调优仍属于HO范畴，（2）中参数具有纬度高，离散且相互依赖等特点，其自动调优一般称为网络结构搜索（NAS）。")]),t._v(" "),i("p",[t._v("网络结构的设计很大程度上还是需要hand-craft，且依赖经验。寻求各种新的更优的网络子结构是一个研究热点。")]),t._v(" "),i("blockquote",[i("p",[t._v("参考资料：https://blog.csdn.net/jinzhuojun/article/details/84698471")])])],1)}),[],!1,null,null,null);s.default=e.exports}}]);