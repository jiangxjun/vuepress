(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{703:function(e,t,a){"use strict";a.r(t);var r=a(1),n=Object(r.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("blockquote",[a("p",[e._v("Torres J F, Hadjout D, Sebaa A, et al. Deep learning for time series forecasting: a survey[J]. Big Data, 2021, 9(1): 3-21.")])]),e._v(" "),a("h2",{attrs:{id:"time-series-definition"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#time-series-definition"}},[e._v("#")]),e._v(" Time series definition")]),e._v(" "),a("p",[e._v("A sequence of values, chronologically ordered, and observed over time.\nNot every time series can be modeled in this way, due to some of the following reasons:\n(1)missing data\n(2)outlying data\n(3)collected at irregular time periods\nIf the data are collected irregularly, this should be accounted for in the model.")]),e._v(" "),a("h2",{attrs:{id:"time-series-components"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#time-series-components"}},[e._v("#")]),e._v(" Time series components")]),e._v(" "),a("p",[e._v("Time series are usually characterized by three components: "),a("strong",[e._v("trend, seasonality, and irregular components")]),e._v(", also known as residuals.")]),e._v(" "),a("h2",{attrs:{id:"key-issues"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#key-issues"}},[e._v("#")]),e._v(" Key issues")]),e._v(" "),a("h3",{attrs:{id:"irregular-component"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#irregular-component"}},[e._v("#")]),e._v(" irregular component")]),e._v(" "),a("p",[e._v("Real-world time series present a meaningful "),a("strong",[e._v("irregular component")]),e._v(" and are not stationary(均值和方差随时间变化), turning this component into the most challenging one to model.\nFor this reason, to make accurate predictions for them is extremely difficult.\nMany classical forecasting methods try to decompose the target time series into trend, seasonality, and residual components and make predictions for all of them seperately.")]),e._v(" "),a("h3",{attrs:{id:"short-and-long-time-series-forecasting"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#short-and-long-time-series-forecasting"}},[e._v("#")]),e._v(" short-and  long time series forecasting")]),e._v(" "),a("p",[e._v("Flexible nonparametric(非参数) models could be used, but this still assumes that the model structure will work over the whole period of the data, which is not always true.\nA better approach consists of allowing the models to vary over time, including:\n①adjusting a parametric model with time-varying parameters;\n②adjusting a nonparametric model with a time-based kernel.")]),e._v(" "),a("h2",{attrs:{id:"methods"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#methods"}},[e._v("#")]),e._v(" Methods")]),e._v(" "),a("p",[e._v("(1)statistical approaches\nARIMA model\n(2)machine-learning algorithms\n(3)"),a("strong",[e._v("deep-learning models")]),e._v("\n①Deep feed forward neural networks(DFFNNs)\n②Recurrent neural networks(RNNs)\ntraditional feed-forward neural networks cannot take into account temporal dependency, and RNNs arise precisely to address this problem.\n③Elman network(ENN)\nENN consists of a classical one-layer feed-forward network but the hidden layer is connected to a new layer, called context layer, using fixed weight equal to one.\n④Long short-term memory(LSTM)\nLSTM recurrent networks emerge to solve the vanishing gradient problem.\n⑤Gated recurrent units(GRU)\nGRU is a simplification of LSTMs due to the high computational cost of the LSTM networks.\n⑥Bidirectional RNN(BRNN)\nA BRNN can be seen as two RNNs together, where the different hidden units have two values, one computed by forward and another one by backward.\n⑦Deep recurrenct neural network(DRNN)\nA DRNN can be considered as an RNN with more than one layer, alse called stacked RNN.\n⑧Convolutional neural network(CNN)\nA variant of CNN, called temporal CNNs(TCNs) has emerged for data sequence, competing directly with DRNNs in terms of execution times and memory requirements.")]),e._v(" "),a("h2",{attrs:{id:"practical-aspects"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#practical-aspects"}},[e._v("#")]),e._v(" Practical aspects")]),e._v(" "),a("p",[e._v("Implementation——deep-learning frameworks\nHyper-parameters——model parameters and optimization parameters\nHyper-parameter optimization——search strategies and libraries\nHardware performance——CPU/GPU/TPU/IPU")]),e._v(" "),a("h2",{attrs:{id:"applications"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#applications"}},[e._v("#")]),e._v(" Applications")])])}),[],!1,null,null,null);t.default=n.exports}}]);