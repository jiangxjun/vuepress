(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{544:function(t,a,s){t.exports=s.p+"assets/img/architecture.04aa3eff.png"},545:function(t,a,s){t.exports=s.p+"assets/img/input.f3b8aaa6.png"},546:function(t,a,s){t.exports=s.p+"assets/img/pre-training.c561818e.png"},547:function(t,a,s){t.exports=s.p+"assets/img/MLM.a5d3681b.png"},548:function(t,a,s){t.exports=s.p+"assets/img/tasks.e3d7030d.png"},687:function(t,a,s){"use strict";s.r(a);var i=s(1),n=Object(i.a)({},(function(){var t=this,a=t.$createElement,i=t._self._c||a;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("h2",{attrs:{id:"introduction"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),i("p",[t._v("BERT（Bidirectional Encoder Representations from Transformers）是一个新的语言表达模型，设计被用于预训练的未标记文本深度双向表示，作用于所有层的左右上下文。预训练的BERT只需要一个额外的输出层就可以实现模型微调，针对问答（question answering）和语言推理（language inference）等问题，不需要进行大量实质性的模型修改。")]),t._v(" "),i("h2",{attrs:{id:"language-model"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#language-model"}},[t._v("#")]),t._v(" Language Model")]),t._v(" "),i("p",[t._v("语言模型预训练已经被证明可以有效地提高许多自然语言处理任务的性能。")]),t._v(" "),i("table",[i("thead",[i("tr",[i("th",[t._v("任务")]),t._v(" "),i("th",[t._v("层次")])])]),t._v(" "),i("tbody",[i("tr",[i("td",[t._v("token-level")]),t._v(" "),i("td",[t._v("命名实体识别（NER）、SQuAD问答任务等")])]),t._v(" "),i("tr",[i("td",[t._v("sentence-level")]),t._v(" "),i("td",[t._v("自然语言推断（NLI）、情感识别、句子语义等价判断等")])])])]),t._v(" "),i("p",[t._v("针对自然语处理的下游任务（downstream-task），预训练的语言模型发挥重要作用。现有的语言模型（LM）应用到NLP任务主要有两种策略：feature-based和fine-tuning"),i("sup",{staticClass:"footnote-ref"},[i("a",{attrs:{href:"#footnote1"}},[t._v("[1]")]),i("a",{staticClass:"footnote-anchor",attrs:{id:"footnote-ref1"}})]),t._v("。")]),t._v(" "),i("table",[i("thead",[i("tr",[i("th",[t._v("feature-based")]),t._v(" "),i("th",[t._v("fine-tuning")])])]),t._v(" "),i("tbody",[i("tr",[i("td",[t._v("ELMo ("),i("em",[t._v("Peters et al., 2018")]),t._v(")")]),t._v(" "),i("td",[t._v("OpenAI GPT ("),i("em",[t._v("Radford et al., 2018")]),t._v(")")])])])]),t._v(" "),i("p",[t._v("BERT是基于fine-tuning预训练方法的改进，提出了Masked Language Model（"),i("strong",[t._v("MLM")]),t._v("）实现深度双向表示（deep bidirectional representation）的预训练；此外，引入了Next Sentence Prediction（"),i("strong",[t._v("NSP")]),t._v("）任务，目的是让模型能够更好地理解句子间的关系。")]),t._v(" "),i("h2",{attrs:{id:"bert-architecture"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#bert-architecture"}},[t._v("#")]),t._v(" BERT Architecture")]),t._v(" "),i("center",[i("img",{attrs:{src:s(544),title:"BERT结构",width:"80%"}})]),t._v(" "),i("p",[t._v("BERT的结构是一个多层的双向的Transformer Encoder，模型要比Transformer深。")]),t._v(" "),i("table",[i("thead",[i("tr",[i("th",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("S")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm BERT}_{\\rm BASE}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("BERT")])])]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("BASE")])])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])]),t._v(" "),i("th",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("L")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("G")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm BERT}_{\\rm LARGE}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("BERT")])])]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("LARGE")])])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])]),t._v(" "),i("th",[t._v("Transformer")])])]),t._v(" "),i("tbody",[i("tr",[i("td",[t._v("层数：Transformer blocks ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("L")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("L")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("L")])])])]),t._v(")=12")]),t._v(" "),i("td",[t._v("Transformer blocks ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("L")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("L")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("L")])])])]),t._v(")=24")]),t._v(" "),i("td",[t._v("Encode block = 6")])]),t._v(" "),i("tr",[i("td",[t._v("Hidden size ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("H")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])]),t._v(") = 768")]),t._v(" "),i("td",[t._v("Hidden size ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("H")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])]),t._v(") = 1024")]),t._v(" "),i("td")]),t._v(" "),i("tr",[i("td",[t._v("feed-forward/filter size = "),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("4")]),i("mo",[t._v("×")]),i("mi",[t._v("H")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("4\\times H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7278em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord"},[t._v("4")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])]),t._v(" =3072")]),t._v(" "),i("td",[t._v("feed-forward/filter size = "),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("4")]),i("mo",[t._v("×")]),i("mi",[t._v("H")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("4\\times H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7278em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord"},[t._v("4")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])]),t._v(" =4096")]),t._v(" "),i("td")]),t._v(" "),i("tr",[i("td",[t._v("自注意力头数：self-attention heads ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("A")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("A")])])])]),t._v(") =12")]),t._v(" "),i("td",[t._v("self-attention heads ("),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("A")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("A")])])])]),t._v(") =16")]),t._v(" "),i("td")]),t._v(" "),i("tr",[i("td",[t._v("Total parameters = 110M")]),t._v(" "),i("td",[t._v("Total parameters = 340M")]),t._v(" "),i("td")])])]),t._v(" "),i("p",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")])],1),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("S")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm BERT}_{\\rm BASE}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("BERT")])])]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("BASE")])])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("模型的尺寸和OpenAI GPT保持一致便于性能对比，唯一差别是BERT Transfomer使用的是bidirectional self-attention （也被称为"),i("strong",[t._v("Transformer Encoder")]),t._v("）。")]),t._v(" "),i("table",[i("thead",[i("tr",[i("th",[t._v("模型")]),t._v(" "),i("th",[t._v("左右上下文语义")]),t._v(" "),i("th",[t._v("是否并行")])])]),t._v(" "),i("tbody",[i("tr",[i("td",[t._v("Word2Vec")]),t._v(" "),i("td",[t._v("True")]),t._v(" "),i("td",[t._v("True")])]),t._v(" "),i("tr",[i("td",[t._v("ELMo")]),t._v(" "),i("td",[t._v("True ("),i("strong",[t._v("shallow bidirectional")]),t._v(")")]),t._v(" "),i("td",[t._v("False")])]),t._v(" "),i("tr",[i("td",[t._v("OpenAI GPT")]),t._v(" "),i("td",[t._v("False")]),t._v(" "),i("td",[t._v("True")])]),t._v(" "),i("tr",[i("td",[t._v("BERT")]),t._v(" "),i("td",[t._v("True ("),i("strong",[t._v("deep bidirectional")]),t._v(")")]),t._v(" "),i("td",[t._v("True")])])])]),t._v(" "),i("p",[i("strong",[t._v("OpenAI GPT")]),t._v("使用的是constrained self-attention，每一个token只关注左侧的上下文信息，left-context-only版本的self-attention也被称为"),i("strong",[t._v("Transformer Decoder")]),t._v("。")]),t._v(" "),i("p",[t._v("上图中的"),i("strong",[t._v("ELMo模型")]),t._v("使用的是"),i("em",[t._v("concatenation of independency trained left-to-right and right-to-left LSTM")]),t._v("来生成用于下游任务的特征，称为"),i("strong",[t._v("浅层双向模型")]),t._v("（shallow bidirectional model）。")]),t._v(" "),i("p",[i("strong",[t._v("Word2Vec的CBOW")]),t._v("模型是通过token的上文信息和下文信息来预测该token，使用的是词袋模型，不知道单词的顺序信息，相当于直接把该token给mask了。")]),t._v(" "),i("p",[i("strong",[t._v("BERT")]),t._v("可以将每一个token的左边上下文信息和右边的上下文信息结合起来，这样可以更好地根据全文理解每一个token的意思，训练一个"),i("strong",[t._v("深度双向模型")]),t._v("（deep bidirectional model）")]),t._v(" "),i("h2",{attrs:{id:"bert-input-representation"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#bert-input-representation"}},[t._v("#")]),t._v(" BERT Input Representation")]),t._v(" "),i("center",[i("img",{attrs:{src:s(545),title:"BERT的输入表示",width:"60%"}})]),t._v(" "),i("p",[t._v("BERT支持一个句子对输入（句子A和句子B）或单个句子的输入，BERT的输入会在分词的句子里面增加有特殊作用的标志位：")]),t._v(" "),i("ul",[i("li",[i("p",[i("code",[t._v("[CLS]")]),t._v("放在句首，classification，用于下游的分类任务。该符号对应的输出向量作为整个输入文本的语义表示，用于文本分类。")]),t._v(" "),i("ul",[i("li",[i("strong",[t._v("单句子任务")]),t._v("：如情感分析，"),i("code",[t._v("[CLS]")]),t._v("符号对于的输出向量作为整个输入文本的语义表示，用于文本分类。")]),t._v(" "),i("li",[i("strong",[t._v("语句对分类任务")]),t._v("：包括问答（判断问题和答案是否匹配）、语句匹配（两句话是否表达同一个意思）等，使用"),i("code",[t._v("[CLS]")]),t._v("作为文本的语义表示，同时也利用"),i("code",[t._v("[SEP]")]),t._v("作为分割标识符，分别对两句话附加两个不同的文本向量作为区分。")])])]),t._v(" "),i("li",[i("p",[i("code",[t._v("[SEP]")]),t._v("用于分开两个输入的句子（A和B），分别在句子A和B的最后增加该标志")])]),t._v(" "),i("li",[i("p",[i("code",[t._v("[UNK]")]),t._v("表示未知字符")])]),t._v(" "),i("li",[i("p",[i("code",[t._v("[MASK]")]),t._v("用于遮盖句子中的一些单词，将单词用MASK遮盖后，根据上下文语境，利用BERT输出的[MASK]向量预测单词是什么。")])])]),t._v(" "),i("p",[t._v("BERT得到要输入的句子进行分词"),i("strong",[t._v("tokenizer")]),t._v("得到token，然后将句子的token转成"),i("strong",[t._v("Embedding")]),t._v("，用"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("E")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("E")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("E")])])])]),t._v("表示。")]),t._v(" "),i("div",{staticClass:"custom-block tip"},[i("p",{staticClass:"custom-block-title"},[t._v("提示")]),t._v(" "),i("p",[t._v("BERT使用一个词表文件"),i("code",[t._v("vocab.txt")]),t._v("作为对输入进行分词的一个依据，对输入文本进行"),i("code",[t._v("split")]),t._v("操作后得到token。BERT包含三个tokenizer方式：")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("BasicsTokenizer")]),t._v("：清理特殊字符，包括控制符以及替换空白字符为空格。")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("WordpieceTokenier")]),t._v("：对输入进行"),i("code",[t._v("split")]),t._v("，得到token的列表。如果token的一部分仍然在"),i("code",[t._v("vocab.txt")]),t._v("中，则继续分词，分割后除去第一部分外，其他部分都会加上"),i("code",[t._v("##")]),t._v("，得到WordPiece。如"),i("code",[t._v("playing")]),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mo",[t._v("→")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\rightarrow")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.3669em"}}),i("span",{staticClass:"mrel"},[t._v("→")])])])]),i("code",[t._v("play")]),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mo",[t._v("+")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("+")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord"},[t._v("+")])])])]),i("code",[t._v("##ing")]),t._v("。")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("FullTokenier")]),t._v("：调用BasicTokenier得到初步的token列表，对于token再调用WordpieceTokenizer，得到更细分的token。")])])])]),t._v(" "),i("p",[t._v("BERT的输入Embedding包括三个部分构成：Token Embedding，Segment Embedding，Position Embedding。")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("Token Embedding")]),t._v("：通过训练学习得到，如上图中的"),i("code",[t._v("[CLS]")]),t._v(","),i("code",[t._v("dog")]),t._v("等。")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Segment Embedding")]),t._v("：用于区分每一个单词属于句子A或B，如果只有一个句子，用"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",[t._v("E")]),i("mi",[t._v("A")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("E_A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05764em"}},[t._v("E")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"-0.0576em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight"},[t._v("A")])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])]),t._v("表示，通过学习得到")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Position Embedding")]),t._v("：编码单词出现的位置，通过学习得到。在BERT中，默认的句子最大长度为"),i("strong",[t._v("512")]),t._v("。")])])]),t._v(" "),i("h2",{attrs:{id:"pre-training-tasks"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#pre-training-tasks"}},[t._v("#")]),t._v(" Pre-training Tasks")]),t._v(" "),i("p",[t._v("BERT预训练模型包含两个任务："),i("strong",[t._v("Masked LM")]),t._v("和"),i("strong",[t._v("NSP")]),t._v("。")]),t._v(" "),i("h3",{attrs:{id:"masked-lm任务"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#masked-lm任务"}},[t._v("#")]),t._v(" Masked LM任务")]),t._v(" "),i("p",[t._v("在句子中随机用"),i("code",[t._v("[MASK]")]),t._v("替换一部分单词，然后将句子传入BERT模型编码每一个token的信息，最终用"),i("code",[t._v("[MASK]")]),t._v("的编码信息"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("msub",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")]),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("M")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("S")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("K")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm T_{MASK}}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("T")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("MASK")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])])]),t._v("预测该位置正确的token。")]),t._v(" "),i("center",[i("img",{attrs:{src:s(546),title:"BERT预训练过程",width:"70%"}})]),t._v(" "),i("p",[t._v("图源"),i("sup",{staticClass:"footnote-ref"},[i("a",{attrs:{href:"#footnote2"}},[t._v("[2]")]),i("a",{staticClass:"footnote-anchor",attrs:{id:"footnote-ref2"}})]),t._v("。")]),t._v(" "),i("p",[t._v("在原文"),i("sup",{staticClass:"footnote-ref"},[i("a",{attrs:{href:"#footnote1"}},[t._v("[1:1]")]),i("a",{staticClass:"footnote-anchor",attrs:{id:"footnote-ref1:1"}})]),t._v("中，随机mask了每一个sentence的全部WordPiece token的15%。")]),t._v(" "),i("p",[t._v("为了减缓"),i("strong",[t._v("错配")]),t._v("的发生，并不是所有的待遮盖的token都会用"),i("code",[t._v("[MASK]")]),t._v("符号代替，在上述任意选择的15%待遮盖的token中，再任意选择其中的80%用"),i("code",[t._v("[MASK]")]),t._v("代替，10%用任意的其他随机token取代（i.e., 被随机取代的token占总的token的1.5%），剩余的10%保持不变。这样Transformer Encoder就无法知道哪一个token是要被预测，哪一个token被其他任意单词取代。")]),t._v(" "),i("p",[t._v("每一个batch的15%的token会被预测，这样会导致MLM需要"),i("strong",[t._v("更多的epoch才能达到收敛")]),t._v("。文章也说明了花费相对较长一些的computational cost获得更高的预测准确率是值得的。")]),t._v(" "),i("div",{staticClass:"custom-block tip"},[i("p",{staticClass:"custom-block-title"},[t._v("提示")]),t._v(" "),i("p",[t._v("在语言模型（LM）中，常需要用上一个词预测下一个词，BERT中融合了左右两侧上下文信息实现预训练深度双向Transformer，要使得在"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("t")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6151em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("t")])])])]),t._v("时刻看不到"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("t")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("t")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6151em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("t")])])])]),t._v("时刻之后的信息，需要用"),i("code",[t._v("[MASK]")]),t._v("来“遮盖”。")]),t._v(" "),i("center",[i("img",{attrs:{src:s(547),title:"BERT的Masked LM",width:"50%"}})]),t._v(" "),i("p",[t._v("图源"),i("sup",{staticClass:"footnote-ref"},[i("a",{attrs:{href:"#footnote2"}},[t._v("[2:1]")]),i("a",{staticClass:"footnote-anchor",attrs:{id:"footnote-ref2:1"}})]),t._v("。")])],1),t._v(" "),i("h3",{attrs:{id:"nsp任务"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#nsp任务"}},[t._v("#")]),t._v(" NSP任务")]),t._v(" "),i("p",[t._v("下一句预测是将句子"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("A")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("A")])])])]),t._v("和"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("B")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("B")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")])])])]),t._v("输入BERT，预测"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("B")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("B")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")])])])]),t._v("是否是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("A")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("A")])])])]),t._v("的下一句，使用"),i("code",[t._v("[CLS]")]),t._v("的编码信息"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("C")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("C")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")])])])]),t._v("进行预测。")]),t._v(" "),i("p",[t._v("NSP应用的下游任务场景包括：Question Answering (QA)，Natural Language Inference (NLI)等，主要解决的是理解"),i("strong",[t._v("两个文本句子")]),t._v("之间的关系。")]),t._v(" "),i("p",[t._v("原文"),i("sup",{staticClass:"footnote-ref"},[i("a",{attrs:{href:"#footnote1"}},[t._v("[1:2]")]),i("a",{staticClass:"footnote-anchor",attrs:{id:"footnote-ref1:2"}})]),t._v("中的预训练样本，有50%的sentence pair是连续相连的，另外50%的是从语料库中随机选择不连续的。然后通过"),i("code",[t._v("[CLS]")]),t._v("标志位的输出"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("C")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("C")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")])])])]),t._v("预测句子"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("A")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("A")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal"},[t._v("A")])])])]),t._v("的下一句"),i("code",[t._v("IsNext")]),t._v(" or "),i("code",[t._v("NotNext")]),t._v("是"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("B")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("B")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.05017em"}},[t._v("B")])])])]),t._v("。")]),t._v(" "),i("h2",{attrs:{id:"pre-training-procedure"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#pre-training-procedure"}},[t._v("#")]),t._v(" Pre-training Procedure")]),t._v(" "),i("table",[i("thead",[i("tr",[i("th",[t._v("Hyperparameter")]),t._v(" "),i("th",[t._v("value")])])]),t._v(" "),i("tbody",[i("tr",[i("td",[t._v("batch size")]),t._v(" "),i("td",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("256")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("256")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("256")])])])]),t._v(" sequences="),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("256")]),i("mo",[t._v("×")]),i("mn",[t._v("512")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("256 \\times 512")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7278em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord"},[t._v("256")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("×")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("512")])])])]),t._v(" tokens="),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("128")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mn",[t._v("000")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("128,000")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8389em","vertical-align":"-0.1944em"}}),i("span",{staticClass:"mord"},[t._v("128")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.1667em"}}),i("span",{staticClass:"mord"},[t._v("000")])])])]),t._v(" tokens/batch")])]),t._v(" "),i("tr",[i("td",[t._v("epoch")]),t._v(" "),i("td",[t._v("40 epoches for 1M steps")])]),t._v(" "),i("tr",[i("td",[t._v("word corpus size")]),t._v(" "),i("td",[t._v("3.3 billion")])]),t._v(" "),i("tr",[i("td",[t._v("Optimizer")]),t._v(" "),i("td",[t._v("Adam")])]),t._v(" "),i("tr",[i("td",[t._v("learning rate")]),t._v(" "),i("td",[t._v("warmup over the first 10000 steps at "),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mn",[t._v("1")]),i("mi",[t._v("e")]),i("mo",[t._v("−")]),i("mn",[t._v("4")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("1e-4")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7278em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord"},[t._v("1")]),i("span",{staticClass:"mord mathnormal"},[t._v("e")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("−")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("4")])])])]),t._v("  and linear decay after 10000 steps")])]),t._v(" "),i("tr",[i("td",[t._v("Dropout")]),t._v(" "),i("td",[t._v("0.1 on all layers")])]),t._v(" "),i("tr",[i("td",[t._v("activation function")]),t._v(" "),i("td",[t._v("gelu")])]),t._v(" "),i("tr",[i("td",[t._v("training loss")]),t._v(" "),i("td",[t._v("MLM似然值的均值和NSP似然值的均值的和")])])])]),t._v(" "),i("p",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("msub",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")]),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("S")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm BERT_{BASE}}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("BER")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("T")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("BASE")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])])]),t._v("用到了16个TPU（张量处理器），"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("B")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("msub",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("T")]),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("L")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("A")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("R")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("G")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("E")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("{\\rm BERT_{LARGE}}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8333em","vertical-align":"-0.15em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("BER")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("T")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t vlist-t2"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.3283em"}},[i("span",{staticStyle:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathrm mtight"},[t._v("LARGE")])])])])]),i("span",{staticClass:"vlist-s"},[t._v("​")])]),i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.15em"}},[i("span")])])])])])])])])])]),t._v("用到了64个TPU，每一次训练时间4 days。")]),t._v(" "),i("h2",{attrs:{id:"fine-tuning-procedure"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#fine-tuning-procedure"}},[t._v("#")]),t._v(" Fine-tuning Procedure")]),t._v(" "),i("p",[t._v("对于序列级（sequence-level）的分类任务，BERT将最终的隐藏层输出（i.e., Transformer的输出）给第一个token（i.e., "),i("code",[t._v("[CLS]")]),t._v("）的word embedding。"),i("code",[t._v("[CLS]")]),t._v("标志位的输出"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("C")]),i("mo",[t._v("∈")]),i("msup",[i("mi",{attrs:{mathvariant:"double-struck"}},[t._v("R")]),i("mi",[t._v("H")])],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("C \\in \\mathbb{R}^H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7224em","vertical-align":"-0.0391em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("∈")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8413em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathbb"},[t._v("R")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8413em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])])])])])])])])]),t._v("，利用softmax输出标签概率"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("P")]),i("mo",[t._v("=")]),i("mrow",[i("mi",{attrs:{mathvariant:"normal"}},[t._v("s")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("o")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("f")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("t")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("m")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("a")]),i("mi",{attrs:{mathvariant:"normal"}},[t._v("x")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v("(")]),i("mi",[t._v("C")]),i("msup",[i("mi",[t._v("W")]),i("mi",[t._v("T")])],1),i("mo",{attrs:{stretchy:"false"}},[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("P = {\\rm softmax}(CW^T)")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("P")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("=")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"1.0913em","vertical-align":"-0.25em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathrm"},[t._v("softmax")])])]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("C")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8413em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])])])])])])]),i("span",{staticClass:"mclose"},[t._v(")")])])])]),t._v("，其中"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("W")]),i("mo",[t._v("∈")]),i("msup",[i("mi",{attrs:{mathvariant:"double-struck"}},[t._v("R")]),i("mrow",[i("mi",[t._v("K")]),i("mo",[t._v("×")]),i("mi",[t._v("H")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("W \\in \\mathbb{R}^{K \\times H}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7224em","vertical-align":"-0.0391em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("∈")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8413em"}}),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathbb"},[t._v("R")]),i("span",{staticClass:"msupsub"},[i("span",{staticClass:"vlist-t"},[i("span",{staticClass:"vlist-r"},[i("span",{staticClass:"vlist",staticStyle:{height:"0.8413em"}},[i("span",{staticStyle:{top:"-3.063em","margin-right":"0.05em"}},[i("span",{staticClass:"pstrut",staticStyle:{height:"2.7em"}}),i("span",{staticClass:"sizing reset-size6 size3 mtight"},[i("span",{staticClass:"mord mtight"},[i("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),i("span",{staticClass:"mbin mtight"},[t._v("×")]),i("span",{staticClass:"mord mathnormal mtight",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])])])])])])])])])]),t._v("是唯一个在fine-tuning阶段分类层引入的参数，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("K")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("K")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")])])])]),t._v("是分类器数量，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("H")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("H")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.08125em"}},[t._v("H")])])])]),t._v("是Transformer Encoder的隐含层大小。")]),t._v(" "),i("h2",{attrs:{id:"bert应用到具体nlp任务"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#bert应用到具体nlp任务"}},[t._v("#")]),t._v(" BERT应用到具体NLP任务")]),t._v(" "),i("center",[i("img",{attrs:{src:s(548),title:"BERT应用在具体的NLP任务",width:"60%"}})]),t._v(" "),i("p",[t._v("(a)"),i("strong",[t._v("一对句子的分类任务")]),t._v("：多中心的自然语言推断（Multi-centre Natural Language Inference，MNLI）是一个众包蕴含（crowdsource entailment）的分类任务，句子语义等价判断（Quara Question Pairs, QQP）是一个二值分类任务。将两个句子传入BERT，通过"),i("code",[t._v("[CLS]")]),t._v("标志位的输出值（来自Transformer Encoder的输出值）进行句子分类。")]),t._v(" "),i("p",[t._v("(b)"),i("strong",[t._v("单个句子分类任务")]),t._v("：如句子情感分析（Stanford Sentiment Treebank, SST-2），判断句子语法是否可以接受（The Corpus of Linguistic Acceptability, CoLA）等。输入一个句子，通过"),i("code",[t._v("[CLS]")]),t._v("标志位的输出值进行分类。")]),t._v(" "),i("p",[t._v("(c)"),i("strong",[t._v("问答任务")]),t._v("：样本是一个语句对（Question，Paragraph），训练的目标是在Paragraph找出答案的起始位置（Start，End）。")]),t._v(" "),i("p",[t._v("(d)"),i("strong",[t._v("单个句子标注")]),t._v("：如命名实体识别（NER），输入单个句子，然后根据BERT对于每个token的输出"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("T")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("T")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])])]),t._v("预测该token的类别，是属于Person，Organization，Location，Miscellaneous 还是 Other (非命名实体)。")]),t._v(" "),i("h1",{attrs:{id:"questions"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#questions"}},[t._v("#")]),t._v(" Questions")]),t._v(" "),i("p",[t._v("1、BERT的三个Embedding为什么可以进行相加？")]),t._v(" "),i("blockquote",[i("p",[t._v("一串文本可以看作是一些不同频率的时序信号进行叠加，其中每个词的特征可以用叠加波来表示，整个文本序列也可以进一步叠加。哪些是低频信号（如词性？），哪些是高频信号（如语义？），这些都隐藏在Embedding中，也可能已经解耦在不同的维度中。")]),t._v(" "),i("p",[t._v("本质上三个Embedding相加，可以看做是一个特征的融合，BERT可以学习到融合特征的语义信息。")]),t._v(" "),i("p",[t._v("Answer From：https://www.zhihu.com/question/374835153——邱锡鹏老师的回答")])]),t._v(" "),i("p",[t._v("2、使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？")]),t._v(" "),i("blockquote",[i("p",[t._v("在BERT中，Token，Position，Segement Embedding都是"),i("strong",[t._v("通过学习得到")]),t._v("的，Pytorch代码中：")]),t._v(" "),i("div",{staticClass:"language-python line-numbers-mode"},[i("pre",{pre:!0,attrs:{class:"language-python"}},[i("code",[t._v("self"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("word_embeddings "),i("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Embedding"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("vocab_size"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hidden_size"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nself"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("position_embeddings "),i("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Embedding"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("max_position_embeddings"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hidden_size"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nself"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("token_type_embeddings "),i("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Embedding"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("type_vocab_size"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" config"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("hidden_size"),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),i("div",{staticClass:"line-numbers-wrapper"},[i("span",{staticClass:"line-number"},[t._v("1")]),i("br"),i("span",{staticClass:"line-number"},[t._v("2")]),i("br"),i("span",{staticClass:"line-number"},[t._v("3")]),i("br")])]),i("p",[t._v("Pytorch版的BERT代码地址：https://github.com/xieyufei1993/Bert-Pytorch-Chinese-TextClassification")]),t._v(" "),i("p",[t._v("在BERT config中：")]),t._v(" "),i("div",{staticClass:"language-python line-numbers-mode"},[i("pre",{pre:!0,attrs:{class:"language-python"}},[i("code",[i("span",{pre:!0,attrs:{class:"token string"}},[t._v('"max_position_embeddings"')]),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),i("span",{pre:!0,attrs:{class:"token number"}},[t._v("512")]),t._v("\n"),i("span",{pre:!0,attrs:{class:"token string"}},[t._v('"type_vocab_size"')]),i("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),i("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\n")])]),t._v(" "),i("div",{staticClass:"line-numbers-wrapper"},[i("span",{staticClass:"line-number"},[t._v("1")]),i("br"),i("span",{staticClass:"line-number"},[t._v("2")]),i("br")])]),i("p",[t._v("因此，在直接使用Google 的BERT预训练模型时，输入最多512个词（还要除掉"),i("code",[t._v("[CLS]")]),t._v("和"),i("code",[t._v("[SEP]")]),t._v("，即实际510个词），最多两个句子合成一句。这之外的词和句子会没有对应的embedding。")]),t._v(" "),i("p",[t._v("BERT本身的训练对于显存有很高的要求，参数量特别大。如果有足够的硬件资源自己重新训练BERT，可以更改 BERT config，设置更大max_position_embeddings 和 type_vocab_size值去满足自己的需求。")]),t._v(" "),i("p",[t._v("Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点")])]),t._v(" "),i("p",[t._v("3、BERT的第一句前加一个"),i("code",[t._v("[CLS]")]),t._v("标志位作用？")]),t._v(" "),i("blockquote",[i("p",[t._v("在Transformer Encoder的最后一层该位的输出向量可以作为整个语句的语义表示，从而用于下游的分类任务。与文中已有的其他词相比，这一个无明显语义的信息符号"),i("strong",[t._v("更能“公平”地融合")]),t._v("文本中各个词的语义信息。")]),t._v(" "),i("p",[t._v("Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点")])]),t._v(" "),i("p",[t._v("4、BERT的非线性来源？")]),t._v(" "),i("blockquote",[i("p",[t._v("前馈层的gelu激活函数和self-attention，self-attention是非线性的。")]),t._v(" "),i("p",[t._v("Answer From：https://zhuanlan.zhihu.com/p/132554155——超细节的BERT/Transformer知识点")])]),t._v(" "),i("p",[t._v("5、BERT中，长文本问题解决？")]),t._v(" "),i("blockquote",[i("p",[t._v("BERT在进行MLM预训练时，规定了最大的输入长度不超过512个token。对于超过512个token的长文本，无法直接调用开源的pretrained-model进行fine-tuning。在比如阅读理解问题中，输入语句通常远大于512。")]),t._v(" "),i("p",[t._v("(1)"),i("strong",[t._v("Clipping（截断法）")]),t._v("：挑选其中重要的token输入模型。挑选前TOP N个token和文末TOP K个token，保证"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[i("semantics",[i("mrow",[i("mi",[t._v("N")]),i("mo",[t._v("+")]),i("mi",[t._v("K")]),i("mo",[t._v("≤")]),i("mn",[t._v("510")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("N+K \\le 510")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.7667em","vertical-align":"-0.0833em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}}),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.8193em","vertical-align":"-0.136em"}}),i("span",{staticClass:"mord mathnormal",staticStyle:{"margin-right":"0.07153em"}},[t._v("K")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}}),i("span",{staticClass:"mrel"},[t._v("≤")]),i("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2778em"}})]),i("span",{staticClass:"base"},[i("span",{staticClass:"strut",staticStyle:{height:"0.6444em"}}),i("span",{staticClass:"mord"},[t._v("510")])])])]),t._v("；会丢失信息。")]),t._v(" "),i("p",[t._v("(2)"),i("strong",[t._v("RNN循环法")])]),t._v(" "),i("p",[t._v("(3)"),i("strong",[t._v("Pooling（池化法）")]),t._v("：借鉴CNN中池化层作用是选取一个区域内最重要的特征值代替整个feature map的特征值（包括最大池化和平均池化），来实现降维功能，整个过程也被称为下采样（down samping）。同理，对于一个1000token的输入文本，得到1000个Embedding，通过Pooling方式获取其中最重要的510个Embedding（另两个是"),i("code",[t._v("[CLS]")]),t._v(","),i("code",[t._v("[SEP]")]),t._v("）代表整段文本的Embedding。")]),t._v(" "),i("p",[t._v("Answer From："),i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/504204038#:~:text=%E7%94%B1%E4%BA%8E%20BERT%20%E6%9C%80%E5%A4%9A%E5%8F%AA%E8%83%BD%E6%8E%A5%E5%8F%97%20510%20%E4%B8%AAtoken%20%E7%9A%84%E8%BE%93%E5%85%A5%EF%BC%8C%E5%9B%A0%E6%AD%A4%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%B0%86%E9%95%BF%E6%96%87%E6%9C%AC%E5%88%87%E5%89%B2%E6%88%90%E8%8B%A5%E5%B9%B2%E6%AE%B5%E3%80%82%20%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E6%9C%89%202,%2B%20%E7%AC%AC%202%20%E4%B8%AA%E5%8F%A5%E5%AD%90%E7%9A%84%202%20%E6%AE%B5%EF%BC%89%EF%BC%8C%E5%B9%B6%E6%94%BE%E5%88%B0%E4%B8%80%E4%B8%AA%20batch%20%E7%9A%84%E8%BE%93%E5%85%A5%E4%B8%AD%E5%96%82%E7%BB%99%E6%A8%A1%E5%9E%8B%E3%80%82",target:"_blank",rel:"noopener noreferrer"}},[t._v("一般有如下几种方法——此文主要介绍Pooling"),i("OutboundLink")],1)])]),t._v(" "),i("h2",{attrs:{id:"reference"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#reference"}},[t._v("#")]),t._v(" Reference")]),t._v(" "),i("hr",{staticClass:"footnotes-sep"}),t._v(" "),i("section",{staticClass:"footnotes"},[i("ol",{staticClass:"footnotes-list"},[i("li",{staticClass:"footnote-item",attrs:{id:"footnote1"}},[i("p",[t._v("BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., NAACL 2019) "),i("a",{staticClass:"footnote-backref",attrs:{href:"#footnote-ref1"}},[t._v("↩︎")]),t._v(" "),i("a",{staticClass:"footnote-backref",attrs:{href:"#footnote-ref1:1"}},[t._v("↩︎")]),t._v(" "),i("a",{staticClass:"footnote-backref",attrs:{href:"#footnote-ref1:2"}},[t._v("↩︎")])])]),t._v(" "),i("li",{staticClass:"footnote-item",attrs:{id:"footnote2"}},[i("p",[t._v("理解BERT模型：https://www.jianshu.com/p/46cb208d45c3 "),i("a",{staticClass:"footnote-backref",attrs:{href:"#footnote-ref2"}},[t._v("↩︎")]),t._v(" "),i("a",{staticClass:"footnote-backref",attrs:{href:"#footnote-ref2:1"}},[t._v("↩︎")])])])])])],1)}),[],!1,null,null,null);a.default=n.exports}}]);